{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Course - Spring 2025 - Sharif University of Technology\n",
    "## Homework 4 - VAE (100 points)\n",
    "\n",
    "*Instructor:  Dr. Soleymani*\n",
    "\n",
    "---\n",
    "\n",
    "*Full Name:*\n",
    "\n",
    "*SID:*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O53S9RpIe4tg"
   },
   "source": [
    "### Variational AutoEncoder(VAE)\n",
    "In this notebook we want to implement VAE and also get familiar with latent space and downstream tasks we can do with these latents.\n",
    "\n",
    "First, lets downlaod the dataset(Fashion MNIST) and create train and test data-loaders.\n",
    "These parts of code are implemented and you don't need to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6W6SC1Je6R7"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "train_data = datasets.FashionMNIST('./data', train=True, download=True,\n",
    "                            transform=transforms.ToTensor())\n",
    "test_data = datasets.FashionMNIST('./data', train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# pin memory provides improved transfer speed\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if device == 'cuda' else {}\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=128, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=128, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMNXB6FKnpY3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_image_with_label(image, label):\n",
    "\n",
    "    # Map from numerical labels to real labels\n",
    "    fashion_mnist_classes = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "    ]\n",
    "\n",
    "    # Remove the channel dimension and convert to numpy for plotting\n",
    "    image = image.squeeze().numpy()\n",
    "\n",
    "    # Plot the image\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(f\"Label: {fashion_mnist_classes[label]}\")\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "yDiUrxTNnul-",
    "outputId": "cdaf9bd8-e8d8-4e7e-d2b0-41e935558562"
   },
   "outputs": [],
   "source": [
    "plot_image_with_label(train_data[0][0], train_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrTrsBZTlbbH"
   },
   "source": [
    "### AutoEncoder\n",
    "In this cell, you will implement an AutoEncoder. AutoEncoder consists of an encoder and a decoder. Encoder is responsible for mapping image to latent dimension where the image gets compressed in a low dimensional vector.\n",
    "\n",
    "Fashion-MNIST images have (28, 28) shape, which is a (784, ) vector of real numbers. We want to compress this vector and just keep the most important and informational part of data (like what we do in PCA).\n",
    "\n",
    "Then in decoder, we need to reconstruct the latent vector and recreate the given image with minimum loss. So the decoder does the opposite of what encoder does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XA0pf4ZvkGmN"
   },
   "outputs": [],
   "source": [
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim=2):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, latent_dim),\n",
    "        )\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 28*28),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4FAAeyLlvke"
   },
   "source": [
    "Define model, optimizer, and loss function here.\n",
    "Reconstruction loss forces model to recreate given image from z latent with minimum distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3JBOGAEluuE"
   },
   "outputs": [],
   "source": [
    "\n",
    "# model and optimizer\n",
    "AE = AutoEncoder().to(device)\n",
    "optimizer = optim.Adam(AE.parameters(), lr=1e-3)\n",
    "\n",
    "# loss function (binary cross entropy)\n",
    "def loss_function(recon_x, x):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    return BCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOvRvJtal7Fq"
   },
   "source": [
    "Train the model with the reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4A0PCV-l6ew",
    "outputId": "98b20dd2-9a5e-4cd2-ac8a-87bbf58fa476"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(epochs, model):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "            # data: [batch size, 1, 28, 28]\n",
    "\n",
    "            #call optimizer\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            data = data.to(device)\n",
    "            data = data.view(-1, 28*28)\n",
    "\n",
    "            # call model\n",
    "            recon_batch, _ = model(data)\n",
    "            loss = loss_function(recon_batch, data)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: avg loss {train_loss/len(train_loader.dataset):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22mTVPbUoOgp"
   },
   "source": [
    "reconstruct the first image of test dataset and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839
    },
    "id": "VGjCdipSmIsZ",
    "outputId": "658f9a6b-57f6-4823-e5f2-a28e8a28e7ea"
   },
   "outputs": [],
   "source": [
    "\n",
    "image, label = test_data[0]\n",
    "plot_image_with_label(image, label)\n",
    "\n",
    "# encode and reconstruct model and plot\n",
    "with torch.no_grad():\n",
    "    x_decode, z = AE(image.unsqueeze(0).to(device).view(1, -1))\n",
    "    plot_image_with_label(x_decode.view(28, 28).to(\"cpu\"), label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIusDVdroMJZ"
   },
   "source": [
    "Try to sampe from AE. create random latent vectors, then decode them. you would possibly face with meaningless images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqYnAoSUn2sR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def sample_from_AE():\n",
    "    with torch.no_grad():\n",
    "        # create random vector\n",
    "        z = torch.randn(1, AE.latent_dim, device=device)\n",
    "\n",
    "        #decode and plot it\n",
    "        x = AE.decode(z).view(28, 28).cpu()\n",
    "        plot_image_with_label(x, 1) # random label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7UWJScyXoRry",
    "outputId": "cae6d683-47c0-4d7a-aee9-3e40a837cf3f"
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    sample_from_AE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPpMT47Yohii"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Creating an instance of the MNIST Dataset with\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Create a Dataloader instance for loading data in batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEc9p5CToUsO"
   },
   "outputs": [],
   "source": [
    "\n",
    "class VariationalAE(AutoEncoder):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mu_layer = nn.Linear(128, self.latent_dim)\n",
    "        self.logvar_layer = nn.Linear(128, self.latent_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder[:-1](x)  # up to 128-dim\n",
    "        mu = self.mu_layer(h)\n",
    "        logvar = self.logvar_layer(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c2nAVX_o-6B"
   },
   "source": [
    "Define model, optimizer, and loss here and then train vae.\n",
    "\n",
    "The loss function in a Variational Autoencoder (VAE) consists of two components:\n",
    "\n",
    "1. **Reconstruction Loss** (`criterion(decoded, images)`)  \n",
    "   - Measures how well the decoded output matches the original input.  \n",
    "   - Typically, Mean Squared Error (MSE) or Binary Cross-Entropy (BCE) is used.  \n",
    "   - Encourages the VAE to accurately reconstruct inputs.\n",
    "\n",
    "2. **Kullback-Leibler Divergence (KLD)**  \n",
    "   ```python\n",
    "   KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "**KLD** Measures how much the learned latent distribution deviates from a standard normal distribution (N(0, I)). This value is derived from the KL divergence formula between two Gaussians. It encourages the latent space to be continuous and structured, improving generative capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC4lwXmIpHGN"
   },
   "outputs": [],
   "source": [
    "\n",
    "VAE = VariationalAE().to(device)\n",
    "optimizer = optim.Adam(VAE.parameters(), lr=1e-3)\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T6rlFobBo508",
    "outputId": "2bea6410-c1de-423b-9d75-df14dd334925"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(epochs, model):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "            # data: [batch size, 1, 28, 28]\n",
    "            data = data.to(device).view(-1, 28*28)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}: avg loss {train_loss/len(train_loader.dataset):.4f}\")\n",
    "\n",
    "epochs = 15\n",
    "train(epochs, VAE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRB45yZfsKO-"
   },
   "source": [
    "Sample from VAE. Create noraml(0,1) variables then decode them and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uXkulDwpphK"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def plot_multiple_images(N, images):\n",
    "    images = images.numpy().squeeze(1)\n",
    "    cols = int(np.ceil(np.sqrt(N)))\n",
    "    rows = int(np.ceil(N / cols))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols, rows))\n",
    "    axes = axes.flatten()\n",
    "    for i in range(rows * cols):\n",
    "        if i < N:\n",
    "            axes[i].imshow(images[i], cmap='gray')\n",
    "            axes[i].axis('off')\n",
    "        else:\n",
    "            axes[i].remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def sample_from_model(N, model):\n",
    "    with torch.no_grad():\n",
    "        # p(z) = N(0,I), this distribution is used when calculating KLD. So we can sample z from N(0,I)\n",
    "        sample = torch.randn(N, model.latent_dim, device=device)\n",
    "        # decode samples\n",
    "        images = model.decode(sample).view(N, 1, 28, 28).cpu()\n",
    "        #plot them\n",
    "        plot_multiple_images(N, images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "HCuNRjMqs4_w",
    "outputId": "60359554-5cd1-40e9-b6db-bff75fd9c4d5"
   },
   "outputs": [],
   "source": [
    "sample_from_model(9, VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3xFUUnntWu5"
   },
   "source": [
    "# Traversing Latent Dimensions in VAE\n",
    "\n",
    "This function explores how individual dimensions of the **latent space** influence the generated output. By systematically varying a latent dimensions , we can understand how each dimension encodes different aspects of the data.\n",
    "\n",
    "create an arange for each of dimensions. Then decode all of these combinations and plot them in a single grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MKaI27c8s8XW"
   },
   "outputs": [],
   "source": [
    "def plot_along_axis(model):\n",
    "    # create aranges\n",
    "    z1 =\n",
    "    z2 =\n",
    "    num_z1 = z1.shape[0]\n",
    "    num_z2 = z2.shape[0]\n",
    "    num_z = num_z1 * num_z2\n",
    "\n",
    "    sample = torch.zeros(num_z, 2).to(device)\n",
    "\n",
    "    # create all possible combinations\n",
    "    for i in range(num_z1):\n",
    "        for j in range(num_z2):\n",
    "            idx =\n",
    "            sample[idx][0] =\n",
    "            sample[idx][1] =\n",
    "    # decode and plot them\n",
    "    with torch.no_grad():\n",
    "        sample =\n",
    "    plot_multiple_images(num_z, sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NzdUiHfRt72_",
    "outputId": "ce41519f-70d6-48a0-e07c-1711de597b49"
   },
   "outputs": [],
   "source": [
    "plot_along_axis(VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1B09SNj4MsB"
   },
   "source": [
    "# Clustering the VAE Latent Space\n",
    "\n",
    "This function analyzes the structure of the **latent space** in a Variational Autoencoder (VAE) by applying **K-Means clustering** and visualizing the results.\n",
    "\n",
    "Extract the latent space of test dateset. Use KMeans to cluster this space and plot it in 2d space.\n",
    "\n",
    "Plot one image from each cluster to see if they really represent labels from real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "op288-IuuEyD",
    "outputId": "a0f667d0-3629-450c-e8e4-a30b7e328309"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "def cluster_latent_space(vae, dataloader, n_clusters, device):\n",
    "    vae.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "\n",
    "    # iterate through images\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataloader:\n",
    "            img = img.to(device).view(-1, 28*28)\n",
    "            mu, logvar = vae.encode(img)\n",
    "            z = vae.reparameterize(mu, logvar)\n",
    "            latents.append(z.cpu())\n",
    "            labels.append(label)\n",
    "\n",
    "    latents = torch.cat(latents, dim=0).numpy()\n",
    "    labels = torch.cat(labels, dim=0).numpy()\n",
    "\n",
    "    # KMeans clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(latents)\n",
    "\n",
    "    # t-SNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    latents_2d = tsne.fit_transform(latents)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=latents_2d[:, 0], y=latents_2d[:, 1], hue=clusters, palette='tab10', legend=False)\n",
    "    plt.title('Latent space clustering')\n",
    "    plt.show()\n",
    "\n",
    "    return latents, clusters, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-_dUwbJzrBW"
   },
   "source": [
    "From each cluster, generate an image. Ideally, we expect to get 10 different images from 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "IvtJbOHmzBuQ",
    "outputId": "fff52cde-349f-4d89-ad35-6ad2e8aaea59"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate one sample per cluster center (approximate)\n",
    "latents, clusters, labels = cluster_latent_space(VAE, test_loader, n_clusters=10, device=device)\n",
    "cluster_images = []\n",
    "for c in range(10):\n",
    "    # take mean latent of cluster c\n",
    "    z_c = torch.tensor(latents[clusters == c].mean(axis=0), device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        img = VAE.decode(z_c).view(28, 28).cpu()\n",
    "    cluster_images.append(img)\n",
    "\n",
    "# Plot cluster representatives\n",
    "for idx, img in enumerate(cluster_images):\n",
    "    plot_image_with_label(img, idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1NL1l_uycZH"
   },
   "source": [
    "# Adding a Classifier to VAE\n",
    "\n",
    "This code extends the **Variational Autoencoder (VAE)** by adding a **classification head**. This allows the model to **predict class labels** from the latent space.\n",
    "\n",
    "Add a linear layer that gets encoding of image as input and outputs the image's label. Change the forward function to also returns class logits. You can also add a classify function to returns just the class logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5Ql6SI8u_f7"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class VAEWithClassifier(VariationalAE):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(self.latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        recon, mu, logvar = super().forward(x)\n",
    "        logits = self.classifier(mu)\n",
    "        return recon, mu, logvar, logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8TnQd3AyfHr"
   },
   "source": [
    "Load the pretrained vae weights, then use CrossEntropyLoss to train the model for a few epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NB4bZV1PxZiR",
    "outputId": "fe69236a-0353-4622-84c0-ee9a01188d75"
   },
   "outputs": [],
   "source": [
    "\n",
    "def fine_tune_vae(vae_model, train_loader, test_loader, num_classes, num_epochs=4, learning_rate=1e-3, device=device):\n",
    "    # Add the classification head to the VAE\n",
    "    model = VAEWithClassifier(num_classes).to(device)\n",
    "\n",
    "    # Load the pre-trained VAE weights\n",
    "    model.load_state_dict(vae_model.state_dict(), strict=False)\n",
    "\n",
    "    # Define the loss function\n",
    "    recon_criterion = F.binary_cross_entropy\n",
    "    cls_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for data, labels in train_loader:\n",
    "            data = data.to(device).view(-1, 28*28)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar, logits = model(data)\n",
    "            recon_loss = recon_criterion(recon, data, reduction='sum')\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            cls_loss = cls_criterion(logits, labels)\n",
    "            loss = recon_loss + kl_loss + cls_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}: loss {total_loss/len(train_loader.dataset):.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "fine_tuned_model = fine_tune_vae(VAE, train_loader, test_loader, num_classes=10, num_epochs=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzPEblFnyko_"
   },
   "source": [
    "Evaluate fine tuned model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FH41dcdwyEhW",
    "outputId": "7eaf214d-05a7-436e-aae5-e79404bf814d"
   },
   "outputs": [],
   "source": [
    "\n",
    "fine_tuned_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data = data.to(device).view(-1, 28*28)\n",
    "        labels = labels.to(device)\n",
    "        _, mu, _, logits = fine_tuned_model(data)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Examples in Variational Autoencoders\n",
    "\n",
    "In this experiment, we explore how a small, intentional perturbation to the input image can significantly alter the VAE's output, even though the input still looks visually similar.\n",
    "\n",
    "We use the **Fast Gradient Sign Method (FGSM)** to generate an *adversarial image* that \"fools\" the VAE.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Take a test image.\n",
    "2. Compute the VAEâ€™s reconstruction loss and backpropagate to get gradients w.r.t. the input.\n",
    "3. Generate a perturbed image\n",
    "4. Clamp pixel values to keep them in the valid range \\([0, 1]\\).\n",
    "5. Feed both original and adversarial images through the VAE.\n",
    "6. Compare:\n",
    "   - Original vs Reconstruction\n",
    "   - Adversarial vs Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get an input image from test_loader\n",
    "x, _ = next(iter(test_loader))\n",
    "x = x[0:1].to(device)\n",
    "x.requires_grad = True\n",
    "\n",
    "# Forward pass\n",
    "recon, mu, logvar = VAE(x.view(-1, 28*28))\n",
    "\n",
    "# Compute reconstruction loss\n",
    "loss = F.binary_cross_entropy(recon, x.view(-1, 28*28), reduction='sum')\n",
    "loss.backward()\n",
    "\n",
    "# Generate adversarial example (FGSM)\n",
    "eta = 0.1 * x.grad.sign()\n",
    "x_adv = x + eta\n",
    "x_adv = torch.clamp(x_adv, 0, 1)  # Ensure pixel range is valid\n",
    "\n",
    "# Reconstruct adversarial\n",
    "with torch.no_grad():\n",
    "    recon_adv, _, _ = VAE(x_adv.view(-1, 28*28))\n",
    "\n",
    "plot_image_with_label(x.squeeze(0).cpu(), 0)\n",
    "plot_image_with_label(x_adv.squeeze(0).cpu(), 0)\n",
    "plot_image_with_label(recon_adv.view(28, 28).cpu(), 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Fine-Tuning VAE for MNIST Digit Classification\n",
    "\n",
    "In this experiment, we explore **cross-domain representation learning** by fine-tuning a **VAE originally trained on Fashion-MNIST** to classify digits from the **MNIST** dataset.\n",
    "\n",
    "Although the VAE never saw digits during training, it learned general-purpose visual features (edges, strokes, textures). We now test whether these features can be **repurposed** to classify a completely different kind of visual data.\n",
    "\n",
    "#### Steps:\n",
    "1. The **Fashion-MNIST-trained VAE encoder** is reused as a feature extractor.\n",
    "2. A **classifier head** is added to the encoder.\n",
    "3. The model is fine-tuned on MNIST digit labels.\n",
    "4. few epochs (one) is used to test how quickly it can adapt.\n",
    "\n",
    "Calculate test accuracy on MNIST after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "fine_tuned_model = fine_tune_vae(VAE, train_loader, test_loader, num_classes, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_loader:\n",
    "        data, labels = data.to(device).view(data.shape[0], -1), labels.to(device)\n",
    "        class_logits = fine_tuned_model(data)\n",
    "        _, predicted = torch.max(class_logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
