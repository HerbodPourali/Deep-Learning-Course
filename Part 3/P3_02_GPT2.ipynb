{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='sharif_logo.png' alt=\"SUT logo\" width=150 height=150 align=left class=\"saturate\" >\n",
    "\n",
    "<br>\n",
    "<font face=\"Times New Roman\">\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=7>\n",
    " Deep Learning <br>\n",
    "<font color=2565AE size=5>\n",
    "Computer Engineering Department - Spring 2025  <br>\n",
    "<font color=3C99D size=5>\n",
    "          Homework 3: Practical - GPT2 from Scratch! <br>\n",
    "<font color=696880 size=4>\n",
    "            Designer: Shaygan Adim\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name**:  \n",
    "  \n",
    "**Student Code**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this project, you will implement a scaled-down version of OpenAI's GPT-2 architecture from scratch using PyTorch. You'll train this model on the Snappfood comments with sentiment labels. The goal is to create a generative language model that can produce synthetic Persian comments with controllable sentiment (positive or negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "\n",
    "* Understanding and implementing transformer-based language model architectures  \n",
    "* Learning how to control text generation using special tokens  \n",
    "* Visualizing and analyzing training progress  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "You'll work with a Persian dataset containing Snappfood comments:  \n",
    "\n",
    "* The dataset texts are normalized (No need for any normalizations)\n",
    "* Each comment has a sentiment label (1 for positive, 0 for negative)\n",
    "* The dataset contains text with variations in length and style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and model\n",
    "\n",
    "You should use one of the sota open-source LLM tokenizers. I strongly recommend using Llama 3.3 tokenizer or Gemma-2 tokenizer as they're better than the others in Persian language. (There is no need to implement a tokenizer yourself.)\n",
    "\n",
    "Your model should have the exact srtructure of GPT-2:  \n",
    "  \n",
    "<img src=\"GPT-2.png\" alt=\"\" width=\"600\" height=\"800\">\n",
    "  \n",
    "For the model to be able to smoothly be trained, you should use the config below:\n",
    "\n",
    "* **Embedding Dimension**: 192 (reduced from 768 in original GPT-2)\n",
    "* **Layers**: 3 transformer blocks (reduced from 12 in original GPT-2)\n",
    "* **Attention Heads**: 3 (reduced from 12 in original GPT-2)\n",
    "* **Context Window**: 128 tokens (reduced from 1024 in original GPT-2)\n",
    "\n",
    "Moreover, unlike the original Transformer paper that used fixed sinusoidal position encodings, GPT-2 (and your implementation) should use learnable position embeddings:\n",
    "1. You should create an embedding table of size [n_positions, n_embd] where:\n",
    "\n",
    "    * n_positions is the maximum sequence length (128 in our model)\n",
    "    * n_embd is the embedding dimension (192 in our model)\n",
    "2. For each position in the sequence (0 to sequence_length-1), we look up the corresponding embedding vector.\n",
    "\n",
    "3. These position embeddings are added to the token embeddings before being passed through the transformer blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "* Be aware that you will be questioned about your solution to this assignment in-person. Thus, build a solid understanding through out solving this assignment.\n",
    "* Using ChatGPT and other LLMs are allowed but you should be able to explain every line of your code completely.\n",
    "* You need GPU for this assignment. Use can use Colab or Kaggle for free.\n",
    "* I highly recommend using the exact same hyperparameters and settings provided to match expected results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPIGMAYJWg0Q"
   },
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwwI_YyGWg0T"
   },
   "outputs": [],
   "source": [
    "# Data loading and manipulation\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tokenization utilities\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualization tool\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Runtime utilities\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Typing tool\n",
    "from typing import Dict, Optional, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsdC1ePHWg0U"
   },
   "source": [
    "# Downloading and loading the data\n",
    "\n",
    "In this section we read and load the data from [here](https://www.kaggle.com/datasets/mohammad1ziyar/cleaned-snappfood-persian-sentiment-analysis).\n",
    "\n",
    "You can also see some information about the data in the next cell. In the end, we only want the label and cleaned columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xkchxFa1Wg0U",
    "outputId": "bfcdc14d-20a8-4c7c-ffa2-c45ac268aa24"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Download the dataset using kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"mohammad1ziyar/cleaned-snappfood-persian-sentiment-analysis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "raw_corpus = pd.read_csv(path + \"/cleaned_snappfood.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vg0fAjmAWg0V",
    "outputId": "c755754b-535c-45a4-9f03-706e5917ac4e"
   },
   "outputs": [],
   "source": [
    "raw_corpus.info()\n",
    "raw_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVV8fkElWg0W"
   },
   "outputs": [],
   "source": [
    "raw_corpus = raw_corpus[[\"comment_cleaned\", \"label\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hs2WbUaSWg0W"
   },
   "source": [
    "# Downloading and loading the tokenizer (5 Points)\n",
    "\n",
    "In this section you need to load your tokenizer from hugging face. I recommend [this](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) or [this](https://huggingface.co/google/gemma-2-27b-it).\n",
    "Keep in mind that you might need to login first using your hugging face access token and also sign an agreement thing in model's page to be able to access the model and it's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4d2KgUJWg0W"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a tokenizer (login if needed)\n",
    "\n",
    "# login()  # Uncomment and provide your token if required\n",
    "# Choose a tokenizer with good Persian coverage\n",
    "model_checkpoint = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Use padding side left for autoregressive training\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, add two special tokens to the vocabulary of the tokenizer indicating positivity or negativity of a comment. We will add these tokens manually as the first token of each comment so model will understand the difference between positive and negative comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEGzba_kWg0X",
    "outputId": "ac50a9ef-aca7-4586-e0fd-597ed0f7ed12"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Add custom special tokens to the tokenizer\n",
    "\n",
    "special_tokens = {\"additional_special_tokens\": [\"<|positive|>\", \"<|negative|>\"]}\n",
    "tokenizer.add_special_tokens(special_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Pz_f5L-Wg0X"
   },
   "source": [
    "## Dataset and Dataloader (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom Dataset class for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvXgNmXNWg0X"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.comments = dataframe['comment_cleaned'].tolist()\n",
    "        self.labels = dataframe['label'].tolist()\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        sentiment_prefix = \"<|positive|>\" if self.labels[idx] == 1 else \"<|negative|>\"\n",
    "        text = sentiment_prefix + \" \" + self.comments[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train and validation datasets and dataloaders and also split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEKDOw_sWg0X"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the training and validation datasets and dataloaders\n",
    "\n",
    "train_df, val_df = train_test_split(raw_corpus, test_size=0.1, random_state=42, stratify=raw_corpus['label'])\n",
    "\n",
    "train_dataset = CommentDataset(train_df, tokenizer)\n",
    "val_dataset = CommentDataset(val_df, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhn8d4ceWg0Y"
   },
   "source": [
    "## Model implementation (35 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you should implement the model architecture completely from scratch. No pre-defined torch or other libraries tools are allowed. (Even for the attention mechanism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "145IO1TdWg0Y"
   },
   "outputs": [],
   "source": [
    "class GPT2Config:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        n_positions,  \n",
    "        n_embd,\n",
    "        n_layer,\n",
    "        n_head,\n",
    "        n_inner=None,\n",
    "        activation_function=\"gelu\",\n",
    "        resid_pdrop=0.1,\n",
    "        embd_pdrop=0.1,\n",
    "        attn_pdrop=0.1,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        initializer_range=0.02,\n",
    "        bos_token_id=None,\n",
    "        eos_token_id=None,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_positions = n_positions\n",
    "        self.n_embd = n_embd\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_inner = 4 * n_embd if n_inner is None else n_inner\n",
    "        self.activation_function = activation_function\n",
    "        self.resid_pdrop = resid_pdrop\n",
    "        self.embd_pdrop = embd_pdrop\n",
    "        self.attn_pdrop = attn_pdrop\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.initializer_range = initializer_range\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ipPrEapWg0Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.attn_pdrop\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_drop = nn.Dropout(self.dropout)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.n_positions, config.n_positions)).view(1, 1, config.n_positions, config.n_positions))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(C, dim=2)\n",
    "\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / math.sqrt(k.size(-1))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_drop(self.c_proj(y))\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7YfYPW2Wg0Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, config.n_inner)\n",
    "        self.c_proj = nn.Linear(config.n_inner, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.c_fc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPEgMD4zWg0Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwEBuKSwWg0Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            'wte': nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            'wpe': nn.Embedding(config.n_positions, config.n_embd),\n",
    "            'drop': nn.Dropout(config.embd_pdrop),\n",
    "            'h': nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            'ln_f': nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon),\n",
    "        })\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # tie weights\n",
    "        self.lm_head.weight = self.transformer['wte'].weight\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None, labels: torch.Tensor = None):\n",
    "        B, T = input_ids.size()\n",
    "        device = input_ids.device\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "        tok_emb = self.transformer['wte'](input_ids)\n",
    "        pos_emb = self.transformer['wpe'](pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.transformer['drop'](x)\n",
    "\n",
    "        for block in self.transformer['h']:\n",
    "            x = block(x)\n",
    "        x = self.transformer['ln_f'](x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # shift for causal LM\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=self.config.ignore_index)\n",
    "            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3SlvokpWg0Z"
   },
   "source": [
    "## Train and evaluation (25 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should implement the train and evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICMNy90uWg0Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model: nn.Module, data_loader: DataLoader, optimizer: torch.optim.Optimizer, \n",
    "               scheduler, device: torch.device, log_interval: int) -> tuple:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    step_nums = []\n",
    "    step_losses = []\n",
    "\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attn_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(input_ids, attn_mask, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if step % log_interval == 0:\n",
    "            step_nums.append(step)\n",
    "            step_losses.append(loss.item())\n",
    "\n",
    "    return total_loss / len(data_loader), step_nums, step_losses\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, data_loader: DataLoader, device: torch.device) -> tuple:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    step_nums = []\n",
    "    step_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attn_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits, loss = model(input_ids, attn_mask, labels)\n",
    "            total_loss += loss.item()\n",
    "            step_nums.append(step)\n",
    "            step_losses.append(loss.item())\n",
    "\n",
    "    return total_loss / len(data_loader), step_nums, step_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhnkOmOpWg0Z",
    "outputId": "17e5568c-41cd-4976-9401-170f4cca83bf"
   },
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_positions=128,\n",
    "    n_embd=192,\n",
    "    n_layer=3,\n",
    "    n_head=3,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GPT2(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model for 3 to 5 epochs. It's recommended to use a suitable learning rate scheduler (For example, cosine). Also save training and validation loss periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATyPSWYWWg0Z",
    "outputId": "3b5b5468-c28d-43b0-c343-0488a9e8c954"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Training loop\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5 * len(train_loader))\n",
    "\n",
    "num_epochs = 3\n",
    "log_interval = 10\n",
    "\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "train_step_nums = []\n",
    "train_step_losses = []\n",
    "val_step_nums = []\n",
    "val_step_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, tr_steps, tr_losses = train_epoch(model, train_loader, optimizer, scheduler, device, log_interval)\n",
    "    val_loss, v_steps, v_losses = evaluate(model, val_loader, device)\n",
    "\n",
    "    epoch_train_losses.append(train_loss)\n",
    "    epoch_val_losses.append(val_loss)\n",
    "\n",
    "    train_step_nums.extend([s + epoch * len(train_loader) for s in tr_steps])\n",
    "    train_step_losses.extend(tr_losses)\n",
    "    val_step_nums.extend([s + epoch * len(val_loader) for s in v_steps])\n",
    "    val_step_losses.extend(v_losses)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss in each epoch and also in each steps you saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "id": "QoHYyx9aYXS_",
    "outputId": "5370cf81-bc91-4572-873f-de902954229f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the training and validation results\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# epoch losses\n",
    "axes[0].plot(epoch_train_losses, label='Train')\n",
    "axes[0].plot(epoch_val_losses, label='Val')\n",
    "axes[0].set_title('Epoch Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# train step losses\n",
    "axes[1].plot(train_step_nums, train_step_losses, label='Train step')\n",
    "axes[1].set_title('Train Step Loss')\n",
    "axes[1].grid(True)\n",
    "\n",
    "# val step losses\n",
    "axes[2].plot(val_step_nums, val_step_losses, label='Val step')\n",
    "axes[2].set_title('Val Step Loss')\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference (15 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function below to generate comments (positive or negative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_comment(model: nn.Module, tokenizer: Any, sentiment: str, max_length: int = 50) -> str:\n",
    "    sentiment_token = \"<|positive|>\" if sentiment == 'positive' else \"<|negative|>\"\n",
    "    prompt = sentiment_token\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(next(model.parameters()).device)\n",
    "\n",
    "    model.eval()\n",
    "    generated = input_ids\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            logits, _ = model(generated)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "    text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 10 positive and 10 negative comments and evaluate your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2e5sQvyWg0a",
    "outputId": "b419c61a-79ec-4260-cce4-9e0caceee4c4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Demonstrate the model's sentiment-controlled text generation\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for _ in range(10):\n",
    "    pos_comment = generate_comment(model, tokenizer, 'positive')\n",
    "    neg_comment = generate_comment(model, tokenizer, 'negative')\n",
    "    print(f\"POS: {pos_comment}\")\n",
    "    print(f\"NEG: {neg_comment}\")\n",
    "    print('-'*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Time Hyperparameters (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with these parameters for the best results:\n",
    "  \n",
    "temperature, top_k, top_p\n",
    "\n",
    "Briefly report what you saw and try to explain why is it happening. What is the effect of each one?\n",
    "\n",
    "**Your Report**:  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
