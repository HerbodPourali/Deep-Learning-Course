{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yKp1NLIIia1"
   },
   "source": [
    "<img src='sharif_logo.png' alt=\"SUT logo\" width=150 height=150 align=left class=\"saturate\" >\n",
    "\n",
    "<br>\n",
    "<font face=\"Times New Roman\">\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=7>\n",
    " Deep Learning <br>\n",
    "<font color=2565AE size=5>\n",
    "Computer Engineering Department - Spring 2025  <br>\n",
    "<font color=3C99D size=5>\n",
    "          Homework 2:  <br>\n",
    "<font color=696880 size=4>\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Overview\n",
    "\n",
    "In this assignment, you will explore inference scaling techniques in large language models (LLMs) and evaluate their performance using the Math Benchmark. Throughout the notebook, you will learn about several inference methods, including:\n",
    "\n",
    "- **Chain-of-Thought (CoT):** A method where the model generates intermediate reasoning steps before providing the final answer.\n",
    "- **Best-of-n Sampling:** An approach that generates multiple candidate responses and selects the best one based on a scoring function.\n",
    "- **Beam Search:** A technique that expands several possible sequences simultaneously, choosing the most promising ones based on probability.\n",
    "- **Self-Refinement:** An iterative process where the model revises its output to improve accuracy and coherence.\n",
    "\n",
    "The **Math Benchmark** is a suite of challenging mathematical problems designed to test the reasoning and problem-solving capabilities of LLMs. The benchmark includes a variety of questions ranging from basic arithmetic and algebra to more advanced topics such as geometry and calculus. For example, you might be asked to solve an equation like `2x + 5 = 15` or compute the derivative of a function, tasks that assess the model's ability to handle both straightforward and complex mathematical queries.\n",
    "\n",
    "By the end of this assignment, you will have:\n",
    "- Gained a deeper understanding of inference time scaling methods in LLMs.\n",
    "- Compared the effectiveness of different inference techniques using a rigorous math evaluation framework.\n",
    "\n",
    "Let's dive into the notebook and begin exploring how these methods perform on a challenging set of math problems!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vLLM: Accelerated Inference Engine for LLMs\n",
    "\n",
    "vLLM is an open-source project designed to optimize the loading and inference of large language models. By leveraging advanced memory management techniques and dynamic batching, vLLM significantly speeds up the inference process, making it easier to deploy and experiment with LLMs even on hardware with limited resources\n",
    "So we use vLLM to get results faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyG3Ng0jIl7m"
   },
   "outputs": [],
   "source": [
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWvzLP5FIuc4"
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DY4FWoryOiry"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade numpy\n",
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivC7LIgkLUvR"
   },
   "source": [
    "\n",
    "This command launches a vLLM inference server with:\n",
    "- Model: `DeepSeek-R1-Distill-Qwen-1.5B`\n",
    "- Port: `8000` (default API endpoint)\n",
    "- Precision: `half` (FP16) for memory efficiency\n",
    "- Max context length: `3192` tokens\n",
    "\n",
    "**Note:**  \n",
    "\ud83d\udd39 Ensure you're using a GPU runtime (T4 or better) in Colab  \n",
    "\ud83d\udd39 Only run the next cell if this one executes successfully \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HNmCEaGIzEe"
   },
   "outputs": [],
   "source": [
    "!vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"   --port 8000   --dtype=half   --max-model-len 3192"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFPitveIL8PT"
   },
   "source": [
    "* this cell lunches model in background using vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZSaM7q5LrC0",
    "outputId": "46867f87-5f17-462b-e769-470532c66399"
   },
   "outputs": [],
   "source": [
    "!nohup vllm serve \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" --port 8000 --dtype=half --max-model-len 5192 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0yGjrAiGXWs"
   },
   "source": [
    "## LLM Query Function\n",
    "\n",
    "* This Python function sends prompts to a locally-hosted LLM API and returns the generated response\n",
    "* you can change max_tokens and temperature as you want\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r-QntB7EF4mb"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_llm_response(prompt):\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "\n",
    "        ],\n",
    "    \"max_tokens\": 500,\n",
    "    \"temperature\": 0.6\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzqLI_OPMNOD"
   },
   "source": [
    "# Test response generation\n",
    "- testing model with some Math benchmark quesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PG8o90EML-4",
    "outputId": "5215a402-c5bc-4f86-b059-a5c30228dbb9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Simple generation of sample math questions\n",
    "print(\"Q1:\", question1)\n",
    "resp1 = get_llm_response(question1)\n",
    "print(resp1, \"\n",
    "\")\n",
    "\n",
    "print(\"Q2:\", question2)\n",
    "resp2 = get_llm_response(question2)\n",
    "print(resp2, \"\n",
    "\")\n",
    "\n",
    "print(\"Q3:\", question3)\n",
    "resp3 = get_llm_response(question3)\n",
    "print(resp3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Benchmark Evaluation\n",
    "\n",
    "This cell is dedicated to evaluating the performance of inference scaling methods on the Math Benchmark dataset. The process works as follows:\n",
    "\n",
    "- **Dataset Loading:** It loads the MATH-500 dataset, which contains a set of challenging math problems along with their correct solutions.\n",
    "- **Answer Extraction:** The `extract_answer` function is used to parse and extract the final answer from the generated responses. This function specifically looks for a LaTeX-style format (using `\\boxed{...}`) to reliably pinpoint the answer.\n",
    "- **Normalization and Comparison:** Before comparing, both the predicted answer and the ground truth are normalized using several functions. These functions handle different mathematical expressions, such as fractions, matrices, and algebraic expressions, ensuring that the comparison is fair and accurate regardless of formatting differences.\n",
    "- **Evaluation Loop:** For each problem:\n",
    "  - The ground truth answer is extracted from the provided solution.\n",
    "  - A response is generated by the LLM using a designated function.\n",
    "  - The predicted answer is then extracted and compared against the ground truth.\n",
    "  - The results for each problem, including whether the predicted answer is correct, are saved for later analysis.\n",
    "- **Results Analysis:** After processing all problems, the cell aggregates the results and prints a summary, including the total number of problems evaluated, the number of correct answers, and the overall accuracy.\n",
    "\n",
    "This evaluation method ensures that the output of each inference technique (such as Chain-of-Thought, Best-of-n, Beam Search, and Self-Refinement) is consistently measured against the Math Benchmark, without altering the original answers or evaluation logic.\n",
    "\n",
    "**Note:**  \n",
    "\n",
    "\ud83d\udd39 you don't need to modify this cell. Only rewrite the evaluation function portion then\n",
    "\n",
    "\ud83d\udd39 you need to run this cell before evaluating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jgnyeyBxE2Ci"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from typing import Dict, Optional, Union\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Load the MATH-500 dataset\n",
    "def load_math500_dataset():\n",
    "    dataset = load_dataset(\"HuggingFaceH4/MATH-500\")[\"test\"]\n",
    "    return dataset\n",
    "\n",
    "# Extract the last boxed answer from text\n",
    "def extract_answer(response: str) -> Optional[str]:\n",
    "    if not response:\n",
    "        return None\n",
    "    start_idx = response.rfind('\\\\boxed{')\n",
    "    if start_idx == -1:\n",
    "        return None\n",
    "    brace_count = 1\n",
    "    pos = start_idx + 7  # length of '\\boxed{'\n",
    "    while pos < len(response) and brace_count > 0:\n",
    "        if response[pos] == '{':\n",
    "            brace_count += 1\n",
    "        elif response[pos] == '}':\n",
    "            brace_count -= 1\n",
    "        pos += 1\n",
    "    if brace_count == 0:\n",
    "        answer = response[start_idx + 7:pos - 1]\n",
    "        return answer.strip()\n",
    "    return None\n",
    "\n",
    "# Normalization and comparison functions (unchanged from original)\n",
    "def normalize_number(num_str: str) -> str:\n",
    "    try:\n",
    "        cleaned = re.sub(r'[,\\$\\\\]|\\s*(?:cm|m|kg|ft|in|lb|oz|ml|L)$|\\s*\\\\text{[^}]+}', '', num_str).strip()\n",
    "        if cleaned.startswith('.'):\n",
    "            cleaned = '0' + cleaned\n",
    "        num = float(cleaned)\n",
    "        if abs(num) < 1 and '.' in cleaned:\n",
    "            decimal_places = len(cleaned.split('.')[1])\n",
    "            format_str = f\"{{:.{decimal_places}f}}\"\n",
    "            result = format_str.format(num)\n",
    "        else:\n",
    "            result = str(num)\n",
    "        return result\n",
    "    except:\n",
    "        return num_str\n",
    "\n",
    "def numerically_equal(str1: str, str2: str) -> bool:\n",
    "    try:\n",
    "        return abs(float(str1) - float(str2)) < 1e-10\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def normalize_fraction(fraction_str: str) -> str:\n",
    "    try:\n",
    "        fraction_str = fraction_str.replace('\\\\dfrac', '\\\\frac')\n",
    "        fraction_str = ''.join(fraction_str.split())\n",
    "        fraction_str = re.sub(r'\\s*\\\\text{[^}]+}', '', fraction_str)\n",
    "        mixed_brace = re.match(r'^\\\\frac(\\d+)\\{(\\d+)\\}$', fraction_str)\n",
    "        if mixed_brace:\n",
    "            num, den = mixed_brace.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
    "        no_braces = re.match(r'^\\\\frac(\\d+)(\\d+)$', fraction_str)\n",
    "        if no_braces:\n",
    "            num, den = no_braces.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
    "        if '/' in fraction_str and not any(c in fraction_str for c in '\\\\{}'):\n",
    "            num, den = fraction_str.split('/')\n",
    "            return f\"\\\\frac{{{num.strip()}}}{{{den.strip()}}}\"\n",
    "        standard = re.match(r'^\\\\frac\\{([^{}]+)\\}\\{([^{}]+)\\}$', fraction_str)\n",
    "        if standard:\n",
    "            num, den = standard.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
    "    except:\n",
    "        return fraction_str\n",
    "\n",
    "def normalize_matrix_entry(entry: str) -> str:\n",
    "    entry = ''.join(entry.split())\n",
    "    if '/' in entry and not any(c in entry for c in '\\\\{}'):\n",
    "        if entry.startswith('-'):\n",
    "            num, den = entry[1:].split('/')\n",
    "            return f\"-{num.strip()}/{den.strip()}\"\n",
    "        else:\n",
    "            num, den = entry.split('/')\n",
    "            return f\"{num.strip()}/{den.strip()}\"\n",
    "    entry = entry.replace('\\\\dfrac', '\\\\frac')\n",
    "    frac_match = re.match(r'^(-)?\\\\frac\\{(\\d+)\\}\\{(\\d+)\\}$', entry)\n",
    "    if frac_match:\n",
    "        sign, num, den = frac_match.groups()\n",
    "        sign = sign if sign else ''\n",
    "        return f\"{sign}{num}/{den}\"\n",
    "    return entry\n",
    "\n",
    "def normalize_matrix(matrix_str: str) -> str:\n",
    "    try:\n",
    "        matrix_str = ''.join(matrix_str.split())\n",
    "        match = re.match(r'^\\\\begin\\{pmatrix\\}(.*?)\\\\end\\{pmatrix\\}$', matrix_str)\n",
    "        if not match:\n",
    "            return matrix_str\n",
    "        content = match.group(1)\n",
    "        rows = content.split('\\\\\\\\')\n",
    "        normalized_rows = []\n",
    "        for row in rows:\n",
    "            if '&' in row:\n",
    "                entries = [normalize_matrix_entry(entry) for entry in row.split('&')]\n",
    "            else:\n",
    "                entries = [normalize_matrix_entry(row)]\n",
    "            normalized_rows.append('&'.join(entries))\n",
    "        result = \"\\\\begin{pmatrix}\" + \"\\\\\\\\\".join(normalized_rows) + \"\\\\end{pmatrix}\"\n",
    "        return result\n",
    "    except:\n",
    "        return matrix_str\n",
    "\n",
    "def normalize_algebraic_expression(expr: str) -> str:\n",
    "    try:\n",
    "        expr = ''.join(expr.split())\n",
    "        monomial_match = re.match(r'^(-?\\d*\\.?\\d*)?([a-zA-Z])(?:\\^(-?\\d+))?$', expr)\n",
    "        if monomial_match:\n",
    "            coeff, var, exp = monomial_match.groups()\n",
    "            coeff = coeff if coeff and coeff not in ['+', '-'] else ('1' if not coeff else '-1')\n",
    "            exp = exp if exp else '1'\n",
    "            if coeff == '1' and exp == '1':\n",
    "                return var\n",
    "            elif coeff == '1':\n",
    "                return f\"{var}^{exp}\"\n",
    "            elif coeff == '-1' and exp == '1':\n",
    "                return f\"-{var}\"\n",
    "            elif coeff == '-1':\n",
    "                return f\"-{var}^{exp}\"\n",
    "            elif exp == '1':\n",
    "                return f\"{coeff}{var}\"\n",
    "            else:\n",
    "                return f\"{coeff}{var}^{exp}\"\n",
    "        pi_term_match = re.match(r'^(-?\\d*\\.?\\d*)\\\\?pi$', expr)\n",
    "        if pi_term_match:\n",
    "            coeff = pi_term_match.group(1)\n",
    "            if not coeff or coeff == '-':\n",
    "                coeff = '-1' if coeff == '-' else '1'\n",
    "            return f\"{coeff}\\\\pi\"\n",
    "        frac_pi_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}\\\\?pi$', expr)\n",
    "        if frac_pi_match:\n",
    "            num, den = frac_pi_match.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\\\\pi\"\n",
    "        frac_match = re.match(r'^\\\\frac{([^{}]+)}{([^{}]+)}$', expr)\n",
    "        if frac_match:\n",
    "            num, den = frac_match.groups()\n",
    "            return f\"\\\\frac{{{num}}}{{{den}}}\"\n",
    "    except:\n",
    "        return expr.lower()\n",
    "\n",
    "def normalize_interval_bound(bound: str) -> str:\n",
    "    if '\\\\infty' in bound:\n",
    "        sign = '-' if bound.startswith('-') else ''\n",
    "        return f\"{sign}\\\\infty\"\n",
    "    return normalize_answer(bound) or bound\n",
    "\n",
    "def normalize_interval(interval_str: str) -> str:\n",
    "    try:\n",
    "        interval_str = ''.join(interval_str.split())\n",
    "        match = re.match(r'^\\\\left?([\\[\\(])(.*?),(.*?)\\\\right?([\\]\\)])$', interval_str)\n",
    "        if not match:\n",
    "            match = re.match(r'^([\\[\\(])(.*?),(.*?)([\\]\\)])$', interval_str)\n",
    "            if not match:\n",
    "                return interval_str\n",
    "        left_bracket, left_bound, right_bound, right_bracket = match.groups()\n",
    "        norm_left = normalize_interval_bound(left_bound)\n",
    "        norm_right = normalize_interval_bound(right_bound)\n",
    "        return f\"\\\\left{left_bracket}{norm_left},{norm_right}\\\\right{right_bracket}\"\n",
    "    except:\n",
    "        return interval_str\n",
    "\n",
    "def normalize_ordered_tuple(tuple_str: str) -> str:\n",
    "    try:\n",
    "        tuple_str = tuple_str.replace('\\\\dfrac', '\\\\frac')\n",
    "        tuple_str = tuple_str.replace('\\\\left', '').replace('\\\\right', '')\n",
    "        tuple_str = re.sub(r'\\\\?\\s+', '', tuple_str)\n",
    "        inner = tuple_str.strip('()')\n",
    "        parts = inner.split(',')\n",
    "        normalized_parts = [normalize_answer(part.strip()) for part in parts if normalize_answer(part.strip())]\n",
    "        return f\"({','.join(normalized_parts)})\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def normalize_answer(answer: str) -> str:\n",
    "    if answer is None:\n",
    "        return \"\"\n",
    "    answer = re.sub(r'\\\\text{[^}]+(?:inches|feet|meters|cm|m|kg|ft|in|lb|oz|ml|L|per|second|minute|hour)[^}]*}', '', answer)\n",
    "    answer = re.sub(r'(?<!\\\\)\\s+', '', answer)\n",
    "    ordered_pair_match = re.match(r'^(?:\\\\left)?\\((.*?)(?:\\\\right)?\\)$', answer)\n",
    "    if ordered_pair_match:\n",
    "        content = ordered_pair_match.group(1)\n",
    "        parts = content.split(',')\n",
    "        normalized_parts = [normalize_answer(part) for part in parts if normalize_answer(part)]\n",
    "        return f\"({','.join(normalized_parts)})\"\n",
    "    answer = ''.join(answer.split())\n",
    "    if not answer:\n",
    "        return None\n",
    "    pm_match = re.match(r'^(.*?)(?:\\\\pm|-)(.*?)$', answer)\n",
    "    if pm_match:\n",
    "        left, right = pm_match.groups()\n",
    "        norm_left = normalize_answer(left) if left else \"\"\n",
    "        norm_right = normalize_answer(right) if right else \"\"\n",
    "        if norm_left or norm_right:\n",
    "            return f\"{norm_left}\\\\pm{norm_right}\"\n",
    "    trig_match = re.match(r'^\\\\(?:sin|cos|tan|cot|sec|csc)\\s*([a-zA-Z])$', answer)\n",
    "    if trig_match:\n",
    "        variable = trig_match.group(1)\n",
    "        func_name = re.match(r'^\\\\(.*?)(?:\\s|$)', answer).group(1)\n",
    "        return f\"\\\\{func_name}{variable}\"\n",
    "    text_match = re.match(r'^(?:\\\\text{)?([A-Za-z]+)(?:})?$', answer)\n",
    "    if text_match:\n",
    "        return text_match.group(1).lower()\n",
    "    if (answer.startswith('\\\\left[') or answer.startswith('\\\\left(') or\n",
    "        answer.startswith('[') or answer.startswith('(')) and \\\n",
    "       (answer.endswith('\\\\right]') or answer.endswith('\\\\right)') or\n",
    "        answer.endswith(']') or answer.endswith(')')):\n",
    "        return normalize_interval(answer)\n",
    "    if answer.startswith('\\\\begin{pmatrix}') and answer.endswith('\\\\end{pmatrix}'):\n",
    "        return normalize_matrix(answer)\n",
    "    answer = answer.replace('\\\\dfrac', '\\\\frac')\n",
    "    if '\\\\frac' in answer or '/' in answer:\n",
    "        return normalize_fraction(answer)\n",
    "    neg_sqrt_match = re.match(r'^-\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
    "    if neg_sqrt_match:\n",
    "        num = neg_sqrt_match.group(1)\n",
    "        return f\"-\\\\sqrt{{{num}}}\"\n",
    "    sqrt_match = re.match(r'^(\\d*)?\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
    "    if sqrt_match:\n",
    "        coeff, num = sqrt_match.groups()\n",
    "        coeff = coeff if coeff else '1'\n",
    "        return f\"\\\\sqrt{{{num}}}\" if coeff == '1' else f\"{coeff}\\\\sqrt{{{num}}}\"\n",
    "    sqrt_with_coeff_match = re.match(r'^(\\d+)\\\\sqrt\\{?(\\d+)\\}?$', answer)\n",
    "    if sqrt_with_coeff_match:\n",
    "        coeff, num = sqrt_with_coeff_match.groups()\n",
    "        return f\"{coeff}\\\\sqrt{{{num}}}\"\n",
    "    base_match = re.match(r'^(\\d+)(?:_\\{?(\\d+)\\}?|_(\\d+))$', answer)\n",
    "    if base_match:\n",
    "        number, base1, base2 = base_match.groups()\n",
    "        base = base1 if base1 else base2\n",
    "        return f\"{number}_{base}\"\n",
    "    percent_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*\\\\?%$', answer)\n",
    "    if percent_match:\n",
    "        return normalize_number(percent_match.group(1))\n",
    "    unit_match = re.match(r'^(\\d+(?:\\.\\d*)?)\\s*(?:(?:\\\\[,\\s])|,)?\\s*(?:\\\\\\\\)?(?:\\\\text{(\\w+)}|\\\\?(?:cm|m|kg|ft|in|lb|oz|ml|L))$', answer)\n",
    "    if unit_match:\n",
    "        return normalize_number(unit_match.group(1))\n",
    "    currency_match = re.match(r'^\\\\?\\$?([\\d,]+\\.?\\d*)$', answer)\n",
    "    if currency_match:\n",
    "        return normalize_number(currency_match.group(1))\n",
    "    if re.match(r'^-?[\\d,]+$', answer):\n",
    "        return normalize_number(answer)\n",
    "    unit_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:\\\\(?:mbox|text|hbox|displaystyle)\\{[^}]+\\})?(?:\\^?\\d)?$', answer)\n",
    "    if unit_match:\n",
    "        return normalize_number(unit_match.group(1))\n",
    "    mc_match = re.match(r'^\\\\text{\\(?([A-Za-z])\\)?}$|^\\(?([A-Za-z])\\)?$', answer)\n",
    "    if mc_match:\n",
    "        return (mc_match.group(1) or mc_match.group(2)).lower()\n",
    "    degree_match = re.match(r'^(-?[\\d,]+(?:\\.\\d*)?)\\s*(?:(?:\\^?\\\\circ)|(?:{\\\\circ})|(?:\u00b0))?$', answer)\n",
    "    if degree_match:\n",
    "        return normalize_number(degree_match.group(1))\n",
    "    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n",
    "    try:\n",
    "        return normalize_algebraic_expression(answer)\n",
    "    except:\n",
    "        pass\n",
    "    answer = answer.replace('\\\\left', '').replace('\\\\right', '')\n",
    "    answer = answer.replace('\\\\(', '(').replace('\\\\)', ')')\n",
    "    answer = answer.replace('\\\\[', '[').replace('\\\\]', ']')\n",
    "    answer = answer.replace('\\\\{', '{').replace('\\\\}', '}')\n",
    "    answer = re.sub(r'\\\\sqrt\\{?(\\d+)\\}?', r'\\\\sqrt{\\1}', answer)\n",
    "    answer = re.sub(r'\\\\sqrt{([^{}]+)}', r'\\\\sqrt\\1', answer)\n",
    "    if re.match(r'^\\d+\\\\%$', answer) or re.match(r'^\\d+$', answer):\n",
    "        answer = re.sub(r'\\\\%$', '', answer)\n",
    "    answer = re.sub(r'\\\\text{([^{}]+)}', r'\\1', answer)\n",
    "    while len(answer) >= 2 and answer[0] == '{' and answer[-1] == '}':\n",
    "        if '\\\\frac' in answer:\n",
    "            break\n",
    "        answer = answer[1:-1]\n",
    "    return answer.lower() if answer else None\n",
    "\n",
    "def compare_answers(correct_answer: str, predicted_answer: Optional[str]) -> bool:\n",
    "    if predicted_answer is None:\n",
    "        return False\n",
    "    if numerically_equal(correct_answer, predicted_answer):\n",
    "        return True\n",
    "    normalized_correct = normalize_answer(correct_answer)\n",
    "    normalized_predicted = normalize_answer(predicted_answer)\n",
    "    if not normalized_correct or not normalized_predicted:\n",
    "        return False\n",
    "    if normalized_correct == \"\" and normalized_predicted == \"\":\n",
    "        return False\n",
    "    if ('\\\\left[' in normalized_correct or '\\\\left(' in normalized_correct) and \\\n",
    "       ('\\\\left[' in normalized_predicted or '\\\\left(' in normalized_predicted):\n",
    "        return normalized_correct == normalized_predicted\n",
    "    return normalized_correct == normalized_predicted\n",
    "\n",
    "# Load existing results\n",
    "def load_existing_results(filename: str) -> list[Dict]:\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "# Save a single result\n",
    "def save_result(filename: str, result: Dict):\n",
    "    results = load_existing_results(filename)\n",
    "    results.append(result)\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# Analyze and print results\n",
    "def analyze_results(results: list[Dict]):\n",
    "    total = len(results)\n",
    "    correct = sum(1 for r in results if r['is_correct'])\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(\"\\n=== Results Summary ===\")\n",
    "    print(f\"Total problems: {total}\")\n",
    "    print(f\"Correct answers: {correct}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    print(\"\\n=== Incorrect Problems ===\")\n",
    "    for r in results:\n",
    "        if not r['is_correct']:\n",
    "            print(f\"Problem {r['index']}:\")\n",
    "            print(f\"Expected: {r['correct_answer']}\")\n",
    "            print(f\"Predicted: {r['predicted_answer']}\")\n",
    "            print(\"---\")\n",
    "\n",
    "# Main evaluation function\n",
    "def evaluate():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"evaluation_results_math500_deepseek.json\"\n",
    "    dataset = load_math500_dataset()\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    processed_indexes = {result['index'] for result in existing_results}\n",
    "    cnt = 0\n",
    "    t=0\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed_indexes:\n",
    "            continue\n",
    "        t += 1\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])  # Extract from 'solution', not 'answer'\n",
    "        response = get_llm_response(problem_text)\n",
    "        predicted_answer = extract_answer(response)\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "        save_result(results_file, result)\n",
    "        if is_correct:\n",
    "          cnt += 1\n",
    "        print(f\"cnt :  {cnt} idx: {t}\")\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNUWF2U_HC8g"
   },
   "source": [
    "# Customizable CoT Prompt Template\n",
    "* modify cot prompt then evaluate on math benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# final answer should be in this format: (because of extract_answer function you can change it if you want)\n",
    "#\\[\n",
    "#\boxed{your_answer_here}\n",
    "#\\]\n",
    "\n",
    "COT_PROMPT = SYSTEM_PROMPT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* generate response with cot prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "58w5yeo9GrP2"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_COT_response(problem):\n",
    "    prompt = COT_PROMPT + \"\\n\" + problem\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "\n",
    "        ],\n",
    "    \"max_tokens\": 1900,\n",
    "    \"temperature\": 0.3\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()['choices'][0]['message']['content'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Una6ucDAHb2_"
   },
   "source": [
    "# Evaluate CoT\n",
    "* modify response generation part to evalute this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Na6kksqZHbOq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_cot():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"evaluation_results_math500_deepseek_cot.json\"\n",
    "    dataset = load_math500_dataset()\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    processed_indexes = {result['index'] for result in existing_results}\n",
    "    cnt = 0\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed_indexes:\n",
    "            continue\n",
    "        if idx >= 30:\n",
    "            break\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "        response = get_COT_response(problem_text)\n",
    "        predicted_answer = extract_answer(response)\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "        save_result(results_file, result)\n",
    "        if is_correct:\n",
    "            cnt += 1\n",
    "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJbLqPXYOLvm",
    "outputId": "c8d30658-7594-4745-e4c2-3595c9a0acfb"
   },
   "outputs": [],
   "source": [
    "evaluate_cot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best-of-N \n",
    "\n",
    "The Best-of-N approach generates several candidate responses for a problem and then selects the one with the highest average token log-likelihood. This ensures that the final answer, formatted within the `\\boxed{}` command, is not only correct in presentation but also statistically the most reliable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1NylbVYkT1kx"
   },
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = You are solving mathematics problems.\n",
    "\n",
    "Please think step by step.\n",
    "\n",
    "Important: Always end your solution with the final answer in this format:\n",
    "\n",
    "\\[\n",
    "\\boxed{your_answer_here}\n",
    "\\]\n",
    "\n",
    "The entire answer should be contained completely within the \\boxed{} command.\n",
    "\n",
    "\n",
    "def best_of_n_response(problem, N=5):\n",
    "    prompt = SYSTEM_PROMPT + \"\n",
    "\" + problem\n",
    "    responses = []\n",
    "    for _ in range(N):\n",
    "        resp = get_llm_response(prompt)\n",
    "        ans = extract_answer(resp)\n",
    "        responses.append((resp, ans, 0.0))\n",
    "\n",
    "    # Group by answer and pick first occurrence\n",
    "    best = None\n",
    "    for resp, ans, score in responses:\n",
    "        if ans is None:\n",
    "            continue\n",
    "        best = (resp, ans, score)\n",
    "        break\n",
    "    if best is None:\n",
    "        best = responses[0] if responses else (\"\", None, 0.0)\n",
    "    return best[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate best of n\n",
    "\n",
    "* modify response generation part to evalute this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jyCkE9ToOHNO"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_best_of_n():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"evaluation_results_math500_deepseek_best_of_n.json\"\n",
    "    dataset = load_math500_dataset()\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    processed_indexes = {result['index'] for result in existing_results}\n",
    "    cnt = 0\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed_indexes:\n",
    "            continue\n",
    "        if idx >= 30:\n",
    "            break\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "        response = best_of_n_response(problem_text, N=3)\n",
    "        predicted_answer = response\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "        save_result(results_file, result)\n",
    "        if is_correct:\n",
    "            cnt += 1\n",
    "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "W_6D_Pb8V3tr",
    "outputId": "e87ea8e5-1b4e-40ce-d947-ec6142786be9"
   },
   "outputs": [],
   "source": [
    "evaluate_best_of_n()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "This cell implements a beam search strategy for generating candidate reasoning chains. The method generates multiple continuations at each reasoning step, scoring each candidate based on its average token log-likelihood. By retaining and expanding only the top candidates, the approach efficiently searches for the most promising chain-of-thought that leads to the final answer in the required format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "v2ZQr1A0WBD0"
   },
   "outputs": [],
   "source": [
    "\n",
    "def call_qwen_model_raw(prompt, step_num, temperature=0.8):\n",
    "    output_text = get_llm_response(prompt)\n",
    "    token_logprobs = [0.0 for _ in output_text.split()] or [0.0]\n",
    "    avg_token_prob = sum(token_logprobs) / len(token_logprobs)\n",
    "    return output_text, avg_token_prob, len(token_logprobs)\n",
    "\n",
    "\n",
    "class BeamCandidate:\n",
    "    def __init__(self, sequence, cumulative_log_prob, step_scores, finished=False, num_token=0):\n",
    "        self.sequence = sequence\n",
    "        self.cumulative_log_prob = cumulative_log_prob\n",
    "        self.step_scores = step_scores\n",
    "        self.finished = finished\n",
    "        self.num_token = num_token\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"BeamCandidate(score={self.cumulative_log_prob:.3f}, finished={self.finished}, \"\n",
    "                f\"sequence={self.sequence})\")\n",
    "\n",
    "\n",
    "def generate_reasoning_steps(context, step_num, top_k):\n",
    "    candidates = []\n",
    "    for _ in range(top_k):\n",
    "        candidate_prompt = context + f\"\n",
    "Step {step_num}:\"\n",
    "        output_text, avg_token_prob, num_token = call_qwen_model_raw(candidate_prompt, step_num)\n",
    "        finished = \"\\boxed{\" in output_text\n",
    "        candidate_step = candidate_prompt + \"\n",
    "\" + output_text\n",
    "        candidates.append((candidate_step, avg_token_prob, num_token, finished))\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def beam_search(init_problem_prompt, beam_width=3, max_steps=3, top_k=2):\n",
    "    initial_candidate = BeamCandidate(init_problem_prompt, 0.0, [], finished=False)\n",
    "    beams = [initial_candidate]\n",
    "\n",
    "    for step_num in range(1, max_steps + 1):\n",
    "        new_beams = []\n",
    "        for candidate in beams:\n",
    "            if candidate.finished:\n",
    "                new_beams.append(candidate)\n",
    "                continue\n",
    "            step_candidates = generate_reasoning_steps(candidate.sequence, step_num, top_k)\n",
    "            for (step_text, score, num_token, finished) in step_candidates:\n",
    "                new_score = (candidate.cumulative_log_prob * len(candidate.step_scores) + score) / (len(candidate.step_scores) + 1)\n",
    "                new_beams.append(BeamCandidate(step_text, new_score, candidate.step_scores + [score], finished, num_token))\n",
    "        if not new_beams:\n",
    "            break\n",
    "        new_beams.sort(key=lambda c: c.cumulative_log_prob, reverse=True)\n",
    "        beams = new_beams[:beam_width]\n",
    "        if all(beam.finished for beam in beams):\n",
    "            break\n",
    "    finished_beams = [b for b in beams if b.finished] or beams\n",
    "    best_candidate = max(finished_beams, key=lambda b: b.cumulative_log_prob)\n",
    "    return best_candidate\n",
    "\n",
    "\n",
    "def run_qwen_beam_search(problem, beam_width, max_steps, top_k, log_level):\n",
    "    prompt = SYSTEM_PROMPT + \"\n",
    "\" + problem\n",
    "    best_candidate = beam_search(prompt, beam_width=beam_width, max_steps=max_steps, top_k=top_k)\n",
    "    final_answer = extract_answer(best_candidate.sequence)\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate beam search\n",
    "* modify response generation part to evalute this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "eR_TyvGMazTc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_beam_search():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"evaluation_results_math500_deepseek_beam_search.json\"\n",
    "    dataset = load_math500_dataset()\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    processed_indexes = {result['index'] for result in existing_results}\n",
    "    cnt = 0\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed_indexes:\n",
    "            continue\n",
    "        if idx >= 30:\n",
    "            break\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "        response = run_qwen_beam_search(problem_text, beam_width=2, max_steps=2, top_k=2, log_level=0)\n",
    "        predicted_answer = response\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "        save_result(results_file, result)\n",
    "        if is_correct:\n",
    "            cnt += 1\n",
    "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "sxMjVBf6bpiy",
    "outputId": "739ec144-b8e5-4403-feaf-f28e4b2f7eb9"
   },
   "outputs": [],
   "source": [
    "evaluate_beam_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Refinement \n",
    "\n",
    "This approach begins by generating an initial solution using the given prompt. It then iteratively refines this output by providing the model with targeted feedback and asking it to improve its response. The process continues until the feedback indicates that no further refinement is necessary, ensuring that the final answer\u2014properly formatted within the `\\boxed{}` command\u2014is as accurate and well-reasoned as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "fAYp7rCbiJQa"
   },
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = You are solving mathematics problems.\n",
    "\n",
    "Please think step by step.\n",
    "\n",
    "Important: Always end your solution with the final answer in this format:\n",
    "\n",
    "\\[\n",
    "\\boxed{your_answer_here}\n",
    "\\]\n",
    "\n",
    "The entire answer should be contained completely within the \\boxed{} command.\n",
    "\n",
    "\n",
    "def generate_content(prompt):\n",
    "    output_text = get_llm_response(prompt)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "def self_refine(problem, max_iter=2):\n",
    "    prompt = SYSTEM_PROMPT + \"\n",
    "\" + problem\n",
    "    current_output = generate_content(prompt)\n",
    "\n",
    "    for iteration in range(max_iter):\n",
    "        feedback_prompt = (\n",
    "            SYSTEM_PROMPT\n",
    "            + \"\n",
    "Here is a candidate solution. If it is correct, say 'OK'.\n",
    "\"\n",
    "            + problem\n",
    "            + \"\n",
    "Candidate:\n",
    "\"\n",
    "            + current_output\n",
    "        )\n",
    "        feedback = generate_content(feedback_prompt)\n",
    "        if 'OK' in feedback:\n",
    "            break\n",
    "        refine_prompt = (\n",
    "            SYSTEM_PROMPT\n",
    "            + \"\n",
    "Improve the solution based on feedback.\n",
    "Problem:\n",
    "\"\n",
    "            + problem\n",
    "            + \"\n",
    "Current solution:\n",
    "\"\n",
    "            + current_output\n",
    "            + \"\n",
    "Feedback:\n",
    "\"\n",
    "            + feedback\n",
    "        )\n",
    "        current_output = generate_content(refine_prompt)\n",
    "\n",
    "    answer = extract_answer(current_output)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Self-Refinement\n",
    "* modify response generation part to evalute this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ln5I9pg_jN7Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_self_refiner():\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    results_file = \"evaluation_results_math500_deepseek_self_refiner.json\"\n",
    "    dataset = load_math500_dataset()\n",
    "    existing_results = load_existing_results(results_file)\n",
    "    processed_indexes = {result['index'] for result in existing_results}\n",
    "    cnt = 0\n",
    "    for idx, item in enumerate(tqdm(dataset, desc=\"Evaluating problems\")):\n",
    "        if idx in processed_indexes:\n",
    "            continue\n",
    "        if idx >= 30:\n",
    "            break\n",
    "        problem_text = item['problem']\n",
    "        correct_answer = extract_answer(item['solution'])\n",
    "        response = self_refine(problem_text, max_iter=2)\n",
    "        predicted_answer = response\n",
    "        is_correct = compare_answers(correct_answer, predicted_answer)\n",
    "        result = {\n",
    "            \"index\": idx,\n",
    "            \"problem\": problem_text,\n",
    "            \"response\": response,\n",
    "            \"correct_answer\": correct_answer,\n",
    "            \"predicted_answer\": predicted_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        }\n",
    "        save_result(results_file, result)\n",
    "        if is_correct:\n",
    "            cnt += 1\n",
    "        print(f\"corrects :  {cnt} idx: {idx}\")\n",
    "    final_results = load_existing_results(results_file)\n",
    "    analyze_results(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "uNAg9wa9jlSu",
    "outputId": "64033f38-3340-4729-be0e-70dd0d792705"
   },
   "outputs": [],
   "source": [
    "evaluate_self_refiner()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}