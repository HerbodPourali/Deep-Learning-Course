{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation with CLIP\n",
    "\n",
    "### DL Course, Dr. Soleymani\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "---\n",
    "*Full Name:* ...\n",
    "\n",
    "*SID:* ...\n",
    "\n",
    "---\n",
    "We have specified the parts to be completed with `TODO` tags inside the code blocks.\n",
    "\n",
    "**NOTES**: \n",
    "* This notebook is tested with *Google Colab* free runtime and you can used that for testing your code.\n",
    "* Ensure all cells are executable\n",
    "* You can ask your questions on [Quera Class](https://quera.org/course/20754)\n",
    "* Write clear code.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBCqU5PnrsgO"
   },
   "source": [
    "In this notebook, we show that discriminative models have powerful generative capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9f3BPXsrsgQ"
   },
   "source": [
    "## Introduction\n",
    "Discriminative models map inputs to representations, while generative models synthesize data\n",
    "from learned latent spaces. This includes models from GANs to diffusion models.\n",
    "However, these approaches require extensive training on\n",
    "large datasets.\n",
    "\n",
    "\n",
    "We will now demonstrate how discriminative models encode rich generative knowledge that can be used through optimization. Discriminative models excel at mapping images to representations $(f : I \\rightarrow v)$, but how can we reverse this process $(f^{-1} : v \\rightarrow I)$? \n",
    "In other words,\n",
    "> we are given a text $T$ and aim to find an image $I$ that maximizes the score between $T$ and $I$ ($S = \\text{score}(T, I)$). How can we determine $I$?\n",
    "\n",
    "A simple solution is to use gradient ascent, where we compute the gradient of $S$ with respect to $I$ and update $I$ accordingly. Note that we are not updating the model's weights, but rather optimizing the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjols9DOq09s"
   },
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAQMk62Ys_yI",
    "outputId": "73a149db-63f5-4cee-b778-c2d1567c9d88"
   },
   "outputs": [],
   "source": [
    "! pip install open_clip_torch\n",
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdy-fcp0q9zV"
   },
   "source": [
    "## Getting the CLIP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c5j_2oHirsgT"
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4g4O9pxI02LP"
   },
   "outputs": [],
   "source": [
    "class CLIPModel:\n",
    "    \"\"\"\n",
    "    A wrapper class for a CLIP model, handling text and image encoding,\n",
    "    as well as image normalization.\n",
    "\n",
    "    Attributes:\n",
    "        model: The CLIP model used for encoding.\n",
    "        tokenizer: Tokenizer for processing text inputs.\n",
    "        mean (Tensor): Mean values for image normalization.\n",
    "        std (Tensor): Standard deviation values for image normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, preprocess, device='cuda'):\n",
    "        \"\"\"\n",
    "        Initializes the CLIPModel with the given model, tokenizer, and preprocessing pipeline.\n",
    "\n",
    "        Args:\n",
    "            model: The CLIP model instance.\n",
    "            tokenizer: Tokenizer for processing text inputs.\n",
    "            preprocess: Preprocessing pipeline containing transformations for images.\n",
    "            device (str, optional): The device to run the model on (default is 'cuda').\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mean = torch.Tensor(preprocess.transforms[-1].mean) \\\n",
    "                         .reshape([1, 3, 1, 1]) \\\n",
    "                         .to(device)\n",
    "        self.std = torch.Tensor(preprocess.transforms[-1].std) \\\n",
    "                        .reshape([1, 3, 1, 1]) \\\n",
    "                        .to(device)\n",
    "\n",
    "    def encode_text(self, texts):\n",
    "        \"\"\"\n",
    "        Encodes input text into an embedding using the CLIP model.\n",
    "\n",
    "        Args:\n",
    "            texts (list or str): A list of text strings or a single text string.\n",
    "                - Shape: (batch_size) (list of strings)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Encoded text embeddings.\n",
    "                - Shape: (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        tokenized_text = self.tokenizer(texts).to(device)\n",
    "        text_embeddings = self.model.encode_text(tokenized_text)\n",
    "        return text_embeddings\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        \"\"\"\n",
    "        Encodes input images into embeddings using the CLIP model after normalization.\n",
    "\n",
    "        Args:\n",
    "            images (Tensor): A batch of images as tensors.\n",
    "                - Shape: (batch_size, 3, height, width)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Encoded image embeddings.\n",
    "                - Shape: (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        normalized_images = self.normalize_image(images)\n",
    "        image_embeddings = self.model.encode_image(normalized_images)\n",
    "        return image_embeddings\n",
    "\n",
    "    def normalize_image(self, x):\n",
    "        \"\"\"\n",
    "        Normalizes images.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input image tensor.\n",
    "                - Shape: (batch_size, 3, height, width)\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized image tensor.\n",
    "                - Shape: (batch_size, 3, height, width)\n",
    "        \"\"\"\n",
    "        return (x - self.mean) / self.std\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Sets the model to evaluation mode.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "def img_show(img):\n",
    "    \"\"\"\n",
    "    Display an image.\n",
    "\n",
    "    Args:\n",
    "        img (Tensor): The image tensor to display.\n",
    "            - Shape: (channels, height, width)\n",
    "    \"\"\"\n",
    "    img = img.permute(1, 2, 0)\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the CLIP model into our wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622,
     "referenced_widgets": [
      "830a3a11bc8b4ffda687582134edf012",
      "a534f81c25ea4449b694c18485b3ad67",
      "fa7a5b2bed1f4deeb8607512b6f9868c",
      "343006d87eaf46888c8b9f191ca03d7f",
      "b9375d2521f34087961e981eeb2c4579",
      "a03a3648d1cb48c594bc80d421d2a897",
      "e78c751099c346c7ae6ee3e37f20b105",
      "cd725918525241d8bded872d7febc949",
      "2cfb4847fb9b4e06a31b2a6d5de15cdb",
      "96f1b695c30b4cfab6149fad512a4aaf",
      "6cc58a237d464c0da4202fe1fea737f4",
      "5198b870af884ccb9e11d8e748403cf4",
      "abc63480960e4d27b8bec8a7e39b8bfe",
      "426002901eae46ab993b93ccd4485683",
      "51ddbe3228da4dc59e6a6780342901ac",
      "b9ed39699957446fbd7b6cd80d689bd4",
      "6ad791a06a374b3886d1a13f28238afb",
      "33564df3cb344eb095b530a5499feebe",
      "9ff757f960bd4ff7b341a9e2a5f1d47c",
      "fc0d765a9d524153bc1a4f0582d85afb",
      "af143ee82faf40c5b459ae58ef474e5a",
      "c201a0fc241d4b80acb4128079f0c692"
     ]
    },
    "id": "LcwZNhDqtjRw",
    "outputId": "325e7412-925f-431c-f370-74c89dc7b531"
   },
   "outputs": [],
   "source": [
    "models_to_load = [\n",
    "    (\"ViT-B-32\", \"laion400m_e32\"),\n",
    "    (\"ViT-B-32\", \"laion2b_s34b_b79k\"),\n",
    "    (\"OpenAI-ViT-B/32\", None),\n",
    "]\n",
    "\n",
    "models = []\n",
    "for model_name, pretrained in models_to_load:\n",
    "    if pretrained is not None:  # openclip models\n",
    "        model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "            model_name=model_name,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        tokenizer = open_clip.get_tokenizer(model_name)\n",
    "\n",
    "    else:  # clip models\n",
    "        model, preprocess = clip.load(model_name.split(\"OpenAI-\")[1])\n",
    "\n",
    "        tokenizer = clip.tokenize\n",
    "\n",
    "    print(f\"Loaded {model_name} {preprocess}\")\n",
    "    model.to(device)\n",
    "    models.append(CLIPModel(model, tokenizer, preprocess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGxCV08ArsgU"
   },
   "source": [
    "## Optimizing an Image for a Target Representation\n",
    "\n",
    "Now, let's implement our solution by first defining a function to compute the similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSgklYJvrsgV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def similarity_score(\n",
    "    image_embeddings,\n",
    "    text_embeddings,\n",
    "    text_weights=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the similarity score between image and text embeddings using cosine similarity.\n",
    "\n",
    "    Args:\n",
    "        image_embeddings (Tensor): A tensor of image embeddings.\n",
    "            - Shape: (batch_size, embedding_dim)\n",
    "        text_embeddings (Tensor): A tensor of text embeddings.\n",
    "            - Shape: (num_texts, embedding_dim)\n",
    "        text_weights (Tensor, optional): Weights for text embeddings. Defaults to None.\n",
    "            - Shape: (num_texts,)\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Similarity scores between image and weighted text embeddings.\n",
    "            - Shape: (batch_size,)\n",
    "    \"\"\"\n",
    "    image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "    text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    if text_weights is None:\n",
    "        text_weights = torch.ones(text_embeddings.shape[0], device=text_embeddings.device)\n",
    "    text_weights = text_weights / text_weights.sum()\n",
    "\n",
    "    weighted_text = (text_weights[:, None] * text_embeddings).sum(dim=0, keepdim=True)\n",
    "    weighted_text = weighted_text / weighted_text.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    sim = (image_embeddings @ weighted_text.t()).squeeze(-1)\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is the core of our method. Here we define an optimizer on an initial random image.\n",
    "\n",
    "We then denoise this image to generate a coherent final image by optimizing it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9EZquAjrsgV"
   },
   "outputs": [],
   "source": [
    "\n",
    "def optimize_image(\n",
    "    model,\n",
    "    target_text,\n",
    "    image_size=224,\n",
    "    learning_rate=1e-1,\n",
    "    num_steps=100\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimizes an image to match a given target text representation using a pre-trained model.\n",
    "    \"\"\"\n",
    "    device = next(model.model.parameters()).device\n",
    "    target_tokens = model.tokenizer(target_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        target_text_embedding = model.model.encode_text(target_tokens)[0]\n",
    "\n",
    "    raw_image = torch.randn(1, 3, image_size, image_size, device=device, requires_grad=True)\n",
    "    optimizer = SGD([raw_image], lr=learning_rate)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda step: max(0.1, 1 - step/num_steps))\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        real_image = raw_to_real_image(raw_image)\n",
    "        image_embedding = model.encode_image(real_image)[0]\n",
    "        sim = similarity_score(image_embedding.unsqueeze(0), target_text_embedding.unsqueeze(0))\n",
    "        loss = -sim.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    return raw_to_real_image(raw_image).detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Hgq7jB7rsgW",
    "outputId": "006d0fa3-e47a-474b-e794-39a62b41add8"
   },
   "outputs": [],
   "source": [
    "generated_image = optimize_image(models[0], \"a photo of a cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp0Jb-tJrsgW"
   },
   "source": [
    "### Resulting image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "C1N0zE8OrsgX",
    "outputId": "8664b0ac-6fdc-4636-f370-b0743d7d5302"
   },
   "outputs": [],
   "source": [
    "img_show(generated_image[0].detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-GRQsM2rsgX"
   },
   "source": [
    "Well, the model identifies the image as a cat—but is it really? Although the image achieves a high similarity score, it doesn’t truly resemble a cat. This optimized image, which tricks the model into making an incorrect prediction, is known as an **adversarial image**.  \n",
    "\n",
    "To better understand why this image was generated, take a look at the following illustration:\n",
    "\n",
    "![mapping-between-images-and-embeddings.png](https://i.postimg.cc/NG2ZbP9J/mapping-between-images-and-embeddings.png)\n",
    "\n",
    "A region of all images corresponding to a {text, image} embedding contains\n",
    "interpretable images as well as noise-like adversarial patterns.\n",
    "Reconstructing an image from an embedding typically leads to\n",
    "such a degenerate noisy image.\n",
    "\n",
    "We now need a method to ensure that the reconstructed image lies within the interpretable region of the manifold. Upon reviewing the generated image, you'll notice that it exhibits high-frequency patterns. In the following sections, we will modify the optimization process to avoid the degenerate high-frequency solutions commonly seen in adversarial examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kJuDzTVrsgX"
   },
   "source": [
    "## Multi-Resolution Optimization\n",
    "We decompose the optimization across multiple scales – a choice that proves surprisingly powerful in guiding solutions toward natural images.\n",
    "We break the degeneracy by decomposing the optimization across multiple scales. Instead of directly optimizing pixels,\n",
    "we express the image as a sum of resolution components:\n",
    "$$\n",
    "I = \\frac{1}{2} + \\frac{1}{2} \\tanh \\left(\\sum_{r \\in \\rho} \\text{resize}_{224}(P_r)\\right) \\qquad (1)\n",
    "$$\n",
    "where $P_r \\in \\mathbb{R}^{r \\times r \\times 3}$ represents the image component at\n",
    "resolution $r$, and $\\rho$ spans from $1 \\times 1$ to $224 \\times 224$. The tanh\n",
    "transformation maps unbounded optimization values to valid\n",
    "pixel intensities while maintaining gradient flow.\n",
    "\n",
    "The optimization objective becomes:\n",
    "$$\n",
    "\\sum_{i,j} \\frac{\\partial \\text{score}_i(\\text{augment}_j(I(P_1, \\ldots, P_{224})))}{\\partial (P_1, \\ldots, P_{224})}\n",
    "$$\n",
    "\n",
    "where $i$ indexes multiple CLIP models and $j$ indexes augmentations. This formulation has several key properties:\n",
    "1. Components are optimized simultaneously across all resolu-\n",
    "tions\n",
    "2. Gradients naturally distribute across scales based\n",
    "on their importance\n",
    "3. High-frequency adversarial patterns are suppressed by scale decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpqmUNzyrA_v"
   },
   "source": [
    "## Image Converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GV2a7wE-WS-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def raw_to_real_image(raw_image):\n",
    "    real_image = torch.tanh(raw_image)\n",
    "    real_image = (real_image + 1) / 2\n",
    "    return real_image.clamp(0, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2As7VDES8SDe"
   },
   "source": [
    "## Augmentation tools\n",
    "As seen in the equation above, we need to augment the image during optimization. This ensures that small changes in the picture do not significantly affect the score, which is a characteristic of natural images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiJtsugVeMtL"
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_jitter(x, size=3):\n",
    "    ox, oy = torch.randint(-size, size + 1, (2,))\n",
    "    return torch.roll(x, shifts=(ox, oy), dims=(2, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGaglVeR-gdu"
   },
   "source": [
    "## Image Generation using Multi-Resolution Optimization\n",
    "In this stage, we will implement the `generate_image` method, which will be used for all five tasks discussed in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sk7tV4in9gq"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_image(\n",
    "    models,\n",
    "    text_weight_pairs,\n",
    "    source_image=None,\n",
    "    original_resolution=224,\n",
    "    large_resolution=224 + 2 * 56,  # adding the buffer on the side\n",
    "    resolutions=range(1, 336 + 1),\n",
    "    batch_size=32,\n",
    "    lr=2e-1,\n",
    "    steps=100,\n",
    "    jitter_scale=56,\n",
    "    noise_scale=0.2,\n",
    "    num_generations=1,\n",
    "    guiding_images=None,\n",
    "):\n",
    "    device = next(models[0].model.parameters()).device\n",
    "\n",
    "    # text setup\n",
    "    texts = [t for _, t in text_weight_pairs]\n",
    "    weights = torch.tensor([w for w, _ in text_weight_pairs], device=device)\n",
    "\n",
    "    # init images\n",
    "    raw_images = torch.randn(num_generations, 3, large_resolution, large_resolution, device=device, requires_grad=True)\n",
    "    optimizer = SGD([raw_images], lr=lr)\n",
    "\n",
    "    text_tokens = [m.tokenizer(texts).to(device) for m in models]\n",
    "    with torch.no_grad():\n",
    "        text_embeds = [m.model.encode_text(tok) for m, tok in zip(models, text_tokens)]\n",
    "\n",
    "    collected = []\n",
    "    components = []\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        real_images = raw_to_real_image(raw_images)\n",
    "        # jitter and noise\n",
    "        jittered = add_jitter(real_images, size=jitter_scale)\n",
    "        jittered = jittered + torch.randn_like(jittered) * noise_scale\n",
    "        # resize to original\n",
    "        resized = torch.nn.functional.interpolate(jittered, size=(original_resolution, original_resolution), mode='bilinear', align_corners=False)\n",
    "\n",
    "        sim_loss = 0\n",
    "        for m, txt_emb in zip(models, text_embeds):\n",
    "            img_emb = m.model.encode_image(resized)\n",
    "            sim = similarity_score(img_emb, txt_emb, text_weights=weights)\n",
    "            sim_loss += -sim.mean()\n",
    "        sim_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % max(1, steps // 10) == 0 or step == steps - 1:\n",
    "            collected.append(resized.detach().cpu())\n",
    "            components.append(resized.detach().cpu())\n",
    "\n",
    "    final_images = raw_to_real_image(raw_images).detach().cpu()\n",
    "    return final_images, components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r21pWO8wrWyD"
   },
   "source": [
    "## Chosing the Models to Use in the Ensemble\n",
    "You can try different model combinations and observe how they affect the results of the following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFJ2j2w8CKKv"
   },
   "outputs": [],
   "source": [
    "chosen_model_ids = [0, 1, 2]\n",
    "models = [x for i, x in enumerate(models) if i in chosen_model_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_GBYth8rixt"
   },
   "source": [
    "## Plotting Tools\n",
    "The following functions help us visualize the results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DphdWhAsuBbP"
   },
   "outputs": [],
   "source": [
    "def display_images(\n",
    "    collected_images,\n",
    "    original_res=224,\n",
    "    large_res=336,\n",
    "    guiding_image=None,\n",
    "    starting_image=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Displays the starting image, guiding image, and generated images side by side.\n",
    "\n",
    "    Args:\n",
    "        collected_images (list of Tensor): A list of tensors representing the generated images at each step.\n",
    "            Each tensor should have the shape (num_generations, channels, height, width).\n",
    "        original_res (int, optional): The resolution of the images to display. Default is 224.\n",
    "        large_res (int, optional): The resolution of the images, including the buffer. Default is 336.\n",
    "        guiding_image (Tensor, optional): The guiding image used during the generation process.\n",
    "            - Shape: (channels, height, width)\n",
    "        starting_image (Tensor, optional): The starting image used for generation.\n",
    "            - Shape: (channels, height, width)\n",
    "    \"\"\"\n",
    "    num_images = collected_images[-1].shape[0] + (guiding_image != None) + (starting_image != None)\n",
    "\n",
    "    plt.figure(figsize=(num_images * 224 / 100, 224 / 100), dpi=112)\n",
    "    subplot_idx = 1\n",
    "\n",
    "    begin = (large_res - original_res) // 2\n",
    "    end = begin + original_res\n",
    "\n",
    "    if starting_image is not None:\n",
    "        plt.subplot(1, num_images, subplot_idx)\n",
    "        img_show(starting_image[0].detach().cpu())\n",
    "        plt.title(\"Starting image\", fontsize=8)\n",
    "        subplot_idx += 1\n",
    "\n",
    "    if guiding_image is not None:\n",
    "        plt.subplot(1, num_images, subplot_idx)\n",
    "        img_show(guiding_image[0].detach().cpu())\n",
    "        plt.title(\"Guiding image\", fontsize=8)\n",
    "        subplot_idx += 1\n",
    "\n",
    "    for i, image in enumerate(collected_images[-1]):\n",
    "        plt.subplot(1, num_images, subplot_idx)\n",
    "        img_show(image[:, begin:end, begin:end].detach().cpu())\n",
    "        if len(collected_images[-1]) == 1:\n",
    "            plt.title(f\"Generated image\", fontsize=8)\n",
    "        else:\n",
    "            plt.title(f\"Generated image v{i + 1}\", fontsize=8)\n",
    "        subplot_idx += 1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ag23xS0_jOn"
   },
   "outputs": [],
   "source": [
    "def visualize_individual_resolutions(\n",
    "    components,\n",
    "    version_i=0,\n",
    "    selected_resolutions=[1, 2, 4, 8, 16, 32, 64],\n",
    "    large_resolution=336,\n",
    "\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes the components of a generated image at different resolutions, both at their original resolution\n",
    "    and interpolated to a larger resolution. This helps to understand the contributions of different resolutions\n",
    "    to the overall generated image.\n",
    "\n",
    "    Args:\n",
    "        components (list of Tensor): A list of tensors representing the image components at various resolutions.\n",
    "            Each tensor is of shape [num_generations, channels, height, width], with different resolutions.\n",
    "        version_i (int, optional): The index for the version of the generated image to display. Defaults to 0.\n",
    "        selected_resolutions (list of int, optional): The list of resolutions to visualize. Each resolution in\n",
    "            the list should correspond to a downsampled version of the image. Default is [1, 2, 4, 8, 16, 32, 64].\n",
    "        large_resolution (int, optional): The resolution to which the components will be interpolated for comparison.\n",
    "            Default is 336.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(3 * len(selected_resolutions), 3 * 2))\n",
    "    for component in components:\n",
    "        if component.shape[2] in selected_resolutions:\n",
    "\n",
    "            plt.subplot(2, len(selected_resolutions), selected_resolutions.index(\n",
    "                component.shape[2]) + 1)\n",
    "\n",
    "            data = component[version_i]\n",
    "            data = raw_to_real_image(data).detach(\n",
    "            ).cpu()\n",
    "            data = data - torch.min(data)\n",
    "            data = data / torch.max(data)\n",
    "            img_show(data)\n",
    "            plt.title(f\"r = {component.shape[2]}\")\n",
    "\n",
    "            plt.subplot(2, len(selected_resolutions), selected_resolutions.index(\n",
    "                component.shape[2])+1+len(selected_resolutions))\n",
    "\n",
    "            data_interpolated = F.interpolate(component, size=(\n",
    "                large_resolution, large_resolution), mode='bicubic')\n",
    "\n",
    "            data = raw_to_real_image(data_interpolated[version_i]).detach(\n",
    "            ).cpu()\n",
    "            data = data - torch.min(data)\n",
    "            data = data / torch.max(data)\n",
    "            img_show(data)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "168jj91PqZAI"
   },
   "source": [
    "## Task 1: Image Generation From Text\n",
    "For the first task, we aim to provide the model with a prompt and generate the corresponding image. The key idea is that we can use multiple text inputs with different weights—including negative weights! Feel free to experiment with different texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCgpY-QZmdYb"
   },
   "outputs": [],
   "source": [
    "text_weight_pairs = [\n",
    "    (1.0, \"a photo of swiss mountain valley\"),\n",
    "    (-0.3, 'cloudy'),\n",
    "    (0.3, \"several homes, lush forest, rivers\"),\n",
    "    (0.3, \"humans, peoples\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Yk4i810opGpt",
    "outputId": "ef05671f-24eb-44c1-9e9c-131830293819"
   },
   "outputs": [],
   "source": [
    "collected_images, components = generate_image(\n",
    "    models,\n",
    "    text_weight_pairs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaFgJ1BHrzTz"
   },
   "source": [
    "### Resulting Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "28H-0ibG93oO",
    "outputId": "4aa23089-dda0-4955-d58c-f5fdc91357e5"
   },
   "outputs": [],
   "source": [
    "display_images(\n",
    "    collected_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY7m3SgsB3zt"
   },
   "source": [
    "### Individual Resolutions\n",
    "An image is expressed as a sum of components at increasing resolutions, from $1 \\times 1$ to $224 \\times 224$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "GYKX59xV-Otn",
    "outputId": "b92f1e26-2232-4574-b862-afedda1fd2f6"
   },
   "outputs": [],
   "source": [
    "visualize_individual_resolutions(components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lh2IWHGCB8Il"
   },
   "source": [
    "## Task 2: Generation Over Multiple Runs\n",
    "Now, we aim to generate multiple images simultaneously for a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGFGDEDVCOIy"
   },
   "outputs": [],
   "source": [
    "text_weight_pairs = [\n",
    "    (1.0, \"a beautiful photo of  Mount Pinatubo eruption., detailed\"),\n",
    "    (-0.3, \"obscured crater\"),\n",
    "    (0.3, \"highly realistic\"),\n",
    "    (-0.3, \"multiple exposure\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JQqdS5HkCOIz",
    "outputId": "6c3ef895-d250-485d-e928-ad1af459ae7c"
   },
   "outputs": [],
   "source": [
    "collected_images, components = generate_image(\n",
    "    models,\n",
    "    text_weight_pairs,\n",
    "    num_generations=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejgxkl-pr_xM"
   },
   "source": [
    "### Resulting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "id": "VPCxx8kECWAg",
    "outputId": "9df799cf-04a9-418b-d218-4222aa079b11"
   },
   "outputs": [],
   "source": [
    "display_images(\n",
    "    collected_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odXtzDaXj_OP"
   },
   "source": [
    "## Task 3: \"Style\" Transfer\n",
    "\n",
    "We're aiming to transfer the style of one image onto another. Do you have any suggestions on how to approach this?\n",
    "\n",
    "(Take a moment to think about it before continuing.)\n",
    "\n",
    "Our approach is to use the original image as the *starting image*, and the image whose style we want to transfer as the *guiding image*.  \n",
    "We then gradually update the starting image so that its embedding becomes closer to that of the guiding image. As a result, the final image ends up being a blend of both—the content of the original image and the style of the guiding image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXB8HOKjCzbs"
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),  # Resize shorter side to 224, maintain aspect ratio\n",
    "            transforms.CenterCrop(224),  # Crop the center 224x224\n",
    "            transforms.ToTensor(),  # Convert to tensor and normalize to [0,1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        tensor = self.transform(image)\n",
    "        return tensor\n",
    "\n",
    "def load_images(image_paths, batch_size=32):\n",
    "    \"\"\"\n",
    "    Load images from list of paths and return batched tensor\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of paths to image files\n",
    "        batch_size (int): Size of batches to return\n",
    "\n",
    "    Returns:\n",
    "        DataLoader that yields tensors of shape [batch_size, 3, 224, 224]\n",
    "    \"\"\"\n",
    "    dataset = ImageDataset(image_paths)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVG_4FO6kp3f"
   },
   "source": [
    "### Starting Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wkvZmtYJkl24",
    "outputId": "282e79ff-c777-401f-fe18-75c4ecb7d0d3"
   },
   "outputs": [],
   "source": [
    "! mkdir -p assets && wget \"https://www.noesnest.com/wp-content/uploads/sites/14/2020/03/san-francisco-at-night.jpg\" -O assets/3-starting-image.jpg\n",
    "\n",
    "image_paths = [\n",
    "    \"assets/3-starting-image.jpg\"\n",
    "]\n",
    "\n",
    "loader = load_images(image_paths)\n",
    "\n",
    "starting_image = next(iter(loader))\n",
    "\n",
    "large_res = 336\n",
    "\n",
    "eps = 0.1 # to offset the image from the ends of the brigthness range\n",
    "starting_image = starting_image * (1 - 2 * eps) + eps\n",
    "\n",
    "# adding padding to make it 336 size in total (not resizing, padding)\n",
    "pad_height = (large_res - starting_image.shape[2]) // 2\n",
    "pad_width = (large_res - starting_image.shape[3]) // 2\n",
    "\n",
    "starting_image = F.pad(\n",
    "    starting_image,\n",
    "    (pad_width, pad_width, pad_height, pad_height),\n",
    "    mode=\"constant\",\n",
    "    value=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "tzzgZghmkz9Z",
    "outputId": "378b7026-d973-4b9c-d794-b541f10df52f"
   },
   "outputs": [],
   "source": [
    "offset = (large_res - 224) // 2\n",
    "plt.figure(figsize=(4 * starting_image.shape[0], 4))\n",
    "for i in range(starting_image.shape[0]):\n",
    "    plt.subplot(1, starting_image.shape[0], i + 1)\n",
    "    img_show(starting_image[i][:, offset:-offset, offset:-offset])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An_WWq4Nl5YA"
   },
   "source": [
    "### \"Style\" Guiding Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZlGjbtJnlXLL",
    "outputId": "cbbb1e4d-e050-4a58-bc73-9855a3ae10f4"
   },
   "outputs": [],
   "source": [
    "! wget \"https://i.postimg.cc/44hZvX9K/majestic-medieval-castle-stockcake.jpg\" -O assets/3-guiding-image.png\n",
    "image_paths = [\n",
    "    \"assets/3-guiding-image.png\"\n",
    "]\n",
    "\n",
    "loader = load_images(image_paths)\n",
    "guiding_image = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "aty7SZz-mF_j",
    "outputId": "caefba99-1c0f-4cf1-b378-786fc60d1802"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4 * guiding_image.shape[0], 4))\n",
    "for i in range(guiding_image.shape[0]):\n",
    "    plt.subplot(1, guiding_image.shape[0], i + 1)\n",
    "    img_show(guiding_image[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wsgMLASmWwN"
   },
   "source": [
    "### Combining the Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "A6JZYfbImRlo",
    "outputId": "11eb83bd-f8f7-44cd-c865-e187bcd4e7e6"
   },
   "outputs": [],
   "source": [
    "collected_images, components = generate_image(\n",
    "    models,\n",
    "    text_weight_pairs=[],\n",
    "    guiding_images=guiding_image,\n",
    "    source_image=starting_image.detach().cpu(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3GP0Pw-sEVX"
   },
   "source": [
    "### Resulting Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "8gFD4li2mc2O",
    "outputId": "084a4e9a-fdd3-416b-e82b-a3db4a03efd0"
   },
   "outputs": [],
   "source": [
    "display_images(\n",
    "    collected_images,\n",
    "    guiding_image = guiding_image,\n",
    "    starting_image = starting_image\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWtAYXrtnICK"
   },
   "source": [
    "## Task 4: Reconstructing an Image from Its Embedding\n",
    "\n",
    "In this section, we explore whether our gradient-based approach can be used to simulate image reconstruction. Do you have any suggestions?\n",
    "\n",
    "(Take a moment to think about this before continuing.)\n",
    "\n",
    "We use the target image as the *guiding image*. To further support the optimization process, we can also provide a well-crafted prompt to guide the reconstruction more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCUYRo-6nPNl"
   },
   "source": [
    "### Getting the Image to Reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-_W5NgAnLBq",
    "outputId": "90405ca0-c4e3-4673-816b-4d95ac55cd68"
   },
   "outputs": [],
   "source": [
    "! wget \"https://i.postimg.cc/KYLtvPNn/4-guiding-image.png\" -O assets/4-guiding-image.png\n",
    "\n",
    "image_paths = [\n",
    "    \"assets/4-guiding-image.png\"\n",
    "]\n",
    "\n",
    "loader = load_images(image_paths)\n",
    "guiding_image = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "WYviWN2teMtU",
    "outputId": "8cefad53-18d3-49d9-a213-fd0dbea6a8aa"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4 * guiding_image.shape[0], 4))\n",
    "for i in range(guiding_image.shape[0]):\n",
    "    plt.subplot(1, guiding_image.shape[0], i + 1)\n",
    "    img_show(guiding_image[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4EZEN2hoLhO"
   },
   "source": [
    "### Reconstructing the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4icwX0T0nTqy",
    "outputId": "68fd4786-3e30-4162-91d0-e810ec00c8c3"
   },
   "outputs": [],
   "source": [
    "# helping the reconstruction with a content agnostic set of prompts\n",
    "text_weight_pairs = [\n",
    "  (0.6,\"cohesive single subject\"),\n",
    "  (-0.6, \"multiple exposure\"),\n",
    "]\n",
    "\n",
    "collected_images, _ = generate_image(\n",
    "  models,\n",
    "  text_weight_pairs = text_weight_pairs,\n",
    "  guiding_images = guiding_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5ce7giYsG0i"
   },
   "source": [
    "## Resulting image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "NfZCvaymn-cv",
    "outputId": "6f564ec1-60a0-4e2e-9ecf-8cdb0c8c7cfe"
   },
   "outputs": [],
   "source": [
    "display_images(\n",
    "    collected_images,\n",
    "    guiding_image = guiding_image,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qojfd_YPxFbu"
   },
   "source": [
    "## Task 5: Inpainting\n",
    "\n",
    "To achieve a more visually appealing result, we aim to edit a specific part of the image. This feature is already supported by the `generate_image` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5p40BbwlfI29",
    "outputId": "7ecffb12-ed7d-452d-edb9-97e32c08f1da"
   },
   "outputs": [],
   "source": [
    "! wget \"https://www.noesnest.com/wp-content/uploads/sites/14/2020/03/san-francisco-at-night.jpg\" -O assets/5-starting-image.jpg\n",
    "\n",
    "image_paths = [\n",
    "    \"assets/5-starting-image.jpg\"\n",
    "]\n",
    "\n",
    "loader = load_images(image_paths)\n",
    "\n",
    "starting_image = next(iter(loader))\n",
    "\n",
    "large_res = 336\n",
    "\n",
    "eps = 0.1 # to offset the image from the ends of the brigthness range\n",
    "starting_image = starting_image * (1 - 2 * eps) + eps\n",
    "\n",
    "# adding padding to make it 336 size in total (not resizing, padding)\n",
    "pad_height = (large_res - starting_image.shape[2]) // 2\n",
    "pad_width = (large_res - starting_image.shape[3]) // 2\n",
    "\n",
    "starting_image = F.pad(\n",
    "    starting_image,\n",
    "    (pad_width, pad_width, pad_height, pad_height),\n",
    "    mode=\"constant\",\n",
    "    value=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpainting Mask\n",
    "To customize the image editing process, you can specify your own mask and prompt to guide the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeAFtbY6fke4"
   },
   "outputs": [],
   "source": [
    "text_weight_pairs = [\n",
    "    (1.0, \"a vast night sky filled with countless twinkling stars\"),\n",
    "    (0.3, \"the stars vary in size and brightness\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "DYk0KnIFfvYQ",
    "outputId": "7d7e9570-74a1-446d-eb82-bd8963350e55"
   },
   "outputs": [],
   "source": [
    "mask = torch.zeros_like(starting_image)\n",
    "mask[:, :, 60:120,70:270] = 1\n",
    "\n",
    "img_show((starting_image * (1 - mask))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "x2Z7W6SlfgN1",
    "outputId": "eb374a95-ca50-4b31-9371-45e49ae58395"
   },
   "outputs": [],
   "source": [
    "collected_images, components = generate_image(\n",
    "    models,\n",
    "    text_weight_pairs,\n",
    "    inpainting_mask=mask.to(device),\n",
    "    source_image = starting_image.detach().cpu(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resulting Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "r3b9DDL-x00l",
    "outputId": "922ecb10-8a54-4b46-d716-a726eaf5edc6"
   },
   "outputs": [],
   "source": [
    "display_images(\n",
    "    collected_images,\n",
    "    starting_image=starting_image\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2cfb4847fb9b4e06a31b2a6d5de15cdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "33564df3cb344eb095b530a5499feebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "343006d87eaf46888c8b9f191ca03d7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96f1b695c30b4cfab6149fad512a4aaf",
      "placeholder": "​",
      "style": "IPY_MODEL_6cc58a237d464c0da4202fe1fea737f4",
      "value": " 605M/605M [00:03&lt;00:00, 129MB/s]"
     }
    },
    "426002901eae46ab993b93ccd4485683": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ff757f960bd4ff7b341a9e2a5f1d47c",
      "max": 605143316,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc0d765a9d524153bc1a4f0582d85afb",
      "value": 605143316
     }
    },
    "5198b870af884ccb9e11d8e748403cf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_abc63480960e4d27b8bec8a7e39b8bfe",
       "IPY_MODEL_426002901eae46ab993b93ccd4485683",
       "IPY_MODEL_51ddbe3228da4dc59e6a6780342901ac"
      ],
      "layout": "IPY_MODEL_b9ed39699957446fbd7b6cd80d689bd4"
     }
    },
    "51ddbe3228da4dc59e6a6780342901ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af143ee82faf40c5b459ae58ef474e5a",
      "placeholder": "​",
      "style": "IPY_MODEL_c201a0fc241d4b80acb4128079f0c692",
      "value": " 605M/605M [00:04&lt;00:00, 197MB/s]"
     }
    },
    "6ad791a06a374b3886d1a13f28238afb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6cc58a237d464c0da4202fe1fea737f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "830a3a11bc8b4ffda687582134edf012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a534f81c25ea4449b694c18485b3ad67",
       "IPY_MODEL_fa7a5b2bed1f4deeb8607512b6f9868c",
       "IPY_MODEL_343006d87eaf46888c8b9f191ca03d7f"
      ],
      "layout": "IPY_MODEL_b9375d2521f34087961e981eeb2c4579"
     }
    },
    "96f1b695c30b4cfab6149fad512a4aaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ff757f960bd4ff7b341a9e2a5f1d47c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a03a3648d1cb48c594bc80d421d2a897": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a534f81c25ea4449b694c18485b3ad67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a03a3648d1cb48c594bc80d421d2a897",
      "placeholder": "​",
      "style": "IPY_MODEL_e78c751099c346c7ae6ee3e37f20b105",
      "value": "open_clip_model.safetensors: 100%"
     }
    },
    "abc63480960e4d27b8bec8a7e39b8bfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ad791a06a374b3886d1a13f28238afb",
      "placeholder": "​",
      "style": "IPY_MODEL_33564df3cb344eb095b530a5499feebe",
      "value": "open_clip_model.safetensors: 100%"
     }
    },
    "af143ee82faf40c5b459ae58ef474e5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9375d2521f34087961e981eeb2c4579": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9ed39699957446fbd7b6cd80d689bd4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c201a0fc241d4b80acb4128079f0c692": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd725918525241d8bded872d7febc949": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e78c751099c346c7ae6ee3e37f20b105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa7a5b2bed1f4deeb8607512b6f9868c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd725918525241d8bded872d7febc949",
      "max": 605143284,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2cfb4847fb9b4e06a31b2a6d5de15cdb",
      "value": 605143284
     }
    },
    "fc0d765a9d524153bc1a4f0582d85afb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
