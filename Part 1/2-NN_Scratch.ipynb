{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5fd69e",
   "metadata": {
    "id": "5e5fd69e"
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://www.sharif.ir/documents/20124/0/logo-fa-IR.png/4d9b72bc-494b-ed5a-d3bb-e7dfd319aec8?t=1609608338755\" alt=\"Logo\" width=\"200\">\n",
    "    <p><b>HW1 @ Deep Learning Course, Dr. Soleymani</b></p>\n",
    "    <p><b>ŸêDesinged by Amirmahdi Meighani</b></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sJ87dPg0aerE",
   "metadata": {
    "id": "sJ87dPg0aerE"
   },
   "source": [
    "\n",
    "*Full Name:*\n",
    "\n",
    "*Student Number:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eYZZ0jkJc__9",
   "metadata": {
    "id": "eYZZ0jkJc__9"
   },
   "source": [
    "# Overview: Building a Neural Network Framework with NumPy\n",
    "\n",
    "In this assignment, you will build a simple **Multi-Layer Perceptron (MLP) framework** using only NumPy. By implementing each component step by step, you will gain a fundamental understanding of **forward and backward propagation**, **loss functions**, and **optimization techniques**.\n",
    "\n",
    "Each part of the assignment builds on the previous one.\n",
    "\n",
    "### Goal\n",
    "To build a modular **Multi-Layer Perceptron (MLP)** framework using NumPy from scratch.\n",
    "\n",
    "### Structure of the Assignment\n",
    "\n",
    "Each section below builds towards the final goal of training an MLP. The first steps focus on implementing fundamental components, while later steps involve integrating them into a complete training pipeline.\n",
    "\n",
    "#### **Step 1: Implementing Core Functions (70 points)**\n",
    "- **Forward & Backward Passes (10 points):** Implement `affine_forward` and `affine_backward` to compute layer outputs and gradients.\n",
    "- **Activation Functions (20 points):** Implement ReLU and Sigmoid functions (`relu_forward/backward`, `sigmoid_forward/backward`).\n",
    "- **Loss Calculation (15 points):** Implement `mse_loss` to compute Mean Squared Error and its gradient.\n",
    "- **Building a Multi-Layer Network (15 points):** Define a `FullyConnectedNet` class that integrates all layers and computes forward and backward passes.\n",
    "- **Optimization (10 points):** Implement `sgd_momentum` to update parameters.\n",
    "\n",
    "#### **Step 2: Training & Evaluation (30 points)**\n",
    "- **Training (25 points):** Train the `FullyConnectedNet` using a `Solver` class.\n",
    "- **Evaluation (5 points):** Assess the model on validation/test data.\n",
    "**__Your score on this section is based on the result of your model and defines how well the previous sections are implemented.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f2e4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "881f2e4c",
    "outputId": "1b9c1c72-55ee-4784-ec1c-327c4322e95a"
   },
   "outputs": [],
   "source": [
    "!pip install future\n",
    "!pip install pandas\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb6140",
   "metadata": {
    "id": "8efb6140"
   },
   "outputs": [],
   "source": [
    "from utils.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from utils.data import get_california_housing_data, get_california_housing_normalized__data\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.solver import *\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def print_mean_std(x,axis=0):\n",
    "    print(f\"  means: {x.mean(axis=axis)}\")\n",
    "    print(f\"  stds:  {x.std(axis=axis)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c3af9e",
   "metadata": {
    "id": "57c3af9e"
   },
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In this exercise we will implement fully-connected networks using a modular approach. For each layer we will implement a generic `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive dout (derivative of loss with respect to outputs) and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build different models with architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649895ab",
   "metadata": {
    "id": "649895ab"
   },
   "source": [
    "# Affine layer: forward (5 Points)\n",
    "Implement the `affine_forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c584b4",
   "metadata": {
    "id": "03c584b4"
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    x_reshaped = x.reshape(x.shape[0], -1)\n",
    "    out = x_reshaped.dot(w) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd5827",
   "metadata": {
    "id": "acdd5827"
   },
   "source": [
    "You can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80db2fe8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80db2fe8",
    "outputId": "fe2b98c2-2c2c-4e31-dd11-fb9a53d659d8"
   },
   "outputs": [],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64b312",
   "metadata": {
    "id": "8e64b312"
   },
   "source": [
    "# Affine layer: backward (5 Points)\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking.\n",
    "\n",
    "\n",
    "Given an affine transformation in a neural network:\n",
    "\n",
    "$\n",
    "Z = XW + b\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ X $ is the input matrix of shape $ (N, D) $\n",
    "- $ W $ is the weight matrix of shape $ (D, M) $\n",
    "- $ b $ is the bias vector of shape $ (1, M) $\n",
    "- $ Z $ is the output before activation of shape $ (N, M) $\n",
    "\n",
    "### Backward Pass Gradients:\n",
    "\n",
    "1. **Gradient w.r.t. input $ X $:**\n",
    " $\n",
    "   dX = dZ W^T\n",
    "$\n",
    "   (Shape: $ (N, D) $)\n",
    "\n",
    "2. **Gradient w.r.t. weights $ W $:**\n",
    "   $\n",
    "   dW = X^T dZ\n",
    "   $\n",
    "   (Shape: $ (D, M) $)\n",
    "\n",
    "3. **Gradient w.r.t. bias $ b $:**\n",
    "   $\n",
    "   db = \\sum_{i=1}^{N} dZ_i\n",
    "   $\n",
    "   (Shape: $ (1, M) $)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d5b9c",
   "metadata": {
    "id": "e61d5b9c"
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    N = x.shape[0]\n",
    "\n",
    "    dx = dout.dot(w.T).reshape(x.shape)\n",
    "    dw = x.reshape(N, -1).T.dot(dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    return dx, dw, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de3af3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64de3af3",
    "outputId": "305c4c6e-8b3e-44e0-caf4-9eb31d58fae9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5910d2",
   "metadata": {
    "id": "af5910d2"
   },
   "source": [
    "# ReLU activation: forward (5 Points)\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537098f",
   "metadata": {
    "id": "c537098f"
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = np.maximum(0, x)\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc17aa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cc17aa4",
    "outputId": "e2cd3ae3-de3f-4844-b207-fab3514f2c0b"
   },
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171f37c",
   "metadata": {
    "id": "6171f37c"
   },
   "source": [
    "# ReLU activation: backward (5 Points)\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function.\n",
    "\n",
    "\n",
    "ReLU Activation Backward Pass:\n",
    "\n",
    "Given the ReLU activation function:\n",
    "\n",
    "$ A = \\max(0, Z) $\n",
    "\n",
    "where $ A $ is the output after activation, and $ Z $ is the pre-activation input.\n",
    "\n",
    "1. **Gradient w.r.t. pre-activation input $ Z $:**\n",
    "   $\n",
    "   dZ = dA \\cdot \\mathbb{1}(Z > 0)\n",
    "   $\n",
    "   \n",
    "   (Element-wise multiplication where $ \\mathbb{1}(Z > 0) $ is an indicator function that is $1$ when $ Z > 0 $ and 0 otherwise.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187da88",
   "metadata": {
    "id": "3187da88"
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx = dout * (cache > 0).astype(float)\n",
    "    return dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88966dd6",
   "metadata": {
    "id": "88966dd6"
   },
   "source": [
    "You can test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81e30fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e81e30fd",
    "outputId": "495f392f-f664-49d5-cf3f-da2e78782209"
   },
   "outputs": [],
   "source": [
    "# Test the relu_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2e4d2",
   "metadata": {
    "id": "b0f2e4d2"
   },
   "source": [
    "# Sigmoid activation: forward (5 Points)\n",
    "Implement the forward pass for the Sigmoid activation function in the `sigmoid_forward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872854b0",
   "metadata": {
    "id": "872854b0"
   },
   "outputs": [],
   "source": [
    "def sigmoid_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of Sigmoid.\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aadfec8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aadfec8",
    "outputId": "3f36942e-600b-4c13-9043-8a7aa83d71e4"
   },
   "outputs": [],
   "source": [
    "# Test the sigmoid_forward function\n",
    "\n",
    "x = np.linspace(-6, 6, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = sigmoid_forward(x)\n",
    "correct_out = np.array([[0.00247262, 0.00732514, 0.0214955 , 0.06138311],\n",
    "                        [0.16296047, 0.36691963, 0.63308037, 0.83703953],\n",
    "                        [0.93861689, 0.9785045 , 0.99267486, 0.99752738]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-7\n",
    "print('Testing sigmoid_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4e6a8",
   "metadata": {
    "id": "9aa4e6a8"
   },
   "source": [
    "# Sigmoid activation: backward (5 Points)\n",
    "Now implement the backward pass for the Sigmoid activation function in the `sigmoid_backward` function.\n",
    "\n",
    "Given the Sigmoid activation function:\n",
    "\n",
    "$\n",
    "A = \\sigma(Z) = \\frac{1}{1 + e^{-Z}}\n",
    "$\n",
    "\n",
    "where $ A $ is the output after activation, and $ Z $ is the pre-activation input.\n",
    "\n",
    "The derivative of the Sigmoid function is:\n",
    "\n",
    "$\n",
    "\\frac{dA}{dZ} = \\sigma(Z) (1 - \\sigma(Z))\n",
    "$\n",
    "\n",
    "The backward pass computes the gradient:\n",
    "\n",
    "1. **Gradient w.r.t. pre-activation input $ Z $:**\n",
    "   $\n",
    "   dZ = dA \\cdot \\sigma(Z) (1 - \\sigma(Z))\n",
    "   $\n",
    "   \n",
    "   (Element-wise multiplication using the derivative of Sigmoid.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe527984",
   "metadata": {
    "id": "fe527984"
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of Sigmoid.\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    sigmoid_out = 1 / (1 + np.exp(-cache))\n",
    "    dx = dout * sigmoid_out * (1 - sigmoid_out)\n",
    "    return dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ec2570",
   "metadata": {
    "id": "b5ec2570"
   },
   "source": [
    "You can test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06a7eef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e06a7eef",
    "outputId": "4ef2f017-5504-4864-d17d-fc6f5770738b"
   },
   "outputs": [],
   "source": [
    "# Test the sigmoid_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: sigmoid_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = sigmoid_forward(x)\n",
    "dx = sigmoid_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-11\n",
    "print('Testing sigmoid_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4492a",
   "metadata": {
    "id": "31d4492a"
   },
   "source": [
    "# Loss layer: MSE (15 Points)\n",
    "Now implement the loss and gradient for mean squared error in the `mse_loss` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5cdd9",
   "metadata": {
    "id": "92f5cdd9"
   },
   "outputs": [],
   "source": [
    "def mse_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for MSE loss.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N,) where x[i] is the predicted vector for\n",
    "        the ith input.\n",
    "    - y: Vector of target values, of shape (N,) where y[i] is the target value\n",
    "        for the ith input.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    x_orig_shape = x.shape\n",
    "    x = np.squeeze(x)\n",
    "\n",
    "    N = x.shape[0]\n",
    "    loss = np.sum((x - y) ** 2) / N\n",
    "    dx = (2 / N) * (x - y)\n",
    "    dx = dx.reshape(x_orig_shape)\n",
    "    return loss, dx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5b9db",
   "metadata": {
    "id": "55b5b9db"
   },
   "source": [
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e850132",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e850132",
    "outputId": "06b0abe3-f512-4fc8-9a86-f19cf1581c40"
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "num_inputs = 50\n",
    "x = np.random.randn(num_inputs)\n",
    "y = np.random.randn(num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: mse_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = mse_loss(x, y)\n",
    "\n",
    "# Test mse_loss function. Loss should be close to 1.9 and dx error should be around e-9\n",
    "print('\\nTesting mse_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7bb75",
   "metadata": {
    "id": "71a7bb75"
   },
   "source": [
    "# Multi-Layer Fully Connected Network (15 Points)\n",
    "In this part, you will implement a fully connected network with an arbitrary number of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd460d2",
   "metadata": {
    "id": "ffd460d2"
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNet(object):\n",
    "    \"\"\"Class for a multi-layer fully connected neural network.\n",
    "\n",
    "    Network contains an arbitrary number of hidden layers, ReLU nonlinearities,\n",
    "    and a softmax loss function for a classification problem or the MSE loss function for\n",
    "    a regression problem. The architecture will be\n",
    "\n",
    "    {affine - relu} x (L - 1) - affine - softmax/mse\n",
    "\n",
    "    where the {...} block is repeated L - 1 times.\n",
    "\n",
    "    Learnable parameters are stored in the self.params dictionary and will be learned\n",
    "    using the Solver class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dims,\n",
    "        input_dim=784,\n",
    "        output_dim=10,\n",
    "        reg=0.0,\n",
    "        weight_scale=1e-2,\n",
    "        dtype=np.float32,\n",
    "    ):\n",
    "        \"\"\"Initialize a new FullyConnectedNet.\n",
    "\n",
    "        Inputs:\n",
    "\n",
    "        - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "        - input_dim: An integer giving the size of the input.\n",
    "        - output_dim: An integer giving the number of classes to classify. It\n",
    "            is 1 for a regression problem.\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "            initialization of the weights.\n",
    "        - dtype: A numpy datatype object; all computations will be performed using\n",
    "            this datatype. float32 is faster but less accurate, so you should use\n",
    "            float64 for numeric gradient checking.\n",
    "        \"\"\"\n",
    "        self.category = 'regression'\n",
    "        self.reg = reg\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        self.params = {}\n",
    "\n",
    "        # Initialize weights and biases for each layer\n",
    "        layer_dims = [input_dim] + hidden_dims + [output_dim]\n",
    "        for i in range(self.num_layers):\n",
    "            self.params[f'W{i+1}'] = np.random.randn(layer_dims[i], layer_dims[i+1]) * weight_scale\n",
    "            self.params[f'b{i+1}'] = np.zeros(layer_dims[i+1])\n",
    "\n",
    "        # Cast all parameters to the correct datatype.\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"Compute loss and gradient for the fully connected net.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels / target values, of shape (N,). y[i] gives the\n",
    "            label / target value for X[i].\n",
    "        - show_distributions: Boolean that if enabled and y in none, show output\n",
    "        of each layer to see effects of diffrent approaches\n",
    "\n",
    "        Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return\n",
    "        scores for a classification problem or the predicted_values for\n",
    "        a regression problem:\n",
    "        - out: Array of shape (N, C) / (N, ) giving classification scores / predicted values, where\n",
    "        scores[i, c] is the classification score for X[i] and class c / predicted_values[i]\n",
    "        is the predicted value for X[i].\n",
    "\n",
    "\n",
    "        If y is not None, then run a training-time forward and backward pass and\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "            names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        X = X.astype(self.dtype)\n",
    "        mode = \"test\" if y is None else \"train\"\n",
    "\n",
    "        out = X\n",
    "        caches = {}\n",
    "        for i in range(self.num_layers - 1):\n",
    "            out, cache = affine_forward(out, self.params[f'W{i+1}'], self.params[f'b{i+1}'])\n",
    "            caches[f'layer_{i+1}'] = cache\n",
    "            out, cache_relu = relu_forward(out)\n",
    "            caches[f'relu_{i+1}'] = cache_relu\n",
    "\n",
    "        out, cache = affine_forward(out, self.params[f'W{self.num_layers}'], self.params[f'b{self.num_layers}'])\n",
    "        caches[f'layer_{self.num_layers}'] = cache\n",
    "\n",
    "        if mode == \"test\":\n",
    "            return out\n",
    "\n",
    "        loss, grads = 0.0, {}\n",
    "\n",
    "        # Data loss\n",
    "        if self.category == 'regression':\n",
    "            loss, dyhat = mse_loss(out, y)\n",
    "        else:\n",
    "            loss, dyhat = softmax_loss(out, y)\n",
    "\n",
    "        # Regularization\n",
    "        for i in range(self.num_layers):\n",
    "            W = self.params[f'W{i+1}']\n",
    "            loss += 0.5 * self.reg * np.sum(W ** 2)\n",
    "\n",
    "        # Backpropagation for final affine layer\n",
    "        dyhat, dw, db = affine_backward(dyhat, caches[f'layer_{self.num_layers}'])\n",
    "        grads[f'W{self.num_layers}'] = dw + self.reg * self.params[f'W{self.num_layers}']\n",
    "        grads[f'b{self.num_layers}'] = db\n",
    "\n",
    "        # Backprop for hidden layers\n",
    "        for i in reversed(range(self.num_layers - 1)):\n",
    "            dyhat = relu_backward(dyhat, caches[f'relu_{i+1}'])\n",
    "            dyhat, dw, db = affine_backward(dyhat, caches[f'layer_{i+1}'])\n",
    "            grads[f'W{i+1}'] = dw + self.reg * self.params[f'W{i+1}']\n",
    "            grads[f'b{i+1}'] = db\n",
    "\n",
    "        return loss, grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a2795",
   "metadata": {
    "id": "a14a2795"
   },
   "source": [
    "## Initial Loss and Gradient Check\n",
    "\n",
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. This is a good way to see if the initial losses seem reasonable.\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-7 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf719033",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf719033",
    "outputId": "6bada870-96cf-4dd8-cf84-42031f6c69e9"
   },
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2 = 2, 15, 20, 30\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(123, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "    print(\"Running check with reg = \", reg)\n",
    "    model = FullyConnectedNet(\n",
    "        [H1, H2],\n",
    "        input_dim=D,\n",
    "        output_dim=1,\n",
    "        reg=reg,\n",
    "        weight_scale=5e-2,\n",
    "        dtype=np.float64\n",
    "    )\n",
    "\n",
    "    loss, grads = model.loss(X, y)\n",
    "    print(\"Initial loss: \", loss)\n",
    "\n",
    "    # Most of the errors should be on the order of e-7 or smaller.\n",
    "    # NOTE: It is fine however to see an error for W2 on the order of e-5\n",
    "    # for the check when reg = 0.0\n",
    "    for name in sorted(grads):\n",
    "        f = lambda _: model.loss(X, y)[0]\n",
    "        grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "        print(f\"{name} relative error: {rel_error(grad_num, grads[name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042ddcf",
   "metadata": {
    "id": "6042ddcf"
   },
   "source": [
    "## SGD+Momentum (10 Points)\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochastic gradient descent.\n",
    "\n",
    "Implement the SGD+momentum update rule in the function `sgd_momentum`.\n",
    "\n",
    "### Question:\n",
    "How does the behavior of the optimization process, specifically the trajectory through the loss landscape, differ when using Stochastic Gradient Descent (SGD) with momentum compared to using vanilla SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e678506",
   "metadata": {
    "id": "2e678506"
   },
   "outputs": [],
   "source": [
    "def sgd_momentum(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent with momentum.\n",
    "    Inputs:\n",
    "    - w: A numpy array giving the current weights.\n",
    "    - dw: A numpy array of the same shape as w giving the gradient of the\n",
    "    loss with respect to w.\n",
    "    - config: A dictionary containing hyperparameter values such as learning\n",
    "    rate, momentum.\n",
    "\n",
    "    Returns:\n",
    "      - next_w: The next point after the update.\n",
    "      - config: The config dictionary to be passed to the next iteration of the\n",
    "        update rule.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - momentum: Scalar between 0 and 1 giving the momentum value.\n",
    "      Setting momentum = 0 reduces sgd_momentum to stochastic gradient descent.\n",
    "    - velocity: A numpy array of the same shape as w and dw used to store a\n",
    "      moving average of the gradients.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    config.setdefault(\"learning_rate\", 1e-2)\n",
    "    config.setdefault(\"momentum\", 0.9)\n",
    "    v = config.get(\"velocity\", np.zeros_like(w))\n",
    "\n",
    "    v = config['momentum'] * v - config['learning_rate'] * dw\n",
    "    next_w = w + v\n",
    "    config[\"velocity\"] = v\n",
    "    return next_w, config\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1730d10",
   "metadata": {
    "id": "f1730d10"
   },
   "source": [
    "Run the following to check your implementation. You should see errors less than e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3595fc5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3595fc5b",
    "outputId": "5206453b-f89a-438c-d9ad-4a09351118ea"
   },
   "outputs": [],
   "source": [
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {\"learning_rate\": 1e-3, \"velocity\": v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "# Should see relative errors around e-8 or less\n",
    "print(\"next_w error: \", rel_error(next_w, expected_next_w))\n",
    "print(\"velocity error: \", rel_error(expected_velocity, config[\"velocity\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b0e50",
   "metadata": {},
   "source": [
    "# Real World Application\n",
    "Now that we have implemented the core functions and built a multilayer network, and tested it on random data to ensure it functions correctly, It is time to train the model on a real dataset. For this, we will use the California Housing dataset, which contains information about housing prices in different districts of California. The dataset includes features such as median income, house age, and population, making it a useful benchmark for regression tasks. Our goal is to train the network to predict house prices based on these features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b7aa1",
   "metadata": {
    "id": "5e9b7aa1"
   },
   "source": [
    "# California housing dataset\n",
    "This is a dataset obtained from the [StatLib repository](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ee041f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "b1ee041f",
    "outputId": "2beb326e-9fb4-403c-ffe9-cee97262ad22"
   },
   "outputs": [],
   "source": [
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "california_housing.frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc5ad10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbc5ad10",
    "outputId": "6e66309c-de69-440b-f778-207ea40f3cce"
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = get_california_housing_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train target values shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation target values shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test target values shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1eeb3",
   "metadata": {
    "id": "30a1eeb3"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = get_california_housing_normalized__data(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1179b5c",
   "metadata": {
    "id": "a1179b5c"
   },
   "source": [
    "# Training (25)\n",
    "Train the best fully connected model that you can on california housing, storing your best model in the `california_housing_best_model` variable.\n",
    "Your final score depends on the accuracy(since your model is performing regression accuracy is equal to loss) of your best model reported in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae7eca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2fae7eca",
    "outputId": "f43b7f5b-083b-4992-a2e5-9b17a5f4893e"
   },
   "outputs": [],
   "source": [
    "california_housing_best_model = None\n",
    "\n",
    "# Train a FullyConnectedNet on California housing data\n",
    "# Batch norm or deeper networks can help; here is a solid default.\n",
    "data = {\n",
    "    'X_train': X_train,\n",
    "    'y_train': y_train.astype(int),\n",
    "    'X_val': X_val,\n",
    "    'y_val': y_val.astype(int),\n",
    "    'X_test': X_test,\n",
    "    'y_test': y_test.astype(int)\n",
    "    }\n",
    "\n",
    "model = FullyConnectedNet(\n",
    "    hidden_dims=[64, 64, 32],\n",
    "    input_dim=X_train.shape[1],\n",
    "    output_dim=1,\n",
    "    reg=0.001,\n",
    "    weight_scale=1e-2,\n",
    "    dtype=np.float64\n",
    ")\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                update_rule=sgd_momentum,\n",
    "                optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                    'momentum': 0.9\n",
    "                },\n",
    "                lr_decay=0.95,\n",
    "                num_epochs=5,\n",
    "                batch_size=200,\n",
    "                print_every=100)\n",
    "\n",
    "solver.train()\n",
    "\n",
    "california_housing_solver = solver\n",
    "california_housing_best_model = model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y2J1QE8lCl9c",
   "metadata": {
    "id": "y2J1QE8lCl9c"
   },
   "source": [
    "# Visualization (5)\n",
    "\n",
    "Choose and apply several relevant visualization techniques to evaluate the performance and behavior of a regression model. For each chosen technique, provide:\n",
    "\n",
    "1. A brief description of the method and its purpose.\n",
    "2. A demonstration of the method. (plot it)\n",
    "3. An explanation of the insights gained from the visualization, focusing on what it reveals about the model's effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e51742",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 942
    },
    "id": "43e51742",
    "outputId": "a7061ef9-2bc9-4962-aa8f-f1851f279e84"
   },
   "outputs": [],
   "source": [
    "# Visualize training loss and train / val RMS error\n",
    "\n",
    "def visualize_training_results(solver, title_prefix=\"\"):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    axs[0].plot(solver.loss_history, label='Training Loss', color='blue')\n",
    "    axs[0].set_title(f\"{title_prefix}Training Loss over Iterations\")\n",
    "    axs[0].set_xlabel('Iteration')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "\n",
    "    train_rms = np.sqrt(np.array(solver.train_acc_history))\n",
    "    val_rms = np.sqrt(np.array(solver.val_acc_history))\n",
    "\n",
    "    axs[1].plot(train_rms, label='Train RMS Error', color='green')\n",
    "    axs[1].plot(val_rms, label='Validation RMS Error', color='red')\n",
    "    axs[1].set_title(f\"{title_prefix}Train/Validation RMS Error over Epochs\")\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('RMS Error')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_training_results(california_housing_solver)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26SZuT841c7R",
   "metadata": {
    "id": "26SZuT841c7R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
