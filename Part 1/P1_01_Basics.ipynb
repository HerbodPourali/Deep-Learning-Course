{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e5fd69e"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img src=\"https://www.sharif.ir/documents/20124/0/logo-fa-IR.png/4d9b72bc-494b-ed5a-d3bb-e7dfd319aec8?t=1609608338755\" alt=\"Logo\" width=\"200\">\n",
        "    <p><b>HW1 @ Deep Learning Course, Dr. Soleymani</b></p>\n",
        "    <p><b>ŸêDesinged by Payam Taebi</b></p>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ87dPg0aerE"
      },
      "source": [
        "\n",
        "*Full Name:*\n",
        "\n",
        "*Student Number:* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQndOAmiVTO3"
      },
      "source": [
        "# Setup Code\n",
        "Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
        "\n",
        "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5PzjwH7VTO4",
        "outputId": "416a75a3-7ab5-4777-a4e2-c64fce68bdd7"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCtoiSyVVTO8"
      },
      "source": [
        "### Google Colab Setup\n",
        "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
        "\n",
        "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHG0slB6VTO8",
        "outputId": "99ac1322-d513-4386-dfa4-4c325cdd1a1b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWjXo-vXVTO_"
      },
      "source": [
        "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
        "\n",
        "```\n",
        "['pytorch_basic.py', 'pytorch_basic.ipynb']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqMvJnNHVTPA",
        "outputId": "c693ed33-294e-4e85-b02f-274a24b0f760"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
        "# Example: If you create a 2020FA folder and put all the files under A1 folder, then '2020FA/A1'\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2020FA/A1'\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'Colab/DEEP/HW01/Practicals'\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko-wLqHWVTPC"
      },
      "source": [
        "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
        "\n",
        "```\n",
        "Hello from pytorch_basic.py!\n",
        "```\n",
        "\n",
        "as well as the last edit time for the file `pytorch_basic.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AoThF9eVTPD",
        "outputId": "5ba216ca-e623-45db-f8f2-02ce4505bcf6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "import time, os\n",
        "os.environ[\"TZ\"] = \"'Asia/Tehran'\"\n",
        "time.tzset()\n",
        "\n",
        "from pytorch_basic import hello\n",
        "hello()\n",
        "\n",
        "pytorch_path = os.path.join(GOOGLE_DRIVE_PATH, 'pytorch_basic.py')\n",
        "edit_time = time.ctime(os.path.getmtime(pytorch_path))\n",
        "print('pytorch_basic.py last edited on %s' % edit_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc83ETI1a3o9"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Python 3 and [PyTorch](https://pytorch.org/) will be used throughout the semseter, so it is important to be familiar with them. This material in this notebook draws from http://cs231n.github.io/python-numpy-tutorial/ and https://github.com/kuleshov/cs228-material/blob/master/tutorials/python/cs228-python-tutorial.ipynb. This material focuses mainly on PyTorch.\n",
        "\n",
        "This notebook will walk you through many of the important features of PyTorch that you will need to use throughout the semester.\n",
        "\n",
        "When completing the notebook, please adhere to the following rules:\n",
        "- Do not write or modify any code outside of code blocks\n",
        "- Do not add or delete any cells from the notebook. You may add new cells to perform scatch work, but delete them before submitting.\n",
        "- Run all cells before submitting. You will only get credit for code that has been run.\n",
        "\n",
        "This notebook contains many inline sanity checks for the code you write. However, **passing these sanity checks does not mean your code is correct!** During grading we may run your code on additional inputs, and we may look at your code to make sure you've followed the specific guildelines for each implementation. You are encouraged to write additional test cases for the functions you are asked to write instead of solely relying on the sanity checks in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MEmHrgBsgX4"
      },
      "source": [
        "# PyTorch (70 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3e_Nux0siHo"
      },
      "source": [
        "[PyTorch](https://pytorch.org/) is an open source machine learning framework. At its core, PyTorch provides a few key features:\n",
        "\n",
        "- A multidimensional **Tensor** object, similar to [numpy](https://numpy.org/) but with GPU accelleration.\n",
        "- An optimized **autograd** engine for automatically computing derivatives\n",
        "- A clean, modular API for building and deploying **deep learning models**\n",
        "\n",
        "We will use PyTorch for all programming assignments throughout the semester. This notebook will focus on the **Tensor API**, as it is the main part of PyTorch that we will use for the first few assignments.\n",
        "\n",
        "You can find more information about PyTorch by following one of the [oficial tutorials](https://pytorch.org/tutorials/) or by [reading the documentation](https://pytorch.org/docs/stable/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdiO3_y-vKQ9"
      },
      "source": [
        "To use PyTorch, we first need to import the `torch` package.\n",
        "\n",
        "We also check the version; the assignments in this course will use PyTorch verion 1.6.0, since this is the default version in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sydFm14itrqq",
        "outputId": "ba7957ba-00b5-4e0c-84c7-8547e5ed7c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrBSx6hYu8ca"
      },
      "source": [
        "## Tensor Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWagwmXuvIle"
      },
      "source": [
        "### Creating and Accessing tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf_SY4RzvAh_"
      },
      "source": [
        "A `torch` **tensor** is a multidimensional grid of values, all of the same type, and is indexed by a tuple of nonnegative integers. The number of dimensions is the **rank** of the tensor; the **shape** of a tensor is a tuple of integers giving the size of the array along each dimension.\n",
        "\n",
        "We can initialize `torch` tensor from nested Python lists. We can access or mutate elements of a PyTorch tensor using square brackets.\n",
        "\n",
        "Accessing an element from a PyTorch tensor returns a PyTorch scalar; we can convert this to a Python scalar using the `.item()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpwfVUvPu_lF",
        "outputId": "bddb8624-7bc5-4bb1-fbf5-2851029f5a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is a:\n",
            "tensor([1, 2, 3])\n",
            "type(a):  <class 'torch.Tensor'>\n",
            "rank of a:  1\n",
            "a.shape:  torch.Size([3])\n",
            "\n",
            "a[0]:  tensor(1)\n",
            "type(a[0]):  <class 'torch.Tensor'>\n",
            "type(a[0].item()):  <class 'int'>\n",
            "\n",
            "a after mutating:\n",
            "tensor([ 1, 10,  3])\n"
          ]
        }
      ],
      "source": [
        "# Create a rank 1 tensor from a Python list\n",
        "a = torch.tensor([1, 2, 3])\n",
        "print('Here is a:')\n",
        "print(a)\n",
        "print('type(a): ', type(a))\n",
        "print('rank of a: ', a.dim())\n",
        "print('a.shape: ', a.shape)\n",
        "\n",
        "# Access elements using square brackets\n",
        "print()\n",
        "print('a[0]: ', a[0])\n",
        "print('type(a[0]): ', type(a[0]))\n",
        "print('type(a[0].item()): ', type(a[0].item()))\n",
        "\n",
        "# Mutate elements using square brackets\n",
        "a[1] = 10\n",
        "print()\n",
        "print('a after mutating:')\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZq4zsnLEgXH"
      },
      "source": [
        "The example above shows a one-dimensional tensor; we can similarly create tensors with two or more dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TcvHxpTFUcL",
        "outputId": "f647d6de-c1d6-4051-8001-53812561989b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is b:\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 5]])\n",
            "rank of b: 2\n",
            "b.shape:  torch.Size([2, 3])\n",
            "\n",
            "b[0, 1]: tensor(2)\n",
            "b[1, 2]: tensor(5)\n",
            "\n",
            "b after mutating:\n",
            "tensor([[  1,   2,   3],\n",
            "        [  4, 100,   5]])\n"
          ]
        }
      ],
      "source": [
        "# Create a two-dimensional tensor\n",
        "b = torch.tensor([[1, 2, 3], [4, 5, 5]])\n",
        "print('Here is b:')\n",
        "print(b)\n",
        "print('rank of b:', b.dim())\n",
        "print('b.shape: ', b.shape)\n",
        "\n",
        "# Access elements from a multidimensional tensor\n",
        "print()\n",
        "print('b[0, 1]:', b[0, 1])\n",
        "print('b[1, 2]:', b[1, 2])\n",
        "\n",
        "# Mutate elements of a multidimensional tensor\n",
        "b[1, 1] = 100\n",
        "print()\n",
        "print('b after mutating:')\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBOsvh53GXa8"
      },
      "source": [
        "Now it's **your turn**. In the file `pytorch_basic.py`, complete the implementation of the functions `create_sample_tensor`, `mutate_tensor`, and `count_tensor_elements` to practice constructing, mutating, and thinking about the shapes of tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjCIUzbaVTPs",
        "outputId": "fb64d90c-380d-4bf4-d0cf-60fc32a0d415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is the sample tensor:\n",
            "tensor([[  0,  10],\n",
            "        [100,   0],\n",
            "        [  0,   0]])\n",
            "\n",
            "After mutating:\n",
            "tensor([[ 4, 10],\n",
            "        [ 5,  6],\n",
            "        [ 0,  0]])\n",
            "\n",
            "Correct shape:  True\n",
            "x[0, 0] correct:  True\n",
            "x[1, 0] correct:  True\n",
            "x[1, 1] correct:  True\n",
            "\n",
            "Number of elements in x:  6\n",
            "Correctly counted:  True\n"
          ]
        }
      ],
      "source": [
        "from pytorch_basic import create_sample_tensor, mutate_tensor, count_tensor_elements\n",
        "\n",
        "# Create a sample tensor\n",
        "x = create_sample_tensor()\n",
        "print('Here is the sample tensor:')\n",
        "print(x)\n",
        "\n",
        "# Mutate the tensor by setting a few elements\n",
        "indices = [(0, 0), (1, 0), (1, 1)]\n",
        "values = [4, 5, 6]\n",
        "mutate_tensor(x, indices, values)\n",
        "print('\\nAfter mutating:')\n",
        "print(x)\n",
        "print('\\nCorrect shape: ', x.shape == (3, 2))\n",
        "print('x[0, 0] correct: ', x[0, 0].item() == 4)\n",
        "print('x[1, 0] correct: ', x[1, 0].item() == 5)\n",
        "print('x[1, 1] correct: ', x[1, 1].item() == 6)\n",
        "\n",
        "# Check the number of elements in the sample tensor\n",
        "num = count_tensor_elements(x)\n",
        "print('\\nNumber of elements in x: ', num)\n",
        "print('Correctly counted: ', num == 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yz_VDA3IvP33"
      },
      "source": [
        "### Tensor constructors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoAlslEdwV-k"
      },
      "source": [
        "PyTorch provides many convenience methods for constructing tensors; this avoids the need to use Python lists. For example:\n",
        "\n",
        "- [`torch.zeros`](https://pytorch.org/docs/1.1.0/torch.html#torch.zeros): Creates a tensor of all zeros\n",
        "- [`torch.ones`](https://pytorch.org/docs/1.1.0/torch.html#torch.ones): Creates a tensor of all ones\n",
        "- [`torch.rand`](https://pytorch.org/docs/1.1.0/torch.html#torch.rand): Creates a tensor with uniform random numbers\n",
        "\n",
        "You can find a full list of tensor creation operations [in the documentation](https://pytorch.org/docs/stable/torch.html#creation-ops)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FL6DXGXzxHBA",
        "outputId": "63cf1200-3a2f-488d-82a5-387c00a5699e"
      },
      "outputs": [],
      "source": [
        "# Create a tensor of all zeros\n",
        "a = torch.zeros(2, 3)\n",
        "print('tensor of zeros:')\n",
        "print(a)\n",
        "\n",
        "# Create a tensor of all ones\n",
        "b = torch.ones(1, 2)\n",
        "print('\\ntensor of ones:')\n",
        "print(b)\n",
        "\n",
        "# Create a 3x3 identity matrix\n",
        "c = torch.eye(3)\n",
        "print('\\nidentity matrix:')\n",
        "print(c)\n",
        "\n",
        "# Tensor of random values\n",
        "d = torch.rand(4, 5)\n",
        "print('\\nrandom tensor:')\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9QuvWYxMsoK"
      },
      "source": [
        "**Your turn**: In the file `pytorch_basic.py`, complete the implementation of `create_tensor_of_pi` to practice using a tensor constructor.\n",
        "\n",
        "Hint: [`torch.full`](https://pytorch.org/docs/stable/generated/torch.full.html#torch.full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_y7Z5I0NIaA",
        "outputId": "99e19caa-33ca-4671-e983-e48ead9d6530"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import create_tensor_of_pi\n",
        "\n",
        "x = create_tensor_of_pi(4, 5)\n",
        "\n",
        "print('x is a tensor:', torch.is_tensor(x))\n",
        "print('x has correct shape: ', x.shape == (4, 5))\n",
        "print('x is filled with sevens: ', (x == 3.14).all().item() == 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz_hiJD33fu1"
      },
      "source": [
        "### Datatypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG1xBunZ3ixx"
      },
      "source": [
        "In the examples above, you may have noticed that some of our tensors contained floating-point values, while others contained integer values.\n",
        "\n",
        "PyTorch provides a [large set of numeric datatypes](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype) that you can use to construct tensors. PyTorch tries to guess a datatype when you create a tensor; functions that construct tensors typically have a `dtype` argument that you can use to explicitly specify a datatype.\n",
        "\n",
        "Each tensor has a `dtype` attribute that you can use to check its data type:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vREVDf_n31Qz",
        "outputId": "be25f3bc-6069-4d38-f13e-3a8a8c89ec31"
      },
      "outputs": [],
      "source": [
        "# Let torch choose the datatype\n",
        "x0 = torch.tensor([1, 2])   # List of integers\n",
        "x1 = torch.tensor([1., 2.]) # List of floats\n",
        "x2 = torch.tensor([1., 2])  # Mixed list\n",
        "print('dtype when torch chooses for us:')\n",
        "print('List of integers:', x0.dtype)\n",
        "print('List of floats:', x1.dtype)\n",
        "print('Mixed list:', x2.dtype)\n",
        "\n",
        "# Force a particular datatype\n",
        "y0 = torch.tensor([1, 2], dtype=torch.float32)  # 32-bit float\n",
        "y1 = torch.tensor([1, 2], dtype=torch.int32)    # 32-bit (signed) integer\n",
        "y2 = torch.tensor([1, 2], dtype=torch.int64)    # 64-bit (signed) integer\n",
        "print('\\ndtype when we force a datatype:')\n",
        "print('32-bit float: ', y0.dtype)\n",
        "print('32-bit integer: ', y1.dtype)\n",
        "print('64-bit integer: ', y2.dtype)\n",
        "\n",
        "# Other creation ops also take a dtype argument\n",
        "z0 = torch.ones(1, 2)  # Let torch choose for us\n",
        "z1 = torch.ones(1, 2, dtype=torch.int16) # 16-bit (signed) integer\n",
        "z2 = torch.ones(1, 2, dtype=torch.uint8) # 8-bit (unsigned) integer\n",
        "print('\\ntorch.ones with different dtypes')\n",
        "print('default dtype:', z0.dtype)\n",
        "print('16-bit integer:', z1.dtype)\n",
        "print('8-bit unsigned integer:', z2.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2reBgQmx_x4"
      },
      "source": [
        "We can **cast** a tensor to another datatype using the [`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to) method; there are also convenience methods like [`.float()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.float) and [`.long()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.long) that cast to particular datatypes:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAMpwGsdyHAw",
        "outputId": "d867eeb2-4692-4a81-d2ec-e7dca28f0c76"
      },
      "outputs": [],
      "source": [
        "x0 = torch.eye(3, dtype=torch.int64)\n",
        "x1 = x0.float()  # Cast to 32-bit float\n",
        "x2 = x0.double() # Cast to 64-bit float\n",
        "x3 = x0.to(torch.float32) # Alternate way to cast to 32-bit float\n",
        "x4 = x0.to(torch.float64) # Alternate way to cast to 64-bit float\n",
        "print('x0:', x0.dtype)\n",
        "print('x1:', x1.dtype)\n",
        "print('x2:', x2.dtype)\n",
        "print('x3:', x3.dtype)\n",
        "print('x4:', x4.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2O8Atl1wMB7"
      },
      "source": [
        "PyTorch provides several ways to create a tensor with the same datatype as another tensor:\n",
        "\n",
        "- PyTorch provides tensor constructors such as [`torch.zeros_like()`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like) that create new tensors with the same shape and type as a given tensor\n",
        "- Tensor objects have instance methods such as [`.new_zeros()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.new_zeros) that create tensors the same type but possibly different shapes\n",
        "- The tensor instance method [`.to()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to) can take a tensor as an argument, in which case it casts to the datatype of the argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1APDsx54xV6p",
        "outputId": "43e32391-e139-4d98-ed45-59ea91c66136"
      },
      "outputs": [],
      "source": [
        "x0 = torch.eye(3, dtype=torch.float64)  # Shape (3, 3), dtype torch.float64\n",
        "x1 = torch.zeros_like(x0)               # Shape (3, 3), dtype torch.float64\n",
        "x2 = x0.new_zeros(4, 5)                 # Shape (4, 5), dtype torch.float64\n",
        "x3 = torch.ones(6, 7).to(x0)            # Shape (6, 7), dtype torch.float64)\n",
        "print('x0 shape is %r, dtype is %r' % (x0.shape, x0.dtype))\n",
        "print('x1 shape is %r, dtype is %r' % (x1.shape, x1.dtype))\n",
        "print('x2 shape is %r, dtype is %r' % (x2.shape, x2.dtype))\n",
        "print('x3 shape is %r, dtype is %r' % (x3.shape, x3.dtype))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPuGPa0v4h_2"
      },
      "source": [
        "**Your turn**: In the file `pytorch_basic.py`, implement the function `multiples_of_ten` which should create and return a tensor of dtype `torch.float64` containing all the multiples of ten in a given range.\n",
        "\n",
        "Hint: [`torch.arange`](https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qddo6C5Bgwcr",
        "outputId": "3e8b092e-b08a-46c3-d70d-4d3c8d84db8a"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import multiples_of_ten\n",
        "\n",
        "start = 5\n",
        "stop = 25\n",
        "x = multiples_of_ten(start, stop)\n",
        "print('Correct dtype: ', x.dtype == torch.float64)\n",
        "print('Correct shape: ', x.shape == (2,))\n",
        "print('Correct values: ', x.tolist() == [10, 20])\n",
        "\n",
        "# If there are no multiples of ten in the given range you should return an empty tensor\n",
        "start = 5\n",
        "stop = 7\n",
        "x = multiples_of_ten(start, stop)\n",
        "print('\\nCorrect dtype: ', x.dtype == torch.float64)\n",
        "print('Correct shape: ', x.shape == (0,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwJL3HVySvXn"
      },
      "source": [
        "Even though PyTorch provides a large number of numeric datatypes, the most commonly used datatypes are:\n",
        "\n",
        "- `torch.float32`: Standard floating-point type; used to store learnable parameters, network activations, etc. Nearly all arithmetic is done using this type.\n",
        "- `torch.int64`: Typically used to store indices\n",
        "- `torch.bool`: Stores boolean values: 0 is false and 1 is true\n",
        "- `torch.float16`: Used for mixed-precision arithmetic, usually on NVIDIA GPUs with [tensor cores](https://www.nvidia.com/en-us/data-center/tensorcore/). You won't need to worry about this datatype in this course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlANfnILvX3S"
      },
      "source": [
        "## Tensor indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP4dRrHhyLO5"
      },
      "source": [
        "We have already seen how to get and set individual elements of PyTorch tensors. PyTorch also provides many other ways of indexing into tensors. Getting comfortable with these different options makes it easy to modify different parts of tensors with ease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo-PoTWNvbba"
      },
      "source": [
        "### Slice indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUqTYvglyVLc"
      },
      "source": [
        "Similar to Python lists and numpy arrays, PyTorch tensors can be **sliced** using the syntax `start:stop` or `start:stop:step`. The `stop` index is always non-inclusive: it is the first element not to be included in the slice.\n",
        "\n",
        "Start and stop indices can be negative, in which case they count backward from the end of the tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEr5BzdUdCtZ",
        "outputId": "16785f0e-22c6-416f-b0b1-85f12cc3d7d1"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([0, 11, 22, 33, 44, 55, 66])\n",
        "print(0, a)        # (0) Original tensor\n",
        "print(1, a[2:5])   # (1) Elements between index 2 and 5\n",
        "print(2, a[2:])    # (2) Elements after index 2\n",
        "print(3, a[:5])    # (3) Elements before index 5\n",
        "print(4, a[:])     # (4) All elements\n",
        "print(5, a[1:5:2]) # (5) Every second element between indices 1 and 5\n",
        "print(6, a[:-1])   # (6) All but the last element\n",
        "print(7, a[-4::2]) # (7) Every second element, starting from the fourth-last"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrcr9PojgTS1"
      },
      "source": [
        "For multidimensional tensors, you can provide a slice or integer for each dimension of the tensor in order to extract different types of subtensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5fOdjTUyhNf",
        "outputId": "fd9ba9c6-b44c-4238-e488-40085d226389"
      },
      "outputs": [],
      "source": [
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "# [[ 1  2  3  4]\n",
        "#  [ 5  6  7  8]\n",
        "#  [ 9 10 11 12]]\n",
        "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "print('shape: ', a.shape)\n",
        "\n",
        "# Get row 1, and all columns.\n",
        "print('\\nSingle row:')\n",
        "print(a[1, :])\n",
        "print(a[1])  # Gives the same result; we can omit : for trailing dimensions\n",
        "print('shape: ', a[1].shape)\n",
        "\n",
        "print('\\nSingle column:')\n",
        "print(a[:, 1])\n",
        "print('shape: ', a[:, 1].shape)\n",
        "\n",
        "# Get the first two rows and the last three columns\n",
        "print('\\nFirst two rows, last two columns:')\n",
        "print(a[:2, -3:])\n",
        "print('shape: ', a[:2, -3:].shape)\n",
        "\n",
        "# Get every other row, and columns at index 1 and 2\n",
        "print('\\nEvery other row, middle columns:')\n",
        "print(a[::2, 1:3])\n",
        "print('shape: ', a[::2, 1:3].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOsR8Pdertku"
      },
      "source": [
        "There are two common ways to access a single row or column of a tensor: using an integer will reduce the rank by one, and using a length-one slice will keep the same rank. Note that this is different behavior from MATLAB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1kHcc5jsF-c",
        "outputId": "25f4f649-c755-40a0-b085-42dec1216658"
      },
      "outputs": [],
      "source": [
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "print('Original tensor')\n",
        "print(a)\n",
        "\n",
        "row_r1 = a[1, :]    # Rank 1 view of the second row of a\n",
        "row_r2 = a[1:2, :]  # Rank 2 view of the second row of a\n",
        "print('\\nTwo ways of accessing a single row:')\n",
        "print(row_r1, row_r1.shape)\n",
        "print(row_r2, row_r2.shape)\n",
        "\n",
        "# We can make the same distinction when accessing columns::\n",
        "col_r1 = a[:, 1]\n",
        "col_r2 = a[:, 1:2]\n",
        "print('\\nTwo ways of accessing a single column:')\n",
        "print(col_r1, col_r1.shape)\n",
        "print(col_r2, col_r2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk625fJfyxV8"
      },
      "source": [
        "Slicing a tensor returns a **view** into the same data, so modifying it will also modify the original tensor. To avoid this, you can use the `clone()` method to make a copy of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXbikYPwyxGA",
        "outputId": "ef3cf399-7d59-4e3e-fc33-65b6e842ee9f"
      },
      "outputs": [],
      "source": [
        "# Create a tensor, a slice, and a clone of a slice\n",
        "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "b = a[0, 1:]\n",
        "c = a[0, 1:].clone()\n",
        "print('Before mutating:')\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "\n",
        "a[0, 1] = 20  # a[0, 1] and b[0] point to the same element\n",
        "b[1] = 30     # b[1] and a[0, 2] point to the same element\n",
        "c[2] = 40     # c is a clone, so it has its own data\n",
        "print('\\nAfter mutating:')\n",
        "print(a)\n",
        "print(b)\n",
        "print(c)\n",
        "\n",
        "print(a.storage().data_ptr() == c.storage().data_ptr())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t5omyKwm9dB"
      },
      "source": [
        "**Your turn**: In the file `pytorch_basic.py`, implement the function `slice_indexing_practice` to practice indexing tensors with different types of slices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKq2mswvqMmw"
      },
      "outputs": [],
      "source": [
        "# We will use this helper function to check your results\n",
        "def check(orig, actual, expected):\n",
        "    if not torch.is_tensor(actual):\n",
        "        return False\n",
        "    expected = torch.tensor(expected)\n",
        "    same_elements = (actual == expected).all().item()\n",
        "    same_storage = (orig.storage().data_ptr() == actual.storage().data_ptr())\n",
        "    return same_elements and same_storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-5UtVXPVTQL",
        "outputId": "13c18e78-19fb-40bc-b02c-c2ced975d0e0"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import slice_indexing_practice\n",
        "\n",
        "# Create the following rank 2 tensor of shape (3, 5)\n",
        "# [[ 1  2  3  4  5]\n",
        "#  [ 6  7  8  9 10]\n",
        "#  [11 12 13 14 15]]\n",
        "x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 8, 10], [11, 12, 13, 14, 15]])\n",
        "out = slice_indexing_practice(x)\n",
        "\n",
        "last_row = out[0]\n",
        "print('last_row:')\n",
        "print(last_row)\n",
        "correct = check(x, last_row, [11, 12, 13, 14, 15])\n",
        "print('Correct: %r\\n' % correct)\n",
        "\n",
        "third_col = out[1]\n",
        "print('third_col:')\n",
        "print(third_col)\n",
        "correct = check(x, third_col, [[3], [8], [13]])\n",
        "print('Correct: %r\\n' % correct)\n",
        "\n",
        "first_two_rows_three_cols = out[2]\n",
        "print('first_two_rows_three_cols:')\n",
        "print(first_two_rows_three_cols)\n",
        "correct = check(x, first_two_rows_three_cols, [[1, 2, 3], [6, 7, 8]])\n",
        "print('Correct: %r\\n' % correct)\n",
        "\n",
        "even_rows_odd_cols = out[3]\n",
        "print('even_rows_odd_cols:')\n",
        "print(even_rows_odd_cols)\n",
        "correct = check(x, even_rows_odd_cols, [[2, 4], [12, 14]])\n",
        "print('Correct: %r\\n' % correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNjhLwb0xY2A"
      },
      "source": [
        "So far we have used slicing to **access** subtensors; we can also use slicing to **modify** subtensors by writing assignment expressions where the left-hand side is a slice expression, and the right-hand side is a constant or a tensor of the correct shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFnky42Rx2I5",
        "outputId": "981417be-32e1-468d-ab01-2a83826e22e1"
      },
      "outputs": [],
      "source": [
        "a = torch.zeros(2, 4, dtype=torch.int64)\n",
        "a[:, :2] = 1\n",
        "a[:, 2:] = torch.tensor([[2, 3], [4, 5]])\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPVCQ5HszihV"
      },
      "source": [
        "**Your turn**: in the file `pytorch_basic.py`, implement the function `slice_assignment_practice` to practice modifying tensors with slicing assignment statements.\n",
        "\n",
        "This function should use slicing assignment operations to modify the first four rows and first six columns of the input tensor so they are equal to\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "0 & 1 & 2 & 2 & 2 & 2 \\\\\n",
        "0 & 1 & 2 & 2 & 2 & 2 \\\\\n",
        "3 & 4 & 3 & 4 & 5 & 5 \\\\\n",
        "3 & 4 & 3 & 4 & 5 & 5 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Your implementation must obey the following:\n",
        "- You should mutate the tensor x in-place and return it\n",
        "- You should only modify the first 4 rows and first 6 columns; all other\n",
        "elements should remain unchanged\n",
        "- You may only mutate the tensor using slice assignment operations, where you\n",
        "assign an integer to a slice of the tensor\n",
        "- You must use <= 6 slicing operations to achieve the desired result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzXlnFqAVTQQ",
        "outputId": "f7e75692-ab5a-4383-b51c-8252b08ec855"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import slice_assignment_practice\n",
        "\n",
        "x = torch.zeros(5, 7, dtype=torch.int64)\n",
        "print('Here is x before calling slice_assignment_practice:')\n",
        "print(x)\n",
        "slice_assignment_practice(x)\n",
        "print('Here is x after calling slice assignment practice:')\n",
        "print(x)\n",
        "\n",
        "expected = [\n",
        "    [0, 1, 2, 2, 2, 2, 0],\n",
        "    [0, 1, 2, 2, 2, 2, 0],\n",
        "    [3, 4, 3, 4, 5, 5, 0],\n",
        "    [3, 4, 3, 4, 5, 5, 0],\n",
        "    [0, 0, 0, 0, 0, 0, 0],\n",
        "]\n",
        "print('Correct: ', x.tolist() == expected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y93rPhGveWw"
      },
      "source": [
        "### Integer tensor indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlTyhjEN0AIE"
      },
      "source": [
        "When you index into torch tensor using slicing, the resulting tensor view will always be a subarray of the original tensor. This is powerful, but can be restrictive.\n",
        "\n",
        "We can also use **index arrays** to index tensors; this lets us construct new tensors with a lot more flexibility than using slices.\n",
        "\n",
        "As an example, we can use index arrays to reorder the rows or columns of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXePPNkjM_SD",
        "outputId": "66a5e598-02c3-43b2-f17e-44e11ddabdbc"
      },
      "outputs": [],
      "source": [
        "# Create the following rank 2 tensor with shape (3, 4)\n",
        "# [[ 1  2  3  4]\n",
        "#  [ 5  6  7  8]\n",
        "#  [ 9 10 11 12]]\n",
        "a = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# Create a new tensor of shape (5, 4) by reordering rows from a:\n",
        "# - First two rows same as the first row of a\n",
        "# - Third row is the same as the last row of a\n",
        "# - Fourth and fifth rows are the same as the second row from a\n",
        "idx = [0, 0, 2, 1, 1]  # index arrays can be Python lists of integers\n",
        "print('\\nReordered rows:')\n",
        "print(a[idx])\n",
        "\n",
        "# Create a new tensor of shape (3, 4) by reversing the columns from a\n",
        "idx = torch.tensor([3, 2, 1, 0])  # Index arrays can be int64 torch tensors\n",
        "print('\\nReordered columns:')\n",
        "print(a[:, idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpIBR1bCQji6"
      },
      "source": [
        "More generally, given index arrays `idx0` and `idx1` with `N` elements each, `a[idx0, idx1]` is equivalent to:\n",
        "\n",
        "```\n",
        "torch.tensor([\n",
        "  a[idx0[0], idx1[0]],\n",
        "  a[idx0[1], idx1[1]],\n",
        "  ...,\n",
        "  a[idx0[N - 1], idx1[N - 1]]\n",
        "])\n",
        "```\n",
        "\n",
        "(A similar pattern extends to tensors with more than two dimensions)\n",
        "\n",
        "We can for example use this to get or set the diagonal of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocIR8R5ZSEaP",
        "outputId": "493b88d2-e4a7-4eb2-d4ed-3b697ac8f6f1"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "idx = [0, 1, 2]\n",
        "print('\\nGet the diagonal:')\n",
        "print(a[idx, idx])\n",
        "\n",
        "# Modify the diagonal\n",
        "a[idx, idx] = torch.tensor([11, 22, 33])\n",
        "print('\\nAfter setting the diagonal:')\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-cr-EqA0vfO"
      },
      "source": [
        "One useful trick with integer array indexing is selecting or mutating one element from each row or column of a matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWA8E8iI0x17",
        "outputId": "06105af3-d0aa-42ac-cb56-b76ec880be1d"
      },
      "outputs": [],
      "source": [
        "# Create a new tensor from which we will select elements\n",
        "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# Take on element from each row of a:\n",
        "# from row 0, take element 1;\n",
        "# from row 1, take element 2;\n",
        "# from row 2, take element 1;\n",
        "# from row 3, take element 0\n",
        "idx0 = torch.arange(a.shape[0])  # Quick way to build [0, 1, 2, 3]\n",
        "idx1 = torch.tensor([1, 2, 1, 0])\n",
        "print('\\nSelect one element from each row:')\n",
        "print(a[idx0, idx1])\n",
        "\n",
        "# Now set each of those elements to zero\n",
        "a[idx0, idx1] = 0\n",
        "print('\\nAfter modifying one element from each row:')\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5_-WUmSVEoR"
      },
      "source": [
        "**Your turn**: in the file `pytorch_basic.py`, implement the functions `shuffle_cols`, `reverse_rows`, and `take_one_elem_per_col` to practice using integer indexing to manipulate tensors. In each of these functions, your implementation should construct the output tensor **using a single indexing operation on the input**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX05_ov5VTQZ",
        "outputId": "e83f2d4f-d7bb-45cc-c8e9-2c64d29883aa"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import shuffle_cols, reverse_rows, take_one_elem_per_col\n",
        "\n",
        "# Build a tensor of shape (4, 3):\n",
        "# [[ 1,  2,  3],\n",
        "#  [ 4,  5,  6],\n",
        "#  [ 7,  8,  9],\n",
        "#  [10, 11, 12]]\n",
        "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
        "print('Here is x:')\n",
        "print(x)\n",
        "\n",
        "y1 = shuffle_cols(x)\n",
        "print('\\nHere is shuffle_cols(x):')\n",
        "print(y1)\n",
        "expected = [[1, 1, 3, 2], [4, 4, 6, 5], [7, 7, 9, 8], [10, 10, 12, 11]]\n",
        "y1_correct = torch.is_tensor(y1) and y1.tolist() == expected\n",
        "print('Correct: %r\\n' % y1_correct)\n",
        "\n",
        "y2 = reverse_rows(x)\n",
        "print('Here is reverse_rows(x):')\n",
        "print(y2)\n",
        "expected = [[10, 11, 12], [7, 8, 9], [4, 5, 6], [1, 2, 3]]\n",
        "y2_correct = torch.is_tensor(y2) and y2.tolist() == expected\n",
        "print('Correct: %r\\n' % y2_correct)\n",
        "\n",
        "y3 = take_one_elem_per_col(x)\n",
        "print('Here is take_one_elem_per_col(x):')\n",
        "print(y3)\n",
        "expected = [4, 2, 12]\n",
        "y3_correct = torch.is_tensor(y3) and y3.tolist() == expected\n",
        "print('Correct: %r' % y3_correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGt8ZPb_vixw"
      },
      "source": [
        "### Boolean tensor indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CkQaRj01xmU"
      },
      "source": [
        "Boolean tensor indexing lets you pick out arbitrary elements of a tensor according to a boolean mask. Frequently this type of indexing is used to select or modify the elements of a tensor that satisfy some condition.\n",
        "\n",
        "In PyTorch, we use tensors of dtype `torch.bool` to hold boolean masks.\n",
        "\n",
        "(Prior to version 1.2.0, there was no `torch.bool` type so instead `torch.uint8` was usually used to represent boolean data, with 0 indicating false and 1 indicating true. Watch out for this in older PyTorch code!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29Zf7rb82Dkd",
        "outputId": "fd9c18c0-0012-4fc0-cc25-84f0dd176b20"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([[1,2], [3, 4], [5, 6]])\n",
        "print('Original tensor:')\n",
        "print(a)\n",
        "\n",
        "# Find the elements of a that are bigger than 3. The mask has the same shape as\n",
        "# a, where each element of mask tells whether the corresponding element of a\n",
        "# is greater than three.\n",
        "mask = (a > 3)\n",
        "print('\\nMask tensor:')\n",
        "print(mask)\n",
        "\n",
        "# We can use the mask to construct a rank-1 tensor containing the elements of a\n",
        "# that are selected by the mask\n",
        "print('\\nSelecting elements with the mask:')\n",
        "print(a[mask])\n",
        "\n",
        "# We can also use boolean masks to modify tensors; for example this sets all\n",
        "# elements <= 3 to zero:\n",
        "a[a <= 3] = 0\n",
        "print('\\nAfter modifying with a mask:')\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtSmmMGodrTX"
      },
      "source": [
        "**Your turn**: In the file `pytorch_basic.py`, implement the function `count_negative_entries` which counts the number of negative entries in a torch tensor. You can easily accomplish this using boolean tensor indexing. Your implementation should perform only a single indexing operation on the input tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hkeYXN9d5xh",
        "outputId": "5a996c6d-8d70-4543-e0f9-2c6e1f6ffa8c"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import count_negative_entries\n",
        "\n",
        "# Make a few test cases\n",
        "torch.manual_seed(598)\n",
        "x0 = torch.tensor([[-1, -1, 0], [0, 1, 2], [3, 4, 5]])\n",
        "x1 = torch.tensor([0, 1, 2, 3])\n",
        "x2 = torch.randn(100, 100)\n",
        "print('Correct for x0: ', count_negative_entries(x0) == 2)\n",
        "print('Correct for x1: ', count_negative_entries(x1) == 0)\n",
        "print('Correct for x2: ', count_negative_entries(x2) == 4984)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q68ZApgH31W9"
      },
      "source": [
        "Now implement the function `make_one_hot` that creates a matrix of **one-hot vectors** from a list of Python integers.\n",
        "\n",
        "A one-hot vector for an integer $n$ is a vector that has a one in its $n$th slot, and zeros in all other slots. One-hot vectors are commonly used to represent categorical variables in machine learning models.\n",
        "\n",
        "For example, given a list `[1, 4, 3, 2]` of integers, your function should produce the tensor:\n",
        "\n",
        "```\n",
        "[[0 1 0 0 0],\n",
        " [0 0 0 0 1],\n",
        " [0 0 0 1 0],\n",
        " [0 0 1 0 0]]\n",
        "```\n",
        "\n",
        "Here the first row corresponds to the first element of the list: it has a one at index 1, and zeros at all other indices. The second row corresponds to the second element of the list: it has a one at index 4, and zeros at all other indices. The other rows follow the same pattern. The output has just enough columns so that none of the rows go out-of-bounds: the largest index in the input is 4, so the output matrix has 5 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaT1kuQ37Rsq",
        "outputId": "89f21348-8a8e-49d4-f4c0-0146abf198ea"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import make_one_hot\n",
        "\n",
        "def check_one_hot(x, y):\n",
        "    C = y.shape[1]\n",
        "    for i, n in enumerate(x):\n",
        "        if n >= C: return False\n",
        "        for j in range(C):\n",
        "            expected = 1.0 if j == n else 0.0\n",
        "            if y[i, j].item() != expected: return False\n",
        "        return True\n",
        "\n",
        "x0 = [1, 4, 3, 2]\n",
        "y0 = make_one_hot(x0)\n",
        "print('Here is y0:')\n",
        "print(y0)\n",
        "print('y0 correct: ', check_one_hot(x0, y0))\n",
        "\n",
        "x1 = [1, 3, 5, 7, 6, 2]\n",
        "y1 = make_one_hot(x1)\n",
        "print('\\nHere is y1:')\n",
        "print(y1)\n",
        "print('y1 correct: ', check_one_hot(x1, y1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-xqELwyqpN"
      },
      "source": [
        "## Reshaping operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql9_eXuU4OG8"
      },
      "source": [
        "### View"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfPb_2BY0HKw"
      },
      "source": [
        "PyTorch provides many ways to manipulate the shapes of tensors. The simplest example is [`.view()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.view): This returns a new tensor with the same number of elements as its input, but with a different shape.\n",
        "\n",
        "We can use `.view()` to flatten matrices into vectors, and to convert rank-1 vectors into rank-2 row or column matrices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kw-M7C_61FZK",
        "outputId": "c1157785-9e78-4507-ebf6-2970e4b4d6be"
      },
      "outputs": [],
      "source": [
        "x0 = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "print('Original tensor:')\n",
        "print(x0)\n",
        "print('shape:', x0.shape)\n",
        "\n",
        "# Flatten x0 into a rank 1 vector of shape (8,)\n",
        "x1 = x0.view(8)\n",
        "print('\\nFlattened tensor:')\n",
        "print(x1)\n",
        "print('shape:', x1.shape)\n",
        "\n",
        "# Convert x1 to a rank 2 \"row vector\" of shape (1, 8)\n",
        "x2 = x1.view(1, 8)\n",
        "print('\\nRow vector:')\n",
        "print(x2)\n",
        "print('shape:', x2.shape)\n",
        "\n",
        "# Convert x1 to a rank 2 \"column vector\" of shape (8, 1)\n",
        "x3 = x1.view(8, 1)\n",
        "print('\\nColumn vector:')\n",
        "print(x3)\n",
        "print('shape:', x3.shape)\n",
        "\n",
        "# Convert x1 to a rank 3 tensor of shape (2, 2, 2):\n",
        "x4 = x1.view(2, 2, 2)\n",
        "print('\\nRank 3 tensor:')\n",
        "print(x4)\n",
        "print('shape:', x4.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHsZ8BPF2PEq"
      },
      "source": [
        "As a convenience, calls to `.view()` may include a single -1 argument; this puts enough elements on that dimension so that the output has the same shape as the input. This makes it easy to write some reshape operations in a way that is agnostic to the shape of the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNWu-R_J2qFY",
        "outputId": "2277daa0-1e58-4ca7-8f10-b22212d2af61"
      },
      "outputs": [],
      "source": [
        "# We can reuse these functions for tensors of different shapes\n",
        "def flatten(x):\n",
        "    return x.view(-1)\n",
        "\n",
        "def make_row_vec(x):\n",
        "    return x.view(1, -1)\n",
        "\n",
        "x0 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "x0_flat = flatten(x0)\n",
        "x0_row = make_row_vec(x0)\n",
        "print('x0:')\n",
        "print(x0)\n",
        "print('x0_flat:')\n",
        "print(x0_flat)\n",
        "print('x0_row:')\n",
        "print(x0_row)\n",
        "\n",
        "x1 = torch.tensor([[1, 2], [3, 4]])\n",
        "x1_flat = flatten(x1)\n",
        "x1_row = make_row_vec(x1)\n",
        "print('\\nx1:')\n",
        "print(x1)\n",
        "print('x1_flat:')\n",
        "print(x1_flat)\n",
        "print('x1_row:')\n",
        "print(x1_row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK-ZB5aB2NPq"
      },
      "source": [
        "As its name implies, a tensor returned by `.view()` shares the same data as the input, so changes to one will affect the other and vice-versa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebT99rUo2McN",
        "outputId": "acaaa941-1e7c-42ae-9432-a499ce1ca9bb"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "x_flat = x.view(-1)\n",
        "print('x before modifying:')\n",
        "print(x)\n",
        "print('x_flat before modifying:')\n",
        "print(x_flat)\n",
        "\n",
        "x[0, 0] = 10   # x[0, 0] and x_flat[0] point to the same data\n",
        "x_flat[1] = 20 # x_flat[1] and x[0, 1] point to the same data\n",
        "\n",
        "print('\\nx after modifying:')\n",
        "print(x)\n",
        "print('x_flat after modifying:')\n",
        "print(x_flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z150qBob4Wkz"
      },
      "source": [
        "### Swapping axes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMDxbyBys78"
      },
      "source": [
        "Another common reshape operation you might want to perform is transposing a matrix. You might be surprised if you try to transpose a matrix with `.view()`: The `view()` function takes elements in row-major order, so **you cannot transpose matrices with `.view()`**.\n",
        "\n",
        "In general, you should only use `.view()` to add new dimensions to a tensor, or to collapse adjacent dimensions of a tensor.\n",
        "\n",
        "For other types of reshape operations, you usually need to use a function that can swap axes of a tensor. The simplest such function is `.t()`, specificially for transposing matrices. It is available both as a [function in the `torch` module](https://pytorch.org/docs/stable/generated/torch.t.html#torch.t), and as a [tensor instance method](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.t):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_B4NuX6zQm-",
        "outputId": "ee210978-64b0-4dcf-e02e-cb7cd0586859"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print('Original matrix:')\n",
        "print(x)\n",
        "print('\\nTransposing with view DOES NOT WORK!')\n",
        "print(x.view(3, 2))\n",
        "print('\\nTransposed matrix:')\n",
        "print(torch.t(x))\n",
        "print(x.t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN93xo98zn0v"
      },
      "source": [
        "For tensors with more than two dimensions, we can use the function [`torch.transpose`](https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose) to swap arbitrary dimensions, or the [`.permute`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute) method to arbitrarily permute dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgN7YB8YzzkA",
        "outputId": "ae7685ad-1976-48e0-c988-09e6feb8b7e8"
      },
      "outputs": [],
      "source": [
        "# Create a tensor of shape (2, 3, 4)\n",
        "x0 = torch.tensor([\n",
        "     [[1,  2,  3,  4],\n",
        "      [5,  6,  7,  8],\n",
        "      [9, 10, 11, 12]],\n",
        "     [[13, 14, 15, 16],\n",
        "      [17, 18, 19, 20],\n",
        "      [21, 22, 23, 24]]])\n",
        "print('Original tensor:')\n",
        "print(x0)\n",
        "print('shape:', x0.shape)\n",
        "\n",
        "# Swap axes 1 and 2; shape is (2, 4, 3)\n",
        "x1 = x0.transpose(1, 2)\n",
        "print('\\nSwap axes 1 and 2:')\n",
        "print(x1)\n",
        "print(x1.shape)\n",
        "\n",
        "# Permute axes; the argument (1, 2, 0) means:\n",
        "# - Make the old dimension 1 appear at dimension 0;\n",
        "# - Make the old dimension 2 appear at dimension 1;\n",
        "# - Make the old dimension 0 appear at dimension 2\n",
        "# This results in a tensor of shape (3, 4, 2)\n",
        "x2 = x0.permute(1, 2, 0)\n",
        "print('\\nPermute axes')\n",
        "print(x2)\n",
        "print('shape:', x2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4SJCVbf-bZ0"
      },
      "source": [
        "### Contiguous tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubOOujO_-pQT"
      },
      "source": [
        "Some combinations of reshaping operations will fail with cryptic errors. The exact reasons for this have to do with the way that tensors and views of tensors are implemented, and are beyond the scope of this assignment. However if you're curious, [this blog post by Edward Yang](http://blog.ezyang.com/2019/05/pytorch-internals/) gives a clear explanation of the problem.\n",
        "\n",
        "What you need to know is that you can typically overcome these sorts of errors by either by calling [`.contiguous()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous) before `.view()`, or by using [`.reshape()`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) instead of `.view()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGC6NERq_CT9",
        "outputId": "fce659d0-6fe6-45b9-9a15-3c31d9b61ee9"
      },
      "outputs": [],
      "source": [
        "x0 = torch.randn(2, 3, 4)\n",
        "\n",
        "try:\n",
        "  # This sequence of reshape operations will crash\n",
        "  x1 = x0.transpose(1, 2).view(8, 3)\n",
        "except RuntimeError as e:\n",
        "  print(type(e), e)\n",
        "\n",
        "# We can solve the problem using either .contiguous() or .reshape()\n",
        "x1 = x0.transpose(1, 2).contiguous().view(8, 3)\n",
        "x2 = x0.transpose(1, 2).reshape(8, 3)\n",
        "print('x1 shape: ', x1.shape)\n",
        "print('x2 shape: ', x2.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJiiBxNE-X8g"
      },
      "source": [
        "### **Your turn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOVzHiX-86Ew"
      },
      "source": [
        "In the file `pytorch_basic.py`, implement the function `reshape_practice` to practice using reshape operations on tensors. Given the 1-dimensional input tensor `x` containing the numbers 0 through 23 in order, it should the following output tensor `y` of shape `(3, 8)` by using reshape operations on x:\n",
        "\n",
        "\n",
        "```\n",
        "y = tensor([[ 0,  1,  2,  3, 12, 13, 14, 15],\n",
        "            [ 4,  5,  6,  7, 16, 17, 18, 19],\n",
        "            [ 8,  9, 10, 11, 20, 21, 22, 23]])\n",
        "```\n",
        "\n",
        "Hint: You will need to create an intermediate tensor of rank 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8reAZGzFVTQ3",
        "outputId": "3658acd6-1222-4bf7-8683-43f19a346454"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import reshape_practice\n",
        "\n",
        "x = torch.arange(24)\n",
        "print('Here is x:')\n",
        "print(x)\n",
        "y = reshape_practice(x)\n",
        "print('Here is y:')\n",
        "print(y)\n",
        "\n",
        "expected = [\n",
        "    [0, 1,  2,  3, 12, 13, 14, 15],\n",
        "    [4, 5,  6,  7, 16, 17, 18, 19],\n",
        "    [8, 9, 10, 11, 20, 21, 22, 23]]\n",
        "print('Correct:', y.tolist() == expected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgcdvD1evxTQ"
      },
      "source": [
        "## Tensor operations\n",
        "So far we have seen how to construct, access, and reshape tensors. But one of the most important reasons to use tensors is for performing computation! PyTorch provides many different operations to perform computations on tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BCVlPHZ4_Qz"
      },
      "source": [
        "### Elementwise operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2wbN18E5CKI"
      },
      "source": [
        "Basic mathematical functions operate elementwise on tensors, and are available as operator overloads, as functions in the `torch` module, and as instance methods on torch objects; all produce the same results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrMkbk535KRZ",
        "outputId": "2d318825-239c-4e32-b3c0-f7dcae480139"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
        "y = torch.tensor([[5, 6, 7, 8]], dtype=torch.float32)\n",
        "\n",
        "# Elementwise sum; all give the same result\n",
        "print('Elementwise sum:')\n",
        "print(x + y)\n",
        "print(torch.add(x, y))\n",
        "print(x.add(y))\n",
        "\n",
        "# Elementwise difference\n",
        "print('\\nElementwise difference:')\n",
        "print(x - y)\n",
        "print(torch.sub(x, y))\n",
        "print(x.sub(y))\n",
        "\n",
        "# Elementwise product\n",
        "print('\\nElementwise product:')\n",
        "print(x * y)\n",
        "print(torch.mul(x, y))\n",
        "print(x.mul(y))\n",
        "\n",
        "# Elementwise division\n",
        "print('\\nElementwise division')\n",
        "print(x / y)\n",
        "print(torch.div(x, y))\n",
        "print(x.div(y))\n",
        "\n",
        "# Elementwise power\n",
        "print('\\nElementwise power')\n",
        "print(x ** y)\n",
        "print(torch.pow(x, y))\n",
        "print(x.pow(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6WwPJMYlYvN"
      },
      "source": [
        "Torch also provides many standard mathematical functions; these are available both as functions in the `torch` module and as instance methods on tensors:\n",
        "\n",
        "You can find a full list of all available mathematical functions [in the documentation](https://pytorch.org/docs/stable/torch.html#pointwise-ops); many functions in the `torch` module have corresponding instance methods [on tensor objects](https://pytorch.org/docs/stable/tensors.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s87mjsnG58vR",
        "outputId": "f4ed1aae-be1e-4df6-a594-3d2978686939"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3, 4]], dtype=torch.float32)\n",
        "\n",
        "print('Square root:')\n",
        "print(torch.sqrt(x))\n",
        "print(x.sqrt())\n",
        "\n",
        "print('\\nTrig functions:')\n",
        "print(torch.sin(x))\n",
        "print(x.sin())\n",
        "print(torch.cos(x))\n",
        "print(x.cos())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDyH9USAuyZ-"
      },
      "source": [
        "### Reduction operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbHP9SpZHoMO"
      },
      "source": [
        "So far we've seen basic arithmetic operations on tensors that operate elementwise. We may sometimes want to perform operations that aggregate over part or all of a tensor, such as a summation; these are called **reduction** operations.\n",
        "\n",
        "Like the elementwise operations above, most reduction operations are available both as functions in the `torch` module and as instance methods on `tensor` objects.\n",
        "\n",
        "The simplest reduction operation is summation. We can use the [`.sum()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.sum) function (or eqivalently [`torch.sum`](https://pytorch.org/docs/stable/generated/torch.sum.html)) to reduce either an entire tensor, or to reduce along only one dimension of the tensor using the `dim` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlmsYJWUE2r3",
        "outputId": "e96720f9-f75f-4e50-c144-9c8938d2a325"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]], dtype=torch.float32)\n",
        "print('Original tensor:')\n",
        "print(x)\n",
        "\n",
        "print('\\nSum over entire tensor:')\n",
        "print(torch.sum(x))\n",
        "print(x.sum())\n",
        "\n",
        "# We can sum over each row:\n",
        "print('\\nSum of each row:')\n",
        "print(torch.sum(x, dim=0))\n",
        "print(x.sum(dim=0))\n",
        "\n",
        "# Sum over each column:\n",
        "print('\\nSum of each column:')\n",
        "print(torch.sum(x, dim=1))\n",
        "print(x.sum(dim=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzKio_3Quz5a"
      },
      "source": [
        "Other useful reduction operations include [`mean`](https://pytorch.org/docs/stable/torch.html#torch.mean), [`min`](https://pytorch.org/docs/stable/torch.html#torch.min), and [`max`](https://pytorch.org/docs/stable/torch.html#torch.max). You can find a full list of all available reduction operations [in the documentation](https://pytorch.org/docs/stable/torch.html#reduction-ops).\n",
        "\n",
        "Some reduction operations return more than one value; for example `min` returns both the minimum value over the specified dimension, as well as the index where the minimum value occurs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFD7aT54H4ik",
        "outputId": "02b81aff-c3a3-4d99-ee61-a7acc36555d5"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[2, 4, 3, 5], [3, 3, 5, 2]], dtype=torch.float32)\n",
        "print('Original tensor:')\n",
        "print(x, x.shape)\n",
        "\n",
        "# Finding the overall minimum only returns a single value\n",
        "print('\\nOverall minimum: ', x.min())\n",
        "\n",
        "# Compute the minimum along each column; we get both the value and location:\n",
        "# The minimum of the first column is 2, and it appears at index 0;\n",
        "# the minimum of the second column is 3 and it appears at index 1; etc\n",
        "col_min_vals, col_min_idxs = x.min(dim=0)\n",
        "print('\\nMinimum along each column:')\n",
        "print('values:', col_min_vals)\n",
        "print('idxs:', col_min_idxs)\n",
        "\n",
        "# Compute the minimum along each row; we get both the value and the minimum\n",
        "row_min_vals, row_min_idxs = x.min(dim=1)\n",
        "print('\\nMinimum along each row:')\n",
        "print('values:', row_min_vals)\n",
        "print('idxs:', row_min_idxs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFwYRESoFr4t"
      },
      "source": [
        "Reduction operations *reduce* the rank of tensors: the dimension over which you perform the reduction will be removed from the shape of the output. If you pass `keepdim=True` to a reduction operation, the specified dimension will not be removed; the output tensor will instead have a shape of 1 in that dimension.\n",
        "\n",
        "When you are working with multidimensional tensors, thinking about rows and columns can become confusing; instead it's more useful to think about the shape that will result from each operation. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjcAveyJFqm7",
        "outputId": "33fbb6cf-53ec-4506-f3b2-e6ea1cf8e4fe"
      },
      "outputs": [],
      "source": [
        "# Create a tensor of shape (128, 10, 3, 64, 64)\n",
        "x = torch.randn(128, 10, 3, 64, 64)\n",
        "print(x.shape)\n",
        "\n",
        "# Take the mean over dimension 1; shape is now (128, 3, 64, 64)\n",
        "x = x.mean(dim=1)\n",
        "print(x.shape)\n",
        "\n",
        "# Take the sum over dimension 2; shape is now (128, 3, 64)\n",
        "x = x.sum(dim=2)\n",
        "print(x.shape)\n",
        "\n",
        "# Take the mean over dimension 1, but keep the dimension from being eliminated\n",
        "# by passing keepdim=True; shape is now (128, 1, 64)\n",
        "x = x.mean(dim=1, keepdim=True)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXMp4tcM0Q_E"
      },
      "source": [
        "**Your turn**: In the file `pytorch_basic.py`, implement the function `zero_row_min` which sets the minimum value along each row of a tensor to zero. You should use reduction and indexing operations, and you should not use any explicit loops.\n",
        "\n",
        "Hint: [`clone`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.clone), [`argmin`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.argmin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaJzt-Y62blF",
        "outputId": "ffd828ca-15a3-4582-bf19-f95d0c1a5cf7"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import zero_row_min\n",
        "\n",
        "x0 = torch.tensor([[10, 20, 30], [2, 5, 1]])\n",
        "print('Here is x0:')\n",
        "print(x0)\n",
        "y0 = zero_row_min(x0)\n",
        "print('Here is y0:')\n",
        "print(y0)\n",
        "expected = [[0, 20, 30], [2, 5, 0]]\n",
        "y0_correct = torch.is_tensor(y0) and y0.tolist() == expected\n",
        "print('y0 correct: ', y0_correct)\n",
        "\n",
        "x1 = torch.tensor([[2, 5, 10, -1], [1, 3, 2, 4], [5, 6, 2, 10]])\n",
        "print('\\nHere is x1:')\n",
        "print(x1)\n",
        "y1 = zero_row_min(x1)\n",
        "print('Here is y1:')\n",
        "print(y1)\n",
        "expected = [[2, 5, 10, 0], [0, 3, 2, 4], [5, 6, 0, 10]]\n",
        "y1_correct = torch.is_tensor(y1) and y1.tolist() == expected\n",
        "print('y1 correct: ', y1_correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRyLyXU2u29N"
      },
      "source": [
        "### Matrix operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DwjbapG6MM_"
      },
      "source": [
        "Note that unlike MATLAB, * is elementwise multiplication, not matrix multiplication. PyTorch provides a number of linear algebra functions that compute different types of vector and matrix products. The most commonly used are:\n",
        "\n",
        "- [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot): Computes inner product of vectors\n",
        "- [`torch.mm`](https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm): Computes matrix-matrix products\n",
        "- [`torch.mv`](https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv): Computes matrix-vector products\n",
        "- [`torch.addmm`](https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm) / [`torch.addmv`](https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv): Computes matrix-matrix and matrix-vector multiplications plus a bias\n",
        "- [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm) / [`torch.baddmm`](https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm): Batched versions of `torch.mm` and `torch.addmm`, respectively\n",
        "- [`torch.matmul`](https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul): General matrix product that performs different operations depending on the rank of the inputs. Confusingly, this is similar to `np.dot` in numpy.\n",
        "\n",
        "You can find a full list of the available linear algebra operators [in the documentation](https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations).\n",
        "\n",
        "Here is an example of using `torch.dot` to compute inner products. Like the other mathematical operators we've seen, most linear algebra operators are available both as functions in the `torch` module and as instance methods of tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRUYW2as6ZCh",
        "outputId": "848655e0-09ff-4997-815a-4bee8d7ce7b0"
      },
      "outputs": [],
      "source": [
        "v = torch.tensor([9,10], dtype=torch.float32)\n",
        "w = torch.tensor([11, 12], dtype=torch.float32)\n",
        "\n",
        "# Inner product of vectors\n",
        "print('Dot products:')\n",
        "print(torch.dot(v, w))\n",
        "print(v.dot(w))\n",
        "\n",
        "# dot only works for vectors -- it will give an error for tensors of rank > 1\n",
        "x = torch.tensor([[1,2],[3,4]], dtype=torch.float32)\n",
        "y = torch.tensor([[5,6],[7,8]], dtype=torch.float32)\n",
        "try:\n",
        "  print(x.dot(y))\n",
        "except RuntimeError as e:\n",
        "  print(e)\n",
        "\n",
        "# Instead we use mm for matrix-matrix products:\n",
        "print('\\nMatrix-matrix product:')\n",
        "print(torch.mm(x, y))\n",
        "print(x.mm(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQRxK34KdHm3"
      },
      "source": [
        "With all the different linear algebra operators that PyTorch provides, there is usually more than one way to compute something. For example to compute matrix-vector products we can use `torch.mv`; we can reshape the vector to have rank 2 and use `torch.mm`; or we can use `torch.matmul`. All give the same results, but the outputs might have different ranks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqEzcnHkdRYA",
        "outputId": "564b2d60-916d-463a-d3de-fc9abf46f269"
      },
      "outputs": [],
      "source": [
        "print('Here is x (rank 2):')\n",
        "print(x)\n",
        "print('\\nHere is v (rank 1):')\n",
        "print(v)\n",
        "\n",
        "# Matrix-vector multiply with torch.mv produces a rank-1 output\n",
        "print('\\nMatrix-vector product with torch.mv (rank 1 output)')\n",
        "print(torch.mv(x, v))\n",
        "print(x.mv(v))\n",
        "\n",
        "# We can reshape the vector to have rank 2 and use torch.mm to perform\n",
        "# matrix-vector products, but the result will have rank 2\n",
        "print('\\nMatrix-vector product with torch.mm (rank 2 output)')\n",
        "print(torch.mm(x, v.view(2, 1)))\n",
        "print(x.mm(v.view(2, 1)))\n",
        "\n",
        "print('\\nMatrix-vector product with torch.matmul (rank 1 output)')\n",
        "print(torch.matmul(x, v))\n",
        "print(x.matmul(v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eqQJ5IUjtNT"
      },
      "source": [
        "**Your turn**: In the file `pytorch_basic.py`, implement the function `batched_matrix_multiply`. You should implement two pathways within this function: one that uses an explicit loop over the batch dimension, and another that performs the batched matrix multiply using a single PyTorch operation and no explicit loop.\n",
        "\n",
        "Hint: [`torch.stack`](https://pytorch.org/docs/master/generated/torch.stack.html), [`bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZD1VQHKVTRQ",
        "outputId": "17243c7d-dc5f-4ac0-f9c2-7a9b28905b50"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import batched_matrix_multiply\n",
        "\n",
        "B, N, M, P = 2, 3, 5, 4\n",
        "x = torch.randn(B, N, M)\n",
        "y = torch.randn(B, M, P)\n",
        "z_expected = torch.stack([x[0] @ y[0], x[1] @ y[1]])\n",
        "\n",
        "# The two may not return exactly the same result; different linear algebra\n",
        "# routines often return slightly different results due to the fact that\n",
        "# floating-point math is non-exact and non-associative.\n",
        "z1 = batched_matrix_multiply(x, y, use_loop=True)\n",
        "z1_diff = (z1 - z_expected).abs().max().item()\n",
        "print('z1 difference: ', z1_diff)\n",
        "print('z1 difference within tolerance: ', z1_diff < 1e-6)\n",
        "\n",
        "z2 = batched_matrix_multiply(x, y, use_loop=False)\n",
        "z2_diff = (z2 - z_expected).abs().max().item()\n",
        "print('\\nz2 difference: ', z2_diff)\n",
        "print('z2 difference within tolerance: ', z2_diff < 1e-6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbCVOr2sVTRR"
      },
      "source": [
        "### Vectorization\n",
        "In many cases, avoiding explicit Python loops in your code and instead using PyTorch operators to handle looping internally will cause your code to run a lot faster. This style of writing code, called **vectorization**, avoids overhead from the Python interpreter, and can also better parallelize the computation (e.g. across CPU cores, on on GPUs). Whenever possible you should strive to write vectorized code.\n",
        "\n",
        "Run the following the compare the speed of the `batched_matrix_multiply` with `use_loop=True` and with `use_loop=False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "a-acTIOpVTRR",
        "outputId": "b27372d8-389e-4de2-d85b-11f7f46e19b2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_basic import batched_matrix_multiply\n",
        "\n",
        "N, M, P = 100, 100, 100\n",
        "loop_times = []\n",
        "no_loop_times = []\n",
        "Bs = list(range(5, 100, 5))\n",
        "num_trials = 20\n",
        "for B in Bs:\n",
        "    loop_trials = []\n",
        "    no_loop_trials = []\n",
        "    for trial in range(num_trials):\n",
        "        x = torch.randn(B, N, M)\n",
        "        y = torch.randn(B, M, P)\n",
        "        t0 = time.time()\n",
        "        z1 = batched_matrix_multiply(x, y, use_loop=True)\n",
        "        t1 = time.time()\n",
        "        z2 = batched_matrix_multiply(x, y, use_loop=False)\n",
        "        t2 = time.time()\n",
        "        loop_trials.append(t1 - t0)\n",
        "        no_loop_trials.append(t2 - t1)\n",
        "    loop_mean = torch.tensor(loop_trials).mean().item()\n",
        "    no_loop_mean = torch.tensor(no_loop_trials).mean().item()\n",
        "    loop_times.append(loop_mean)\n",
        "    no_loop_times.append(no_loop_mean)\n",
        "\n",
        "plt.plot(Bs, loop_times, 'o-', label='use_loop=True')\n",
        "plt.plot(Bs, no_loop_times, 'o-', label='use_loop=False')\n",
        "plt.xlabel('Batch size B')\n",
        "plt.ylabel('Runtime (s)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UISn2pcf9QjY"
      },
      "source": [
        "## Broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTj6f8VN9UZg"
      },
      "source": [
        "Broadcasting is a powerful mechanism that allows PyTorch to work with arrays of different shapes when performing arithmetic operations. Frequently we have a smaller tensor and a larger tensor, and we want to use the smaller tensor multiple times to perform some operation on the larger tensor.\n",
        "\n",
        "For example, suppose that we want to add a constant vector to each row of a tensor. We could do it like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF0Dhzlu9fef",
        "outputId": "d34975ce-7a4d-44e7-a6b4-71a6a5b23cad"
      },
      "outputs": [],
      "source": [
        "# We will add the vector v to each row of the matrix x,\n",
        "# storing the result in the matrix y\n",
        "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
        "v = torch.tensor([1, 0, 1])\n",
        "y = torch.zeros_like(x)   # Create an empty matrix with the same shape as x\n",
        "\n",
        "# Add the vector v to each row of the matrix x with an explicit loop\n",
        "for i in range(4):\n",
        "    y[i, :] = x[i, :] + v\n",
        "\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXpoBKE9vp7"
      },
      "source": [
        "This works; however when the tensor x is very large, computing an explicit loop in Python could be slow. Note that adding the vector v to each row of the tensor x is equivalent to forming a tensor vv by stacking multiple copies of v vertically, then performing elementwise summation of x and vv. We could implement this approach like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2_5cKeu94c2",
        "outputId": "3ad66d35-0f34-4748-e4e2-cfaf5a537240"
      },
      "outputs": [],
      "source": [
        "vv = v.repeat((4, 1))  # Stack 4 copies of v on top of each other\n",
        "print(vv)              # Prints \"[[1 0 1]\n",
        "                       #          [1 0 1]\n",
        "                       #          [1 0 1]\n",
        "                       #          [1 0 1]]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KiRj23p-QIs",
        "outputId": "62192e4d-aacf-474d-dfe4-673bc60c4e3e"
      },
      "outputs": [],
      "source": [
        "y = x + vv  # Add x and vv elementwise\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7NNlSsHBKib"
      },
      "source": [
        "PyTorch broadcasting allows us to perform this computation without actually creating multiple copies of v. Consider this version, using broadcasting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jIiZc-ABBnt",
        "outputId": "e3496e8b-2aaf-40f8-b27a-8b7c85d250fb"
      },
      "outputs": [],
      "source": [
        "# We will add the vector v to each row of the matrix x,\n",
        "# storing the result in the matrix y\n",
        "x = torch.tensor([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12]])\n",
        "v = torch.tensor([1, 0, 1])\n",
        "y = x + v  # Add v to each row of x using broadcasting\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuUBX8YnBSIG"
      },
      "source": [
        "The line y = x + v works even though x has shape (4, 3) and v has shape (3,) due to broadcasting; this line works as if v actually had shape (4, 3), where each row was a copy of v, and the sum was performed elementwise.\n",
        "\n",
        "Broadcasting two tensors together follows these rules:\n",
        "\n",
        "1.   If the tensors do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.\n",
        "2.   The two tensors are said to be *compatible* in a dimension if they have the same size in the dimension, or if one of the tensors has size 1 in that dimension.\n",
        "3.   The tensors can be broadcast together if they are compatible in all dimensions.\n",
        "4.   After broadcasting, each tensor behaves as if it had shape equal to the elementwise maximum of shapes of the two input tensors.\n",
        "5.   In any dimension where one tensor had size 1 and the other tensor had size greater than 1, the first tensor behaves as if it were copied along that dimension\n",
        "\n",
        "If this explanation does not make sense, try reading the explanation from the [documentation](https://pytorch.org/docs/stable/notes/broadcasting.html).\n",
        "\n",
        "Broadcasting usually happens implicitly inside many PyTorch operators. However we can also broadcast explicitly using the function [`torch.broadcast_tensors`](https://pytorch.org/docs/stable/generated/torch.broadcast_tensors.html#torch.broadcast_tensors):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIlIBao3VTRc",
        "outputId": "259ff37c-2a4e-4d86-9d45-1ae81187a776"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
        "v = torch.tensor([1, 0, 1])\n",
        "print('Here is x (before broadcasting):')\n",
        "print(x)\n",
        "print('x.shape: ', x.shape)\n",
        "print('\\nHere is v (before broadcasting):')\n",
        "print(v)\n",
        "print('v.shape: ', v.shape)\n",
        "\n",
        "xx, vv = torch.broadcast_tensors(x, v)\n",
        "print('Here is xx (after) broadcasting):')\n",
        "print(xx)\n",
        "print('xx.shape: ', x.shape)\n",
        "print('\\nHere is vv (after broadcasting):')\n",
        "print(vv)\n",
        "print('vv.shape: ', vv.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWXtBo6eVTRf"
      },
      "source": [
        "Notice that after broadcasting, `x` remains the same but `v` has an extra dimension prepended to its shape, and it is duplicated to have the same shape as `x`; since they have the same shape after broadcasting they can be added elementwise.\n",
        "\n",
        "Not all functions support broadcasting. You can find functions that does not support broadcasting from the official docs. (e.g. [`torch.mm`](https://pytorch.org/docs/stable/torch.html#torch.mm) does not support broadcasting, but [`torch.matmul`](https://pytorch.org/docs/1.1.0/torch.html#torch.matmul) does)\n",
        "\n",
        "Broadcasting can let us easily implement many different operations. For example we can compute an outer product of vectors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W-k7-hpCwlT",
        "outputId": "97b9f2a4-fe28-4e7c-a825-09c3d02d92dd"
      },
      "outputs": [],
      "source": [
        "# Compute outer product of vectors\n",
        "v = torch.tensor([1, 2, 3])  # v has shape (3,)\n",
        "w = torch.tensor([4, 5])     # w has shape (2,)\n",
        "# To compute an outer product, we first reshape v to be a column\n",
        "# vector of shape (3, 1); we can then broadcast it against w to yield\n",
        "# an output of shape (3, 2), which is the outer product of v and w:\n",
        "print(v.view(3, 1) * w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a9EcX20moP_"
      },
      "source": [
        "We can add a vector to each row of a matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bhmBiwcDF1B",
        "outputId": "dbeca99b-502d-4f3b-b6f1-0cdb247091f1"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x has shape (2, 3)\n",
        "v = torch.tensor([1, 2, 3])               # v has shape (3,)\n",
        "print('Here is the matrix:')\n",
        "print(x)\n",
        "print('\\nHere is the vector:')\n",
        "print(v)\n",
        "\n",
        "# x has shape (2, 3) and v has shape (3,) so they broadcast to (2, 3),\n",
        "# giving the following matrix:\n",
        "print('\\nAdd the vector to each row of the matrix:')\n",
        "print(x + v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYloJIvmm_Me"
      },
      "source": [
        "We can add a vector to each column of a matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDTFKACqDK22",
        "outputId": "ae863e00-f651-4ea3-fb49-52e9a8649660"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x has shape (2, 3)\n",
        "w = torch.tensor([4, 5])                  # w has shape (2,)\n",
        "print('Here is the matrix:')\n",
        "print(x)\n",
        "print('\\nHere is the vector:')\n",
        "print(w)\n",
        "\n",
        "# x has shape (2, 3) and w has shape (2,). We reshape w to (2, 1);\n",
        "# then when we add the two the result broadcasts to (2, 3):\n",
        "print('\\nAdd the vector to each column of the matrix:')\n",
        "print(x + w.view(-1, 1))\n",
        "\n",
        "# Another solution is the following:\n",
        "# 1. Transpose x so it has shape (3, 2)\n",
        "# 2. Since w has shape (2,), adding will broadcast to (3, 2)\n",
        "# 3. Transpose the result, resulting in a shape (2, 3)\n",
        "print((x.t() + w).t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9717YmBBpBfr"
      },
      "source": [
        "Multiply a tensor by a set of constants:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UjWDp_XDc_-",
        "outputId": "504caf73-6336-44d2-e21d-35beb1478c86"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # x has shape (2, 3)\n",
        "c = torch.tensor([1, 10, 11, 100])        # c has shape (4)\n",
        "print('Here is the matrix:')\n",
        "print(x)\n",
        "print('\\nHere is the vector:')\n",
        "print(c)\n",
        "\n",
        "# We do the following:\n",
        "# 1. Reshape c from (4,) to (4, 1, 1)\n",
        "# 2. x has shape (2, 3). Since they have different ranks, when we multiply the\n",
        "#    two, x behaves as if its shape were (1, 2, 3)\n",
        "# 3. The result of the broadcast multiplication between tensor of shape\n",
        "#    (4, 1, 1) and (1, 2, 3) has shape (4, 2, 3)\n",
        "# 4. The result y has shape (4, 2, 3), and y[i] (shape (2, 3)) is equal to\n",
        "#    c[i] * x\n",
        "y = c.view(-1, 1, 1) * x\n",
        "print('\\nMultiply x by a set of constants:')\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2EHXFBFq1ea"
      },
      "source": [
        "**Your turn**: In the file `pytorch_basic.py`, implement the function `normalize_columns` that normalizes the columns of a matrix. It should compute the mean and standard deviation of each column, then subtract the mean and divide by the standard deviation for each element in the column.\n",
        "\n",
        "Example:\n",
        "```\n",
        "x = [[ 0,  30,  600],\n",
        "     [ 1,  10,  200],\n",
        "     [-1,  20,  400]]\n",
        "```\n",
        "- The first column has mean 0 and std 1\n",
        "- The second column has mean 20 and std 10\n",
        "- The third column has mean 400 and std 200\n",
        "\n",
        "After normalizing the columns, the result should be:\n",
        "```\n",
        "y = [[ 0,  1,  1],\n",
        "     [ 1, -1, -1],\n",
        "     [-1,  0,  0]]\n",
        "```\n",
        "\n",
        "Recall that given scalars $x_1,\\ldots,x_M$ the mean $\\mu$ and standard deviation $\\sigma$ are given by\n",
        "\n",
        "$$\\mu=\\frac{1}{M}\\sum_{i=1}^M x_i \\hspace{4pc} \\sigma = \\sqrt{\\frac{1}{M-1}\\sum_{i=1}^M(x_i-\\mu)^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVh1DMqMr3zl",
        "outputId": "103ecaba-9767-4223-b328-929d135cb303"
      },
      "outputs": [],
      "source": [
        "from pytorch_basic import normalize_columns\n",
        "\n",
        "x = torch.tensor([[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]])\n",
        "y = normalize_columns(x)\n",
        "print('Here is x:')\n",
        "print(x)\n",
        "print('Here is y:')\n",
        "print(y)\n",
        "\n",
        "x_expected = [[0., 30., 600.], [1., 10., 200.], [-1., 20., 400.]]\n",
        "y_expected = [[0., 1., 1.], [1., -1., -1.], [-1., 0., 0.]]\n",
        "y_correct = y.tolist() == y_expected\n",
        "x_correct = x.tolist() == x_expected\n",
        "print('y correct: ', y_correct)\n",
        "print('x unchanged: ', x_correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlJs-yN4VTRp"
      },
      "source": [
        "### Out-of-place vs in-place operators\n",
        "Most PyTorch operators are classified into one of two categories:\n",
        "- **Out-of-place operators:** return a new tensor. Most PyTorch operators behave this way.\n",
        "- **In-place operators:** modify and return the input tensor. Instance methods that end with an underscore (such as `add_()` are in-place. Operators in the `torch` namespace can be made in-place using the `out=` keyword argument.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnwGzmU9VTRp",
        "outputId": "e45824a5-551d-4c13-b508-e73372c2e871"
      },
      "outputs": [],
      "source": [
        "# Out-of-place addition creates and returns a new tensor without modifying the inputs:\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([3, 4, 5])\n",
        "print('Out-of-place addition:')\n",
        "print('Before addition:')\n",
        "print('x: ', x)\n",
        "print('y: ', y)\n",
        "z = x.add(y)  # Same as z = x + y or z = torch.add(x, y)\n",
        "print('\\nAfter addition (x and y unchanged):')\n",
        "print('x: ', x)\n",
        "print('y: ', y)\n",
        "print('z: ', z)\n",
        "print('z is x: ', z is x)\n",
        "print('z is y: ', z is y)\n",
        "\n",
        "# In-place addition modifies the input tensor:\n",
        "print('\\n\\nIn-place Addition:')\n",
        "print('Before addition:')\n",
        "print('x: ', x)\n",
        "print('y: ', y)\n",
        "x.add_(y)  # Same as x += y or torch.add(x, y, out=x)\n",
        "print('\\nAfter addition (x is modified):')\n",
        "print('x: ', x)\n",
        "print('y: ', y)\n",
        "print('z: ', z)\n",
        "print('z is x: ', z is x)\n",
        "print('z is y: ', z is y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNTk5heeVTRr"
      },
      "source": [
        "In general, **you should avoid in-place operations** since they can cause problems when computing gradients using autograd (which we will cover in a future assignment)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN6FfqU9wFeG"
      },
      "source": [
        "## Running on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds6SDTbrwOc1"
      },
      "source": [
        "One of the most important features of PyTorch is that it can use graphics processing units (GPUs) to accelerate its tensor operations.\n",
        "\n",
        "We can easily check whether PyTorch is configured to use GPUs:\n",
        "\n",
        "Tensors can be moved onto any device using the .to method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RkoFEVVKWlW",
        "outputId": "e6dc8088-9d7d-41cf-82da-5dc6ba17b78a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available:\n",
        "  print('PyTorch can use GPUs!')\n",
        "else:\n",
        "  print('PyTorch cannot use GPUs.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i_5n_XuKr5k"
      },
      "source": [
        "You can enable GPUs in Colab via Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU.\n",
        "\n",
        "This may cause the Colab runtime to restart, so we will re-import torch in the next cell.\n",
        "\n",
        "We have already seen that PyTorch tensors have a `dtype` attribute specifying their datatype. All PyTorch tensors also have a `device` attribute that specifies the device where the tensor is stored -- either CPU, or CUDA (for NVIDA GPUs). A tensor on a CUDA device will automatically use that device to accelerate all of its operations.\n",
        "\n",
        "Just as with datatypes, we can use the [`.to()`](https://pytorch.org/docs/1.1.0/tensors.html#torch.Tensor.to) method to change the device of a tensor. We can also use the convenience methods `.cuda()` and `.cpu()` methods to move tensors between CPU and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D03s614dMCvy",
        "outputId": "1be5a138-9422-4ae2-8ebe-e291dc96f2ae"
      },
      "outputs": [],
      "source": [
        "# Construct a tensor on the CPU\n",
        "x0 = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
        "print('x0 device:', x0.device)\n",
        "\n",
        "# Move it to the GPU using .to()\n",
        "x1 = x0.to('cuda')\n",
        "print('x1 device:', x1.device)\n",
        "\n",
        "# Move it to the GPU using .cuda()\n",
        "x2 = x0.cuda()\n",
        "print('x2 device:', x2.device)\n",
        "\n",
        "# Move it back to the CPU using .to()\n",
        "x3 = x1.to('cpu')\n",
        "print('x3 device:', x3.device)\n",
        "\n",
        "# Move it back to the CPU using .cpu()\n",
        "x4 = x2.cpu()\n",
        "print('x4 device:', x4.device)\n",
        "\n",
        "# We can construct tensors directly on the GPU as well\n",
        "y = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float64, device='cuda')\n",
        "print('y device / dtype:', y.device, y.dtype)\n",
        "\n",
        "# Calling x.to(y) where y is a tensor will return a copy of x with the same\n",
        "# device and dtype as y\n",
        "x5 = x0.to(y)\n",
        "print('x5 device / dtype:', x5.device, x5.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-TDxICdOmJo"
      },
      "source": [
        "Performing large tensor operations on a GPU can be **a lot faster** than running the equivalent operation on CPU.\n",
        "\n",
        "Here we compare the speed of adding two tensors of shape (10000, 10000) on CPU and GPU:\n",
        "\n",
        "(Note that GPU code may run asynchronously with CPU code, so when timing the speed of operations on the GPU it is important to use `torch.cuda.synchronize` to synchronize the CPU and GPU.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW14ZF-_PK7t",
        "outputId": "8bb4e814-6e8b-4c9c-96eb-e03641d485df"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "a_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n",
        "b_cpu = torch.randn(10000, 10000, dtype=torch.float32)\n",
        "\n",
        "a_gpu = a_cpu.cuda()\n",
        "b_gpu = b_cpu.cuda()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "t0 = time.time()\n",
        "c_cpu = a_cpu + b_cpu\n",
        "t1 = time.time()\n",
        "c_gpu = a_gpu + b_gpu\n",
        "torch.cuda.synchronize()\n",
        "t2 = time.time()\n",
        "\n",
        "# Check that they computed the same thing\n",
        "diff = (c_gpu.cpu() - c_cpu).abs().max().item()\n",
        "print('Max difference between c_gpu and c_cpu:', diff)\n",
        "\n",
        "cpu_time = 1000.0 * (t1 - t0)\n",
        "gpu_time = 1000.0 * (t2 - t1)\n",
        "print('CPU time: %.2f ms' % cpu_time)\n",
        "print('GPU time: %.2f ms' % gpu_time)\n",
        "print('GPU speedup: %.2f x' % (cpu_time / gpu_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HEAVPEwviYb"
      },
      "source": [
        "You should see that running the same computation on the GPU was more than 10~30 times faster than on the CPU! Due to the massive speedups that GPUs offer, we will use GPUs to accelerate much of our machine learning code starting in Assignment 2.\n",
        "\n",
        "**Your turn**: Use the GPU to accelerate the following matrix multiplication operation. You should see 5~10x speedup by using the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqEUdst7SAuZ",
        "outputId": "aca9f502-2515-4ca9-c379-b95f27aedf98"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from pytorch_basic import mm_on_cpu, mm_on_gpu\n",
        "\n",
        "x = torch.rand(512, 4096)\n",
        "w = torch.rand(4096, 4096)\n",
        "\n",
        "t0 = time.time()\n",
        "y0 = mm_on_cpu(x, w)\n",
        "t1 = time.time()\n",
        "\n",
        "y1 = mm_on_gpu(x, w)\n",
        "torch.cuda.synchronize()\n",
        "t2 = time.time()\n",
        "\n",
        "print('y1 on CPU:', y1.device == torch.device('cpu'))\n",
        "diff = (y0 - y1).abs().max().item()\n",
        "print('Max difference between y0 and y1:', diff)\n",
        "print('Difference within tolerance:', diff < 5e-2)\n",
        "\n",
        "cpu_time = 1000.0 * (t1 - t0)\n",
        "gpu_time = 1000.0 * (t2 - t1)\n",
        "print('CPU time: %.2f ms' % cpu_time)\n",
        "print('GPU time: %.2f ms' % gpu_time)\n",
        "print('GPU speedup: %.2f x' % (cpu_time / gpu_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ReSfKmzXiK"
      },
      "source": [
        "## Data Loader\n",
        "\n",
        "\n",
        "\n",
        "PyTorch has two classes from [`torch.utils.data` to work with data](https://pytorch.org/docs/stable/data.html#module-torch.utils.data):\n",
        "- [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) which represents the actual data items, such as images or pieces of text, and their labels\n",
        "- [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) which is used for processing the dataset in batches in an efficient manner.\n",
        "\n",
        "It provides:\n",
        "\n",
        "* Batching: Automatically groups samples into batches.\n",
        "* Shuffling: Randomly shuffles data at every epoch.\n",
        "* Parallel Loading: Uses multiple workers for faster data loading.\n",
        "* Transformation Support: Applies transformations to data on the fly.\n",
        "\n",
        "PyTorch has domain-specific libraries with utilities for common data types such as [TorchText](https://pytorch.org/text/stable/index.html), [TorchVision](https://pytorch.org/vision/stable/index.html) and [TorchAudio](https://pytorch.org/audio/stable/index.html).\n",
        "\n",
        "## Using DataLoader with MNIST\n",
        "\n",
        "Here we will use TorchVision and `torchvision.datasets` which provides easy access to [many common visual datasets](https://pytorch.org/vision/stable/datasets.html). In this example we'll use the [MNIST class](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) for loading the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database).\n",
        "\n",
        "\n",
        "Let's implement a simple example using the MNIST dataset and visualize some images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPjMi4twzMp7",
        "outputId": "06e16f41-2066-42f9-cc6c-5193de018907"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TODO:\n",
        "\n",
        "# Define transformations (ToTensor and Normalize to range [-1,1])\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot9Fresq2w17",
        "outputId": "5c910f78-e21c-4522-957f-d9e0caf19960"
      },
      "outputs": [],
      "source": [
        "# Print information about train_loader\n",
        "print(\"Train DataLoader Information:\")\n",
        "print(\"Number of batches:\", len(dataloader))\n",
        "print(\"Batch size:\", dataloader.batch_size)\n",
        "data_batch, target_batch = next(iter(dataloader))\n",
        "print(\"Shape of data tensor in next batch:\", data_batch.shape)\n",
        "print(\"Shape of target tensor in next batch:\", target_batch.shape)\n",
        "print(\"Target values in next batch:\", target_batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzGBBc2g376L"
      },
      "source": [
        "The data loaders provide a way of iterating (making a loop over) the datasets, each time getting a new batch of data with the given batch size.\n",
        "\n",
        "The first element of the data batch (`data`) is a 4th-order tensor of size (`batch_size`, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels, where the first value is the number of color channels (only 1 in this case as it's gray scale).\n",
        "\n",
        "The second element of the batch (`target`) is a vector containing the correct (or \"target\") classes (\"0\", \"1\", ..., \"9\") for each training digit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "7hWXqmcn4CdS",
        "outputId": "54c9aedc-00aa-4709-ee72-a1291d0d2a44"
      },
      "outputs": [],
      "source": [
        "# Get one batch\n",
        "dataiter = iter(dataloader)\n",
        "images, labels = next(dataiter)\n",
        "print('The shape of images is:',images.shape)\n",
        "\n",
        "# Display the batch\n",
        "\n",
        "fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(images[i].squeeze(), cmap=\"gray\")\n",
        "    ax.set_title(f\"Label: {labels[i].item()}\")\n",
        "    ax.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXjUv3tj3ZQr"
      },
      "source": [
        "## CIFAR-10 Dataset\n",
        "The CIFAR-10 dataset is a widely used dataset for training machine learning and computer vision models. It consists of **60,000 color images** in **10 classes**, with **6,000 images per class**. The dataset is divided into **50,000 training images** and **10,000 test images**. Each image is **32x32 pixels** and comes in **RGB format**.\n",
        "\n",
        "### Classes\n",
        "The CIFAR-10 dataset contains the following **10 mutually exclusive classes**:\n",
        "\n",
        "1. Airplane\n",
        "2. Automobile\n",
        "3. Bird\n",
        "4. Cat\n",
        "5. Deer\n",
        "6. Dog\n",
        "7. Frog\n",
        "8. Horse\n",
        "9. Ship\n",
        "10. Truck\n",
        "\n",
        "Each class has **6,000 images** distributed across the training and test sets.\n",
        "\n",
        "\n",
        "### Dataset Properties\n",
        "- **Number of Classes:** 10\n",
        "- **Number of Images:** 60,000 (50,000 training + 10,000 test)\n",
        "- **Image Size:** 32x32 pixels\n",
        "- **Color Channels:** RGB (3 channels)\n",
        "- **Format:** `.png` images stored in Python `pickle` format\n",
        "\n",
        "\n",
        "### Applications\n",
        "The CIFAR-10 dataset is widely used for:\n",
        "- Image classification tasks\n",
        "- Benchmarking deep learning models\n",
        "- Testing convolutional neural networks (CNNs)\n",
        "- Transfer learning research\n",
        "\n",
        "### References\n",
        "- [CIFAR-10 Official Website](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "- [Torchvision Documentation](https://pytorch.org/vision/stable/datasets.html#cifar10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh18QC_046VF"
      },
      "source": [
        "### Usage in `torchvision.datasets`\n",
        "CIFAR-10 is available in the `torchvision.datasets` module and can be easily loaded in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1gSsvhc4lZ3",
        "outputId": "2a998841-070b-415a-fa9d-127277351780"
      },
      "outputs": [],
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load training and test datasets\n",
        "trainset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "testset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVTXsnLT4_b6"
      },
      "source": [
        "### Example Visualization\n",
        "To visualize some sample images from the dataset, use the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "id": "ckHpWZde4oc3",
        "outputId": "c00a1a32-819a-4d5e-c96b-aa4d283509ee"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# Define class labels\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Function to unnormalize and display images\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # Unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "N = 20\n",
        "rows = 4\n",
        "# Show images\n",
        "imshow(make_grid(images[:N], N//rows))\n",
        "# Print class labels\n",
        "for i in range(rows):\n",
        "    print('    '.join(classes[labels[j + i*(N//rows)]] for j in range(N//rows)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP1XXUxF7MHe",
        "outputId": "d2bf8f44-4207-4786-dbf7-21f0af5d9ab3"
      },
      "outputs": [],
      "source": [
        "# Print information about train_loader\n",
        "print(\"Train DataLoader Information:\")\n",
        "print(\"Number of batches:\", len(trainloader))\n",
        "print(\"Batch size:\", trainloader.batch_size)\n",
        "data_batch, target_batch = next(iter(trainloader))\n",
        "print(\"Shape of data tensor in next batch:\", data_batch.shape)\n",
        "print(\"Shape of target tensor in next batch:\", target_batch.shape)\n",
        "print(\"Target values in next batch:\", target_batch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoFnoejbCNt8"
      },
      "source": [
        "## Save Models!\n",
        "\n",
        "\n",
        "Saving and loading models is a crucial part of deep learning workflows. PyTorch provides flexible mechanisms to save and load models, whether you need to save the entire model, just the model parameters (state_dict), or both. This guide covers different methods for saving and loading PyTorch models efficiently.\n",
        "\n",
        "### 1. Saving and Loading Model State Dict (Recommended)\n",
        "\n",
        "The most common and recommended way to save a PyTorch model is to save its `state_dict`, which contains all the learnable parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ55oJ_REvOv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple model\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc1(x)\n",
        "\n",
        "model = SimpleModel()\n",
        "\n",
        "# Save the model state dict\n",
        "torch.save(model.state_dict(), \"model_state.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md_0DRiBFM6D",
        "outputId": "ec00d9de-a709-4082-d0ef-322e1a315c7f"
      },
      "outputs": [],
      "source": [
        "# load the model from disk\n",
        "\n",
        "model_loaded = SimpleModel()\n",
        "model_loaded.load_state_dict(torch.load(\"model_state.pth\"))\n",
        "\n",
        "# Set the model to evaluation mode (important for inference)\n",
        "model_loaded.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gIyk5GAFA7C"
      },
      "source": [
        "### 2. Saving and Loading the Entire Model\n",
        "\n",
        "You can also save the entire model, including the model architecture and parameters. However, this method is less flexible and may cause compatibility issues when loading the model in different environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecL-4brDFXie"
      },
      "outputs": [],
      "source": [
        "# save the entire model\n",
        "torch.save(model, \"entire_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1NlfJDbFcSq",
        "outputId": "eac5feb1-4fb9-483c-84af-e0ede8501f5c"
      },
      "outputs": [],
      "source": [
        "# load the entire model\n",
        "model_loaded = torch.load(\"entire_model.pth\" , weights_only=False)\n",
        "model_loaded.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQGfj5eXFqVw"
      },
      "source": [
        "### 3. Saving and Loading Model with Optimizer State\n",
        "\n",
        "When training deep learning models, saving the optimizer state along with the model can be useful for resuming training without losing progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pNKCaMZFtXM"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "checkpoint = {\n",
        "    'epoch': 10,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, \"model_checkpoint.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt8Yr9gSFxlV",
        "outputId": "91cc7e48-d29e-4553-fa13-f2535d47a13b"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"model_checkpoint.pth\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "\n",
        "# Set to evaluation mode if needed\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmPhdWGEGARO"
      },
      "source": [
        "### 4. Best Practices for Saving and Loading Models\n",
        "\n",
        "- **Use `.state_dict()` method** for saving and loading models whenever possible.\n",
        "- **Save additional metadata** like epoch, loss, and optimizer state when saving models for resuming training.\n",
        "- **Use relative paths** instead of absolute paths to avoid compatibility issues.\n",
        "- **Set the model to `.eval()` mode** when performing inference to ensure proper behavior of layers like dropout and batch normalization.\n",
        "- **Ensure compatibility** when loading models by using the same model architecture and PyTorch version.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v7vOtqwGK6L"
      },
      "source": [
        "Now you know all the necessary background knowledge to start training the models that you like :)\n",
        "\n",
        "\n",
        "Lets try it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk5DRJkuW0ke"
      },
      "source": [
        "# NOW WE WANT TO TRAIN ON A VERY SIMPLE 1D FUNCTION TO SEE HOW NEURAL NETWORKS WORK (20 points)\n",
        "\n",
        "In this notebook, we will train a neural network to approximate a **simple 1D function**. Instead of using a standard dataset like CIFAR or MNIST, we generate synthetic data from a known mathematical function.\n",
        "\n",
        "Our chosen function is:\n",
        "$$\n",
        "f(x) = e^{-x^2} \\sin(10x)\n",
        "$$\n",
        "This function exhibits **high-frequency oscillations near \\( x = 0 \\)** and decays smoothly as \\( x \\) moves away. The goal is to train a neural network to learn this function **purely from noisy samples**, demonstrating how neural networks approximate complex patterns.\n",
        "\n",
        "We will:\n",
        "- **Generate training data** with added noise.\n",
        "- **Define a simple neural network** in PyTorch.\n",
        "- **Train the network** using gradient descent.\n",
        "- **Visualize** how the model learns step by step.\n",
        "\n",
        "Let‚Äôs get started! üöÄ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4uzxe-VSsgD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "\n",
        "def true_function(x):\n",
        "    # TODO: Define the true function\n",
        "    return np.exp(-x**2) * np.sin(10 * x)\n",
        "    # End of TODO\n",
        "\n",
        "\n",
        "\n",
        "# General function to plot any function in a given domain\n",
        "def plot_function(func, domain=(-2, 2), num_points=400, title=\"Function Plot\", label=\"Function\", color=\"b\"):\n",
        "    x_values = np.linspace(domain[0], domain[1], num_points)\n",
        "    y_values = func(x_values)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(x_values, y_values, label=label, color=color, linewidth=2)\n",
        "    plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
        "    plt.axvline(0, color='gray', linestyle='--', linewidth=0.8)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"f(x)\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "WVH5S3nsdJak",
        "outputId": "cbfc0f85-9496-4a56-92ae-3b78ef3e48fd"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot the true function\n",
        "plot_function(true_function, title=\"True Function: $f(x) = e^{-x^2} \\sin(10x)$\", label=\"True Function\", color=\"r\")\n",
        "# End of TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "912kAyXPdNXW"
      },
      "source": [
        "## Generating Training Data\n",
        "\n",
        "To train our neural network, we need to create a **dataset** from our function.\n",
        "\n",
        "### Steps:\n",
        "1. **Sample random points** from the domain $ x \\in [-2,2]  $.\n",
        "2. **Evaluate** the true function $ f(x) = e^{-x^2} \\sin(10x) $ at these points.\n",
        "3. **Add Gaussian noise** to simulate real-world data imperfections.\n",
        "\n",
        "This noisy dataset will be used to train the neural network. Below is the implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "iSPlDpVYUgIS",
        "outputId": "a00b158b-e67f-40c2-93c3-7cc69326284e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Function to generate training data\n",
        "def generate_training_data(true_func, num_samples=1000, noise_std=0.1, domain=(-2, 2)):\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    x_train = np.random.uniform(domain[0], domain[1], num_samples)  # Randomly sample x values\n",
        "    y_train = true_func(x_train) + np.random.normal(0, noise_std, num_samples)  # Add Gaussian noise\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    x_train_tensor = torch.tensor(x_train, dtype=torch.float32).view(-1, 1)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    return x_train_tensor, y_train_tensor, x_train, y_train\n",
        "\n",
        "# Function to plot training data alongside the true function\n",
        "def plot_training_data(x_train, y_train, true_func, domain=(-2, 2), num_points=400):\n",
        "    x_values = np.linspace(domain[0], domain[1], num_points)\n",
        "    y_values = true_func(x_values)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(x_train, y_train, color=\"r\", label=\"Noisy Samples\", alpha=0.6)\n",
        "    plt.plot(x_values, y_values, label=\"True Function\", color=\"b\", linestyle=\"dashed\", linewidth=2)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"f(x)\")\n",
        "    plt.title(\"Training Data with True Function\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Generate and plot data\n",
        "x_train_tensor, y_train_tensor, x_train, y_train = generate_training_data(true_function)\n",
        "plot_training_data(x_train, y_train,true_function)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDnDcQPreCWN"
      },
      "source": [
        "## Defining the Neural Network Model\n",
        "\n",
        "Now that we have our training data, we need to create a **neural network** to learn the function.\n",
        "\n",
        "### Architecture:\n",
        "- **Input Layer:** 1 neuron (since our input $ x $ is 1D).\n",
        "- **Hidden Layers:** Two fully connected layers with 32 neurons each.\n",
        "- **Activation Function:** ReLU (helps model non-linearity).\n",
        "- **Output Layer:** 1 neuron (predicts $ y $).\n",
        "\n",
        "This is a **simple feedforward network** that should be able to approximate the function. Below is the implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lexRuCytVS4e",
        "outputId": "0765deb8-ef28-4a1d-c202-c87c59a9dc9b"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the neural network model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        # TODO: init the layers\n",
        "        # Input -> Hidden Layer 1\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # Hidden Layer 1 -> Hidden Layer 2\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        # Hidden Layer 2 -> Output\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        # ReLU activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # End of TODO\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: Implement the forward function.\n",
        "        # note: No activation in final layer (for regression)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "        # End of TODO\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = NeuralNetwork()\n",
        "\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWtly0XieRbJ"
      },
      "source": [
        "## Training the Neural Network\n",
        "\n",
        "Now that we have defined our model, we need to train it using our dataset.\n",
        "\n",
        "### Training Process:\n",
        "1. **Use mini-batches** to train the model efficiently.\n",
        "2. **Compute the loss** using Mean Squared Error (MSE).\n",
        "3. **Backpropagate the gradients** to update model weights.\n",
        "4. **Monitor training progress** by plotting the function every few epochs.\n",
        "\n",
        "We define a function `train_model()` to handle the entire training process and update the model iteratively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "653NhzwEe4Ha"
      },
      "outputs": [],
      "source": [
        "def plot_learned_function(model, epoch, loss, true_function ,domain=(-2, 2), num_points=400 ):\n",
        "    x_values = np.linspace(domain[0], domain[1], num_points)\n",
        "    x_tensor = torch.tensor(x_values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(x_tensor).numpy()\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(x_train, y_train, color=\"r\", label=\"Noisy Samples\", alpha=0.6)\n",
        "    plt.plot(x_values, true_function(x_values), label=\"True Function\", color=\"b\", linestyle=\"dashed\", linewidth=2)\n",
        "    plt.plot(x_values, y_pred, label=f\"Learned Function (Epoch {epoch})\", color=\"g\", linewidth=3)  # Thicker line\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"f(x)\")\n",
        "    plt.title(f\"Function Approximation at Epoch {epoch} (Loss: {loss:.6f})\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_curve(loss_history):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(loss_history, label=\"Training Loss\", color=\"r\", linewidth=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"Loss Curve During Training (Final Loss: {loss_history[-1]:.6f})\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Annotate final loss\n",
        "    plt.text(len(loss_history) * 0.8, loss_history[-1] * 1.1,\n",
        "             f\"Final Loss: {loss_history[-1]:.6f}\", fontsize=12, color=\"black\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U17UbzkkVjOY",
        "outputId": "422741f9-ea84-4cd0-c221-6675b7706fd1"
      },
      "outputs": [],
      "source": [
        "# Function to train the model\n",
        "def train_model(model, x_train_tensor, y_train_tensor,true_function, num_epochs=10, batch_size=32, lr=0.01, plot_interval=1 ):\n",
        "\n",
        "    # TODO: Define loss function and optimizer. Create the proper DataLoader.\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    # End of TODO\n",
        "\n",
        "\n",
        "    loss_history = []  # Store loss values for visualization\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            # TODO: Forward pass and backpropagate\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "            # Forward pass\n",
        "            y_pred = model(batch_x)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(y_pred, batch_y)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # End of TODO\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "        # TODO: Store average loss in loss_history\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        loss_history.append(avg_loss)\n",
        "        # End of TODO\n",
        "\n",
        "\n",
        "        # TODO: Plot function every 'plot_interval' epochs. And Print progress\n",
        "        # every 100 epochs\n",
        "        if epoch % plot_interval == 0 or epoch == num_epochs - 1:\n",
        "            model.eval()  # Set model to evaluation mode\n",
        "            x_values = torch.linspace(-2, 2, 400).view(-1, 1)\n",
        "            y_values = true_function(x_values.numpy())\n",
        "            y_pred = model(x_values).detach().numpy()\n",
        "\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            plt.scatter(x_train, y_train, color=\"r\", label=\"Noisy Samples\", alpha=0.6)\n",
        "            plt.plot(x_values.numpy(), y_values, label=\"True Function\", color=\"b\", linewidth=2)\n",
        "            plt.plot(x_values.numpy(), y_pred, label=\"Model Prediction\", color=\"g\", linewidth=2)\n",
        "            plt.xlabel(\"x\")\n",
        "            plt.ylabel(\"f(x)\")\n",
        "            plt.title(f\"Epoch {epoch}: Model Approximation\")\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "            model.train()\n",
        "        if epoch % 100 == 0 or epoch == num_epochs - 1:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        # End of TODO\n",
        "\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "# Train the model and store loss history\n",
        "loss_history = train_model(model, x_train_tensor, y_train_tensor , true_function)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "cunEJf32WF1M",
        "outputId": "ff62d4f2-a1cc-4f20-c95c-0033c1d8669e"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curve\n",
        "plot_loss_curve(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JroWcYHfY_c"
      },
      "outputs": [],
      "source": [
        "# Define two new non-sinusoidal mathematical functions\n",
        "def function_1(x):\n",
        "    return np.exp(-x**2)  # Gaussian Bell Curve\n",
        "\n",
        "def function_2(x):\n",
        "    return 0.5 * x**3 - x**2 + 0.5 * x  # Cubic Polynomial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BMKM4gfAm2GE",
        "outputId": "3ef26e80-49a8-4565-dbe8-cd777af0a919"
      },
      "outputs": [],
      "source": [
        "# TODO: Train on Gaussian Bell Curve\n",
        "# generate data - plot data - create model - train model - plot loss\n",
        "\n",
        "\n",
        "x_train_tensor, y_train_tensor, x_train, y_train = generate_training_data(function_1)\n",
        "plot_training_data(x_train, y_train,function_1)\n",
        "\n",
        "loss_history = train_model(model, x_train_tensor, y_train_tensor , function_1)\n",
        "plot_loss_curve(loss_history)\n",
        "\n",
        "# End of TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nh6QyQw6m0Ll",
        "outputId": "f855e5f5-f598-42a9-9f84-7dce74930d28"
      },
      "outputs": [],
      "source": [
        "# TODO: Train on Cubic Polynomial\n",
        "# generate data - plot data - create model - train model - plot loss\n",
        "\n",
        "x_train_tensor, y_train_tensor, x_train, y_train = generate_training_data(function_2)\n",
        "plot_training_data(x_train, y_train,function_2)\n",
        "\n",
        "loss_history = train_model(model, x_train_tensor, y_train_tensor , function_2)\n",
        "plot_loss_curve(loss_history)\n",
        "\n",
        "\n",
        "# End of TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8J1ETZJkCr2"
      },
      "outputs": [],
      "source": [
        "# Define the hard function\n",
        "def hard_function(x):\n",
        "    return np.exp(-x**2) * np.sin(10*x) + 0.5 * np.tanh(5*x) + 0.3 * np.cos(20*x) + 0.1*x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xB6C67lunVHk",
        "outputId": "f511c853-aecb-4b96-ed86-d950fb264848"
      },
      "outputs": [],
      "source": [
        "#  TODO: Train on the hard function\n",
        "# generate data - plot data - create model - train model - plot loss\n",
        "x_train_tensor, y_train_tensor, x_train, y_train = generate_training_data(hard_function)\n",
        "plot_training_data(x_train, y_train,hard_function)\n",
        "\n",
        "loss_history = train_model(model, x_train_tensor, y_train_tensor , hard_function)\n",
        "plot_loss_curve(loss_history)\n",
        "\n",
        "\n",
        "# End of TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf3jmNK-nsiF"
      },
      "source": [
        "Now you will implement three different neural networks using PyTorch: a shallow neural network, a medium-depth neural network, and a deep neural network. Follow the steps below to complete the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQZcfS_KpNSB"
      },
      "source": [
        "- Define a class `ShallowNetwork` inheriting from `nn.Module`.\n",
        "- Implement the `__init__` method with parameters `input_dim=1`, `hidden_dim=16`, and `output_dim=1`.\n",
        "- Add layers:\n",
        "  - `fc1`: fully connected layer from `input_dim` to `hidden_dim`.\n",
        "  - `fc2`: fully connected layer from `hidden_dim` to `output_dim`.\n",
        "  - Activation: `nn.ReLU()`.\n",
        "- Implement the `forward` method to apply activation after `fc1` and pass the output through `fc2`.\n",
        "\n",
        "\n",
        "\n",
        "- Define a class `MediumNetwork` inheriting from `nn.Module`.\n",
        "- Implement `__init__` with `input_dim=1`, `hidden_dim=32`, and `output_dim=1`.\n",
        "- Add layers:\n",
        "  - `fc1`: `input_dim` to `hidden_dim`.\n",
        "  - `fc2`: `hidden_dim` to `hidden_dim`.\n",
        "  - `fc3`: `hidden_dim` to `output_dim`.\n",
        "  - Activation: `nn.ReLU()`.\n",
        "- Implement `forward` with activation after `fc1` and `fc2`, then pass through `fc3`.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Define a class `DeepNetwork` inheriting from `nn.Module`.\n",
        "- Implement `__init__` with `input_dim=1`, `hidden_dim=64`, and `output_dim=1`.\n",
        "- Add layers:\n",
        "  - `fc1`: `input_dim` to `hidden_dim`.\n",
        "  - `fc2`, `fc3`, `fc4`: each `hidden_dim` to `hidden_dim`.\n",
        "  - `fc5`: `hidden_dim` to `output_dim`.\n",
        "  - Activation: `nn.ReLU()`.\n",
        "- Implement `forward` with activation after `fc1`, `fc2`, `fc3`, and `fc4`, then pass through `fc5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmqYMGDMmXwF"
      },
      "outputs": [],
      "source": [
        "# TODO: create the NNs that we described above\n",
        "\n",
        "# Define a shallow neural network (1 hidden layer)\n",
        "class ShallowNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=16, output_dim=1):\n",
        "        super(ShallowNetwork, self).__init__()\n",
        "        # Fully connected layer from input_dim to hidden_dim\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # Fully connected layer from hidden_dim to output_dim\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        # Activation function (ReLU)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply activation after the first layer\n",
        "        x = self.activation(self.fc1(x))\n",
        "        # Pass through the second layer\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define a medium-depth neural network (2 hidden layers)\n",
        "class MediumNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1):\n",
        "        super(MediumNetwork, self).__init__()\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        # Activation function (ReLU)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply activation after the first layer\n",
        "        x = self.activation(self.fc1(x))\n",
        "        # Apply activation after the second layer\n",
        "        x = self.activation(self.fc2(x))\n",
        "        # Pass through the third layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define a deep neural network (4 hidden layers)\n",
        "class DeepNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=64, output_dim=1):\n",
        "        super(DeepNetwork, self).__init__()\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc5 = nn.Linear(hidden_dim, output_dim)\n",
        "        # Activation function (ReLU)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply activation after the first layer\n",
        "        x = self.activation(self.fc1(x))\n",
        "        # Apply activation after the second layer\n",
        "        x = self.activation(self.fc2(x))\n",
        "        # Apply activation after the third layer\n",
        "        x = self.activation(self.fc3(x))\n",
        "        # Apply activation after the fourth layer\n",
        "        x = self.activation(self.fc4(x))\n",
        "        # Pass through the fifth layer\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# END of TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UbpoA0Vxn7Ua",
        "outputId": "76fe4a08-761f-4462-ab7e-48ee804c946d"
      },
      "outputs": [],
      "source": [
        "# TODO: Train Shallow Network\n",
        "model = ShallowNetwork()\n",
        "\n",
        "x_train_tensor, y_train_tensor, x_train, y_train = generate_training_data(hard_function)\n",
        "plot_training_data(x_train, y_train,hard_function)\n",
        "\n",
        "loss_history1 = train_model(model, x_train_tensor, y_train_tensor , hard_function)\n",
        "plot_loss_curve(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i0184YKnqJ8i",
        "outputId": "539abcc8-a908-4971-89d1-3243f7c3f2fd"
      },
      "outputs": [],
      "source": [
        "# TODO Train Medium Network\n",
        "model = MediumNetwork()\n",
        "\n",
        "x_train_tensor, y_train_tensor, x_train, y_train = generate_training_data(hard_function)\n",
        "plot_training_data(x_train, y_train,hard_function)\n",
        "\n",
        "loss_history2 = train_model(model, x_train_tensor, y_train_tensor , hard_function)\n",
        "plot_loss_curve(loss_history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mTTG025_qJux",
        "outputId": "ea6f448e-78db-41f2-f42f-0a9ba2ab9941"
      },
      "outputs": [],
      "source": [
        "# TODO Train Deep Network\n",
        "model = DeepNetwork()\n",
        "\n",
        "x_train_tensor, y_train_tensor, x_train, y_train = generate_training_data(hard_function)\n",
        "plot_training_data(x_train, y_train,hard_function)\n",
        "\n",
        "loss_history3 = train_model(model, x_train_tensor, y_train_tensor , hard_function)\n",
        "plot_loss_curve(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "QomgdSwOn8bc",
        "outputId": "de82c2f6-952b-4575-e535-aefce9489ad1"
      },
      "outputs": [],
      "source": [
        "# TODO: create a function named compare_loss_curves\n",
        "# and plot the loss of three models in one plot to compare them\n",
        "def compare_loss_curves(loss_history1,loss_history2,loss_history3):\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(loss_history1, label=\"Shallow Network\", color=\"b\", linewidth=2)\n",
        "  plt.plot(loss_history2, label=\"Medium Network\", color=\"g\", linewidth=2)\n",
        "  plt.plot(loss_history3, label=\"Deep Network\", color=\"r\", linewidth=2)\n",
        "\n",
        "  plt.title(f\"Loss Curve Comparison: Different Network Depths\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "compare_loss_curves(loss_history1,loss_history2,loss_history3)\n",
        "# END of TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxjiaHtsontR"
      },
      "source": [
        "Now want to see the effect of number of layers and neurons. first we define a simple funtion and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "Xr1jivjwoJUf",
        "outputId": "e2d4994c-d359-4926-c593-c2b2f08197ff"
      },
      "outputs": [],
      "source": [
        "# Define the function to fit\n",
        "def hybrid_function(x):\n",
        "    return np.exp(-x**2) + 0.5*x**3 - 0.3*x\n",
        "\n",
        "# Generate training data\n",
        "print(\"\\nGenerating Data for Hybrid Function\")\n",
        "x_train_tensor, y_train_tensor, x_train, y_train = generate_training_data(true_func=hybrid_function)\n",
        "\n",
        "# Plot training data\n",
        "plot_training_data(x_train, y_train, true_func=hybrid_function)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7OrlXlSrcoh"
      },
      "source": [
        "you will implement three different neural networks with varying depths and neuron counts using PyTorch to observe the effect of layers and the number of neurons. Note that all of these models have the same number of neurons.\n",
        "\n",
        "- Define a class `WideShallowNetwork` inheriting from `nn.Module`.\n",
        "- Implement `__init__` with `input_dim=1`, `hidden_dim=128`, and `output_dim=1`.\n",
        "- Add layers:\n",
        "  - `fc1`: fully connected layer from `input_dim` to `hidden_dim`.\n",
        "  - `fc2`: fully connected layer from `hidden_dim` to `output_dim`.\n",
        "  - Activation: `nn.ReLU()`.\n",
        "- Implement the `forward` method to apply activation after `fc1` and pass the output through `fc2`.\n",
        "\n",
        "\n",
        "\n",
        "- Define a class `BalancedNetwork` inheriting from `nn.Module`.\n",
        "- Implement `__init__` with `input_dim=1`, `hidden_dim=64`, and `output_dim=1`.\n",
        "- Add layers:\n",
        "  - `fc1`: `input_dim` to `hidden_dim`.\n",
        "  - `fc2`: `hidden_dim` to `hidden_dim`.\n",
        "  - `fc3`: `hidden_dim` to `output_dim`.\n",
        "  - Activation: `nn.ReLU()`.\n",
        "- Implement `forward` with activation after `fc1` and `fc2`, then pass through `fc3`.\n",
        "\n",
        "\n",
        "- Define a class `DeepNarrowNetwork` inheriting from `nn.Module`.\n",
        "- Implement `__init__` with `input_dim=1`, `hidden_dim=32`, and `output_dim=1`.\n",
        "- Add layers:\n",
        "  - `fc1`: `input_dim` to `hidden_dim`.\n",
        "  - `fc2`, `fc3`, `fc4`: each `hidden_dim` to `hidden_dim`.\n",
        "  - `fc5`: `hidden_dim` to `output_dim`.\n",
        "  - Activation: `nn.ReLU()`.\n",
        "- Implement `forward` with activation after `fc1`, `fc2`, `fc3`, and `fc4`, then pass through `fc5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro80FbsWoox8"
      },
      "outputs": [],
      "source": [
        "# TODO: create the NNs that we described above\n",
        "\n",
        "# Wide-Shallow Network (1 layer with many neurons)\n",
        "class WideShallowNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=128, output_dim=1):\n",
        "        super(WideShallowNetwork, self).__init__()\n",
        "        # Fully connected layer from input_dim to hidden_dim\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # Fully connected layer from hidden_dim to output_dim\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        # Activation function (ReLU)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply activation after the first layer\n",
        "        x = self.activation(self.fc1(x))\n",
        "        # Pass through the second layer\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Balanced Network (2 layers, medium neurons)\n",
        "class BalancedNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=64, output_dim=1):\n",
        "        super(BalancedNetwork, self).__init__()\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        # Activation function (ReLU)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply activation after the first layer\n",
        "        x = self.activation(self.fc1(x))\n",
        "        # Apply activation after the second layer\n",
        "        x = self.activation(self.fc2(x))\n",
        "        # Pass through the third layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Deep-Narrow Network (4 layers, fewer neurons per layer)\n",
        "class DeepNarrowNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1):\n",
        "        super(DeepNarrowNetwork, self).__init__()\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc5 = nn.Linear(hidden_dim, output_dim)\n",
        "        # Activation function (ReLU)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply activation after the first layer\n",
        "        x = self.activation(self.fc1(x))\n",
        "        # Apply activation after the second layer\n",
        "        x = self.activation(self.fc2(x))\n",
        "        # Apply activation after the third layer\n",
        "        x = self.activation(self.fc3(x))\n",
        "        # Apply activation after the fourth layer\n",
        "        x = self.activation(self.fc4(x))\n",
        "        # Pass through the fifth layer\n",
        "        x = self.fc5(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AP9J3ObZovLn",
        "outputId": "6e6ddda7-cb50-4a77-cddc-1a5378552b3a"
      },
      "outputs": [],
      "source": [
        "# TODO: Train Wide-Shallow Network\n",
        "model = WideShallowNetwork()\n",
        "\n",
        "loss_history1 = train_model(model, x_train_tensor, y_train_tensor , hybrid_function)\n",
        "plot_loss_curve(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2bwACTpwsZ2R",
        "outputId": "317f0ef0-6de2-461b-d103-55f83d29d524"
      },
      "outputs": [],
      "source": [
        "# TODO: Train Balanced Network\n",
        "model = BalancedNetwork()\n",
        "\n",
        "\n",
        "loss_history2 = train_model(model, x_train_tensor, y_train_tensor , hybrid_function)\n",
        "plot_loss_curve(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jv5xVTj-sZOP",
        "outputId": "fa5240cc-5ff0-4dcf-be06-1ddaed15d74f"
      },
      "outputs": [],
      "source": [
        "# TODO: Train Deep-Narrow Network\n",
        "model = DeepNarrowNetwork()\n",
        "\n",
        "\n",
        "loss_history3 = train_model(model, x_train_tensor, y_train_tensor , hybrid_function)\n",
        "plot_loss_curve(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "MVJyMaV9o1r6",
        "outputId": "502f96cb-8b4c-4557-99a2-ddb951912ab8"
      },
      "outputs": [],
      "source": [
        "# TODO: create a function named compare_loss_curves\n",
        "# and plot the loss of three models in one plot to compare them\n",
        "def compare_loss_curves(loss1,loss2,loss3):\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(loss1, label=\"Wide-Shallow (1 Layer, 128 Neurons)\", color=\"b\", linewidth=2)\n",
        "  plt.plot(loss2, label=\"Balanced (2 Layer, 64 Neurons)\", color=\"g\", linewidth=2)\n",
        "  plt.plot(loss3, label=\"Deep-Narrow (4 Layer, 32 Neurons)\", color=\"r\", linewidth=2)\n",
        "\n",
        "  plt.title(f\"Loss Curve Comparison: Different Network Depths\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "compare_loss_curves(loss_history1,loss_history2,loss_history3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB5GpnNxpFb_"
      },
      "source": [
        "Now we see their performance of a hard and complex function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "ob4p25fIo4nZ",
        "outputId": "208bbc64-9c11-4c94-8f39-b4a18edd80eb"
      },
      "outputs": [],
      "source": [
        "# Define the complex sinusoidal function\n",
        "def complex_sinusoidal_function(x):\n",
        "    return np.exp(-x**2) * np.sin(10*x) + 0.3 * np.sin(30*x) + 0.2 * np.cos(50*x)\n",
        "\n",
        "# Generate training data\n",
        "print(\"\\nGenerating Data for Complex Sinusoidal Function\")\n",
        "x_train_tensor, y_train_tensor, x_train, y_train = generate_training_data(true_func=complex_sinusoidal_function)\n",
        "\n",
        "# Plot training data\n",
        "plot_training_data(x_train, y_train, true_func=complex_sinusoidal_function)\n",
        "\n",
        "def train_model(model, x_train_tensor, y_train_tensor,true_function, num_epochs=10, batch_size=32, lr=0.01, plot_interval=1 ):\n",
        "\n",
        "    # TODO: Define loss function and optimizer. Create the proper DataLoader.\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    # End of TODO\n",
        "\n",
        "\n",
        "    loss_history = []  # Store loss values for visualization\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            # TODO: Forward pass and backpropagate\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "            # Forward pass\n",
        "            y_pred = model(batch_x)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(y_pred, batch_y)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # End of TODO\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "        # TODO: Store average loss in loss_history\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        loss_history.append(avg_loss)\n",
        "        # End of TODO\n",
        "\n",
        "\n",
        "        # TODO: Plot function every 'plot_interval' epochs. And Print progress\n",
        "        # every 100 epochs\n",
        "        if epoch % plot_interval == 0 or epoch == num_epochs - 1:\n",
        "            model.eval()  # Set model to evaluation mode\n",
        "            x_values = torch.linspace(-2, 2, 400).view(-1, 1)\n",
        "            y_values = true_function(x_values.numpy())\n",
        "            y_pred = model(x_values).detach().numpy()\n",
        "\n",
        "            plt.figure(figsize=(8, 5))\n",
        "            plt.scatter(x_train, y_train, color=\"r\", label=\"Noisy Samples\", alpha=0.6)\n",
        "            plt.plot(x_values.numpy(), y_values, label=\"True Function\", color=\"b\", linewidth=2)\n",
        "            plt.plot(x_values.numpy(), y_pred, label=\"Model Prediction\", color=\"g\", linewidth=2)\n",
        "            plt.xlabel(\"x\")\n",
        "            plt.ylabel(\"f(x)\")\n",
        "            plt.title(f\"Epoch {epoch}: Model Approximation\")\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "            model.train()\n",
        "        if epoch % 100 == 0 or epoch == num_epochs - 1:\n",
        "            print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        # End of TODO\n",
        "\n",
        "\n",
        "    return loss_history , y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j5tQk6gWpGqG",
        "outputId": "3a93b3e5-ad59-4918-964e-8acd7d8355c4"
      },
      "outputs": [],
      "source": [
        "# TODO: Train Wide-Shallow Network\n",
        "model = WideShallowNetwork()\n",
        "\n",
        "loss_history1 , y1 = train_model(model, x_train_tensor, y_train_tensor , complex_sinusoidal_function)\n",
        "plot_loss_curve(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nP1nWRHAs-bm",
        "outputId": "79092ef7-f4a3-4f8d-8978-665ec3468d32"
      },
      "outputs": [],
      "source": [
        "# TODO: Train Balanced Network\n",
        "model = BalancedNetwork()\n",
        "\n",
        "loss_history2 , y2= train_model(model, x_train_tensor, y_train_tensor , complex_sinusoidal_function)\n",
        "plot_loss_curve(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KDUnkfp-s98c",
        "outputId": "ce4ca135-4233-4754-983e-6d21fbae47b4"
      },
      "outputs": [],
      "source": [
        "# TODO: Train Deep-Narrow Network\n",
        "model = DeepNarrowNetwork()\n",
        "\n",
        "loss_history3 ,y3 = train_model(model, x_train_tensor, y_train_tensor , complex_sinusoidal_function)\n",
        "plot_loss_curve(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Eher0f13to0t",
        "outputId": "3ed789d3-61b0-4804-fec5-037ad607b800"
      },
      "outputs": [],
      "source": [
        "# TODO: create a function named compare_learned_functions\n",
        "# and plot the output of three models in one plot to compare them with the real funtion\n",
        "def compare_learned_functions(true_func,pred1,pred2,pred3):\n",
        "  x_values = torch.linspace(-2, 2, 400).view(-1, 1)\n",
        "  y_values = true_func(x_values.numpy())\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot()\n",
        "  plt.plot(x_values.numpy(), y_values, label=\"True Function\", color=\"black\", linewidth=2 , linestyle = 'dashed')\n",
        "  plt.plot(x_values.numpy(),pred1, label=\"Wide-Shallow\", color=\"b\", linewidth=2)\n",
        "  plt.plot(x_values.numpy(),pred2, label=\"Balanced\", color=\"g\", linewidth=2)\n",
        "  plt.plot(x_values.numpy(),pred3, label=\"Deep-Narrow\", color=\"r\", linewidth=2)\n",
        "\n",
        "  plt.title(f\"Comparison of Learned Function: Depths vs. Width\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.show()\n",
        "\n",
        "compare_learned_functions(complex_sinusoidal_function,y1,y2,y3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ATi28ow5pQi9",
        "outputId": "5b29829b-55df-49f7-84aa-98c1f5dabb8d"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot the loss of three models in one plot to compare them\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "plt.plot(loss_history1, label=\"Wide-Shallow (1 Layer, 128 Neurons)\", color=\"b\", linewidth=2)\n",
        "plt.plot(loss_history2, label=\"Balanced (2 Layer, 64 Neurons)\", color=\"g\", linewidth=2)\n",
        "plt.plot(loss_history3, label=\"Deep-Narrow (4 Layer, 32 Neurons)\", color=\"r\", linewidth=2)\n",
        "\n",
        "plt.title(f\"Loss Curve Comparison: Different Network Depths\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVVBn83epqHx"
      },
      "source": [
        "# MNIST Classification (10 points)\n",
        "In the next part we will use MNIST data and train a model to classify the digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBIscEI-pqKf"
      },
      "source": [
        "## Step 1: Load and Preprocess the MNIST Dataset\n",
        "\n",
        "\n",
        "Before training our model, we need to load the MNIST dataset. The MNIST dataset consists of 60,000 training images and 10,000 test images of handwritten digits (0-9). Each image is 28√ó28 pixels in grayscale.\n",
        "\n",
        "We'll use PyTorch's `torchvision.datasets` module to download and load the dataset. We also need to transform the images into tensors and normalize them to improve training stability.\n",
        "\n",
        "### üîπ Data Preprocessing\n",
        "- Convert images to tensors using `ToTensor()`.\n",
        "- Normalize images to have zero mean and unit variance using `Normalize()`.\n",
        "- Use PyTorch's `DataLoader` to efficiently load the dataset in batches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjI2zeTxpW9t",
        "outputId": "f4e15ca0-442b-4c42-c08f-9addbc638116"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# TODO: Define transformation: convert images to tensors and normalize them\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize with mean=0.5 and std=0.5\n",
        "])\n",
        "\n",
        "# Then download and load the training and test datasets using the transform\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Convert to DataLoaders for batch processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# End of TODO\n",
        "# Check dataset size\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7tI2gvirPDT"
      },
      "source": [
        "## Step 2: Define a Simple MLP Model\n",
        "\n",
        "\n",
        "Now that we have the MNIST dataset ready, we will define a simple feedforward neural network (MLP) to classify the digits.\n",
        "\n",
        "### üîπ Model Architecture\n",
        "- **Input Layer**: 28x28 pixels (flattened to 784)\n",
        "- **Hidden Layers**: Two fully connected layers (Linear layers)\n",
        "- **Activation Function**: ReLU for non-linearity\n",
        "- **Output Layer**: 10 neurons (one per digit), using Softmax\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKIVa34WrKg4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# TODO: Define a simple MLP model that gets the image and outputs 10 numbers (logits)\n",
        "# No softmax needed (handled in loss function)\n",
        "\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# END of TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_RpnEhRsUsV"
      },
      "source": [
        "## Step 3: Train the MLP Model and Visualize Layer Activations\n",
        "\n",
        "\n",
        "Now that we have defined our MLP model, we need to:\n",
        "1. **Train** the model on MNIST.\n",
        "2. **Capture activations** from each layer after every epoch.\n",
        "3. **Reduce activation dimensions** using **t-SNE**.\n",
        "4. **Visualize** the transformed activations in 2D.\n",
        "\n",
        "### üîπ Why Visualize Activations?\n",
        "- Helps understand how the network **transforms** input data.\n",
        "- Detects potential issues like **vanishing activations** or **saturation**.\n",
        "- Gives insights into **how different layers learn representations**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTGpalNwvAyD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm  # Progress bar\n",
        "\n",
        "# Define device (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = SimpleMLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHRte-3PvX6a",
        "outputId": "1cdb28da-f0e0-44cd-cc8c-38a55bcf1ce7"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# Implement functionality to capture activations from specific layers of a neural network model.\n",
        "# 1. Initialize a data structure to store activations from layers like \"fc1\", \"fc2\", and \"fc3\".\n",
        "# 2. Define a hook function that:\n",
        "#    - Captures the output of a given layer.\n",
        "#    - Detaches it from the computation graph, moves it to the CPU, and converts it to a NumPy array.\n",
        "#    - Stores the result in the activation data structure.\n",
        "# 3. Register this hook function to the target layers using PyTorch's `register_forward_hook`.\n",
        "\n",
        "activations = {}  # Global dictionary to store activations\n",
        "\n",
        "def capture_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        if name not in activations:\n",
        "            activations[name] = []  # Initialize list if not present\n",
        "        activations[name].append(output.detach().cpu().numpy())  # Store activations\n",
        "    return hook\n",
        "\n",
        "# Register hooks to capture activations after each layer\n",
        "model.fc1.register_forward_hook(capture_activation('fc1'))\n",
        "model.fc2.register_forward_hook(capture_activation('fc2'))\n",
        "model.fc3.register_forward_hook(capture_activation('fc3'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE9QvwEwvoan"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement a function to visualize neural network activations across layers.\n",
        "# The function should:\n",
        "# 1. Accept activation data from various layers and the current epoch.\n",
        "# 2. Perform dimensionality reduction on activations using t-SNE.\n",
        "# 3. Efficiently sample a subset of activations for performance.\n",
        "# 4. Create subplots for t-SNE visualizations for each layer.\n",
        "# 5. Use appropriate color maps, labels, and titles for clarity.\n",
        "# 6. Display the visualizations in a well-organized layout.\n",
        "\n",
        "\n",
        "\n",
        "def plot_activations(activation_data, epoch):\n",
        "    sample_size = 1000  # Limit samples to avoid memory issues\n",
        "\n",
        "    # Ensure activations exist before processing\n",
        "    activations = {\n",
        "        layer: np.concatenate(data, axis=0)[:sample_size] if len(data) > 0 else np.array([])\n",
        "        for layer, data in activation_data.items()\n",
        "    }\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(activations), figsize=(15, 5))\n",
        "\n",
        "    # üîπ Set a single global title for all subplots\n",
        "    plt.suptitle(f'Activations Distributions Across Layers (Epoch {epoch+1})', fontsize=14)\n",
        "\n",
        "    for i, (layer, activations_layer) in enumerate(activations.items()):\n",
        "        if activations_layer.size > 0:  # Ensure non-empty activations\n",
        "            n_samples = activations_layer.shape[0]\n",
        "            perplexity = min(30, max(5, n_samples - 1))  # Ensure a valid range\n",
        "\n",
        "            # Perform t-SNE\n",
        "            tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
        "            reduced_activations = tsne.fit_transform(activations_layer)\n",
        "\n",
        "            # üîπ Generate colors for scatter plot\n",
        "            colors = np.arange(n_samples)  # Color by index for a gradient effect\n",
        "\n",
        "            # üîπ Fix cmap issue: Use `c` for coloring\n",
        "            scatter = axes[i].scatter(\n",
        "                reduced_activations[:, 0], reduced_activations[:, 1],\n",
        "                c=colors, cmap='viridis', s=20, alpha=0.75, edgecolors='k'\n",
        "            )\n",
        "\n",
        "            fig.colorbar(scatter, ax=axes[i])  # Optional: Add colorbar per subplot\n",
        "            axes[i].set_title(f'{layer} - t-SNE (Epoch {epoch +1})')\n",
        "\n",
        "        else:\n",
        "            axes[i].text(0.5, 0.5, 'No activations', ha='center', va='center', fontsize=12)\n",
        "            axes[i].set_title(f'{layer} - t-SNE (Epoch {epoch + 1})')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit suptitle\n",
        "    plt.show()\n",
        "\n",
        "# End of TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZiYA1dXrSYj"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement a training function with visualization\n",
        "# 1. Set the model to training mode.\n",
        "# 2. Iterate through multiple epochs:\n",
        "#    a. Reset activation storage at the start of each epoch to ensure fresh data.\n",
        "#    b. Initialize a running loss variable to track training progress.\n",
        "#    c. Use a progress bar to provide real-time feedback on training.\n",
        "# 3. Within each epoch, loop over the training dataset:\n",
        "#    a. Move images and labels to the appropriate device (CPU/GPU).\n",
        "#    b. Zero the gradients of the optimizer before each forward pass.\n",
        "#    c. Perform a forward pass through the model.\n",
        "#    d. Compute the loss using the given criterion.\n",
        "#    e. Perform backpropagation to compute gradients.\n",
        "#    f. Update model parameters using the optimizer.\n",
        "#    g. Accumulate the loss for tracking progress.\n",
        "#    h. Update the progress bar with the current loss.\n",
        "# 4. After each epoch, print the average loss for monitoring performance.\n",
        "# 5. Visualize activations at the end of each epoch using plot_activations.\n",
        "\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_with_visualization(model, train_loader, criterion, optimizer, epochs=5):\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        activations.clear()  # Reset activations for the new epoch\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Use tqdm to display progress with live loss updates\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
        "\n",
        "        for images, labels in progress_bar:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # Reset gradients\n",
        "            outputs = model(images)  # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # üîπ Update tqdm progress bar with live loss\n",
        "            progress_bar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
        "\n",
        "        # üîπ Compute and print final loss after epoch\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # üîπ Visualize activations after each epoch\n",
        "        plot_activations(activations, epoch)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# End of TODO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P1TuKte-yUFs",
        "outputId": "1ecc6127-b956-40d5-9642-51ec572fba81"
      },
      "outputs": [],
      "source": [
        "# TODO: Train the model and visualize activations\n",
        "train_with_visualization(model, train_loader, criterion, optimizer, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2fjnTunuhyR"
      },
      "source": [
        "## Understanding Activation Clustering in the Output Layer (FC3)\n",
        "\n",
        "In our t-SNE visualization, we notice that **activations in the FC3 layer** form roughly **10 clusters**.\n",
        "\n",
        "**Why does the FC3 layer form 10 distinct clusters?**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbBwvJsshIjW"
      },
      "source": [
        "### My answer\n",
        "The FC3 layer forms 10 distinct clusters because it represents the logits for the 10 digit classes (0-9). Each neuron in FC3 corresponds to a digit, and the network learns to activate one neuron strongly per class, leading to clear separation. t-SNE preserves this structure, revealing well-defined clusters that indicate strong class-specific representations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xXWE_hGhJVv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
