{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5fd69e",
   "metadata": {
    "id": "5e5fd69e"
   },
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"https://www.sharif.ir/documents/20124/0/logo-fa-IR.png/4d9b72bc-494b-ed5a-d3bb-e7dfd319aec8?t=1609608338755\" alt=\"Logo\" width=\"200\">\n",
    "    <p><b>HW1 @ Deep Learning Course, Dr. Soleymani</b></p>\n",
    "    <p><b>ŸêDesinged by Amirmahdi Meighani</b></p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sJ87dPg0aerE",
   "metadata": {
    "id": "sJ87dPg0aerE"
   },
   "source": [
    "\n",
    "*Full Name:*\n",
    "\n",
    "*Student Number:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiTc9GwyxFbP"
   },
   "source": [
    "# Efficient Gradient Checkpointing for Memory-Constrained Deep Learning (50 points)\n",
    "Deep learning experiments are often limited by GPU memory constraints, making it challenging to train large models. To overcome this, we implemented gradient checkpointing, a technique that significantly reduces memory usage by strategically storing intermediate activations and recomputing them during backpropagation. This allows us to train models that would otherwise exceed our GPU‚Äôs memory capacity.\n",
    "\n",
    "In this guide, you'll first implement gradient checkpointing from scratch to understand its inner workings. Then, you'll learn how to leverage PyTorch's built-in checkpointing feature for more efficient deep learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tcGtSaizW0e"
   },
   "source": [
    "### How Gradient Checkpointing Works\n",
    "We divide the neural network into segments and only store activations at the segment boundaries (checkpoints). The activations for intermediate layers are discarded and recomputed during backpropagation.\n",
    "\n",
    "Step-by-Step Process:\n",
    "\n",
    "\n",
    "\n",
    "*   **Forward Pass (Training Phase)**\n",
    "\n",
    "\n",
    "\n",
    "1.  Divide the model into segments (e.g., every few layers).\n",
    "2.  Save activations only at checkpoint layers.\n",
    "3.  Discard activations of intermediate layers.\n",
    "4.  Proceed as usual to compute the final output.\n",
    "\n",
    "\n",
    "\n",
    "*   **Backward Pass (Gradient Calculation)**\n",
    "\n",
    "\n",
    "\n",
    "1. Recompute missing activations for each segment.\n",
    "2. Compute gradients using the recomputed activations.\n",
    "3. Update model parameters with computed gradients.\n",
    "\n",
    "\n",
    "By recomputing only small segments at a time, we save significant memory while keeping the computational cost manageable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VBfGyOGyMou"
   },
   "source": [
    "First import what you need and check your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDhYof0c_WJQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Additional imports for checkpointing examples\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "# End of Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXVCjK-oyCbW",
    "outputId": "12cc639b-c8e1-4049-eee4-71744a13c870"
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # You must use cuda for this notebook\n",
    "\n",
    "\n",
    "# Function to measure GPU memory usage\n",
    "def get_gpu_memory_usage():\n",
    "    # Todo: return the gpu memory useage for comparison\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / (1024 ** 2)  # MB\n",
    "    return 0.0\n",
    "    # End of Todo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xK84pCyvyYpG"
   },
   "source": [
    "We have created a small model for using it with and with out lazy gradient. You can change it if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoGbByYRIavG"
   },
   "outputs": [],
   "source": [
    "# Define a model for intensive GPU usage\n",
    "# You can change it\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.before = nn.Sequential(\n",
    "            nn.Linear(200*200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.after = nn.Sequential(\n",
    "            nn.Linear(32, 3000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3000, 200*200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200*200, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        bottleneck_out = self.before(x)\n",
    "        final_out = self.after(bottleneck_out)\n",
    "        return final_out, bottleneck_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B749SC2jysWf"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_dataloader(batch_size=128):\n",
    "    # Todo: Create synthetic data and return a dataloader\n",
    "    # The data must be compatible with the model you use\n",
    "    num_samples = 1024\n",
    "    x = torch.randn(num_samples, 200 * 200)\n",
    "    # Simple linear target with noise\n",
    "    y = torch.sum(x, dim=1, keepdim=True) * 0.001 + torch.randn(num_samples, 1) * 0.01\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    # End of Todo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YL0hFZZIx6-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(use_lazy_grad=False, num_epochs=1):\n",
    "    dataloader = create_dataloader()\n",
    "    print('Data is Created')\n",
    "    model = Model().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    mem_usage = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_x, batch_y in tqdm(dataloader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x.requires_grad = True  # Track gradients\n",
    "            final_out, bottleneck_out = model(batch_x)\n",
    "\n",
    "\n",
    "            if use_lazy_grad:\n",
    "                # Todo: implement the lazy gradient method, using the bottleneck\n",
    "                # as the only segmentation. You should compute gradient w.r.t\n",
    "                # bottleneck output and then backpropagate manually\n",
    "                loss = criterion(final_out, batch_y)\n",
    "                grad_final = torch.autograd.grad(loss, final_out, create_graph=False, retain_graph=True)[0]\n",
    "                grad_bottleneck = torch.autograd.grad(final_out, bottleneck_out, grad_final, retain_graph=True)[0]\n",
    "                bottleneck_out.backward(grad_bottleneck)\n",
    "                # End of Todo\n",
    "\n",
    "            else:\n",
    "                # Todo: normal back prop of loss\n",
    "                loss = criterion(final_out, batch_y)\n",
    "                loss.backward()\n",
    "                # End of Todo\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            mem_usage.append(get_gpu_memory_usage())\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return mem_usage, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HIHESsQzC4X",
    "outputId": "d6f26f83-8246-4c56-ef7b-17dab3943722"
   },
   "outputs": [],
   "source": [
    "# Done:\n",
    "# Run training with and without lazy gradient propagation\n",
    "# Note that you should empty cache of cuda before, between and after your trainings\n",
    "# So your results will be valid. Save the mem_usage and elapsed_time of each one\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "mem_lazy, time_lazy = train_model(use_lazy_grad=True)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "mem_normal, time_normal = train_model(use_lazy_grad=False)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# End of Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "4Wri9xYQ1wdN",
    "outputId": "a30a2867-cee6-4059-8422-14f53951e06a"
   },
   "outputs": [],
   "source": [
    "# Done: Plot memory usage and compare the time\n",
    "# that each method needs\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mem_lazy, label='Lazy gradient')\n",
    "plt.plot(mem_normal, label='Standard')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('GPU Memory (MB)')\n",
    "plt.legend()\n",
    "plt.title('Memory usage comparison')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Lazy gradient time: {time_lazy:.2f}s, Standard time: {time_normal:.2f}s\")\n",
    "\n",
    "# End of Done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCfcyl9H16Sb"
   },
   "source": [
    "PyTorch provides a built-in gradient checkpointing feature through torch.utils.checkpoint. This makes it easy to implement checkpointing without manually managing activation storage and recomputation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLrb5hYsHcjv"
   },
   "source": [
    "First, let's create a simple Sequential model and checkpoint it. We can also verify that the checkpointing doesn't change the value of gradients or the activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etml_C4IHda9",
    "outputId": "30ceddf2-d38a-4eca-87c4-f4386a716394"
   },
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "\n",
    "\n",
    "# Done: create a simple Sequential model and then create the model inputs.\n",
    "# get the modules in the model. These modules should be in the order\n",
    "# the model should be executed. Then set the number of checkpoint segments.\n",
    "# Now call the checkpoint API and get the output.\n",
    "# finally run the backwards pass on the model. For simplicity,\n",
    "# we won't calculate the loss and rather backprop on output.sum()\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 32), nn.ReLU(),\n",
    "    nn.Linear(32, 16), nn.ReLU(),\n",
    "    nn.Linear(16, 4)\n",
    ").to(device)\n",
    "\n",
    "inp = torch.randn(8, 10, device=device, requires_grad=True)\n",
    "modules = list(model.children())\n",
    "segments = 2\n",
    "out = checkpoint_sequential(modules, segments, inp)\n",
    "\n",
    "(out.sum()).backward()\n",
    "\n",
    "# End of Done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egn1PoE2L65t"
   },
   "source": [
    "Now that we have executed the checkpointed pass on the model, let's also run the non-checkpointed model and verify that the checkpoint API doesn't change the model outputs or the parameter gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTo9dV21L87M"
   },
   "outputs": [],
   "source": [
    "# Done: use the non-checkpointed mode. create a new variable using the same\n",
    "# tensor data. get the model output.\n",
    "\n",
    "model.zero_grad(set_to_none=True)\n",
    "inp_nc = inp.detach().clone().requires_grad_(True)\n",
    "out = model(inp_nc)\n",
    "out.sum().backward()\n",
    "\n",
    "# End of Done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pFFg-c9MLbL"
   },
   "source": [
    "Now that we have done the checkpointed and non-checkpointed pass of the model and saved the output and parameter gradients, let's compare their values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQ9zmfkcMN7T"
   },
   "outputs": [],
   "source": [
    "# Done: compare the output and parameters gradients with and without checkpoint.\n",
    "# they must be equal.\n",
    "\n",
    "assert torch.allclose(output_checkpointed, out_not_checkpointed)\n",
    "for name in grad_checkpointed:\n",
    "    assert torch.allclose(grad_checkpointed[name], grad_not_checkpointed[name])\n",
    "\n",
    "# End of Done\n",
    "\n",
    "print(\"All checks passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56A_N_M3Ma4t"
   },
   "source": [
    "So, from this example, we can see that it's very easy to use checkpointing on Sequential models and that the checkpoint API doesn't alter any data. The Checkpoint API implementation is based on autograd and hence there is no need for explicitly specifying what the execution of backwards should look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEeJgxBwR_JP"
   },
   "source": [
    "# Gradient Accumulation in Deep Learning (25 points)\n",
    "\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "Training deep learning models with large batch sizes can be difficult due to memory limitations, especially when using large datasets or deep networks. **Gradient Accumulation** is a technique that allows us to simulate large batch sizes without increasing memory usage. This tutorial will cover:\n",
    "\n",
    "1. **The theory behind Gradient Accumulation**\n",
    "2. **Using it in PyTorch**\n",
    "3. **Observing its effects**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Theory Behind Gradient Accumulation\n",
    "\n",
    "### 2.1 What is Gradient Accumulation?\n",
    "Instead of updating the model parameters after every mini-batch, **Gradient Accumulation** allows us to accumulate gradients over multiple mini-batches before performing an update. This effectively simulates training with a larger batch size.\n",
    "\n",
    "### 2.2 Why Use Gradient Accumulation?\n",
    "- **Overcome Memory Limits**: Training with large batch sizes often exceeds GPU memory capacity. Accumulating gradients allows training on smaller mini-batches while maintaining the benefits of larger batch training.\n",
    "- **Stable Training**: Larger batch sizes help in stable updates and reducing variance in gradient estimation.\n",
    "- **Effective Batch Size**: If GPU memory allows batch size `B` but we need `N`, we can accumulate gradients for `N/B` steps before updating.\n",
    "\n",
    "### 2.3 How Does It Work?\n",
    "If `loss` is computed on `batch_size = B`, instead of calling `optimizer.step()` every step, we:\n",
    "1. Compute gradients on `B` and accumulate them.\n",
    "2. Repeat for `K` iterations, accumulating gradients.\n",
    "3. Update weights only after `K` steps.\n",
    "4. Reset gradients after update.\n",
    "\n",
    "This results in an **effective batch size** of `B * K` without needing extra memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3M4jGKzjswGd"
   },
   "source": [
    "first create a simple model and implement the function to train with accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlVph1YjhQdg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_with_gradient_accumulation(model, dataloader, accumulation_steps):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    mem_usage = []\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # Done: Get loss and Normalize loss by accumulation steps\n",
    "        # Then backpropagate loss.  Accumulate gradients and update after\n",
    "        # `accumulation_steps`\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets) / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        mem_usage.append(get_gpu_memory_usage())\n",
    "\n",
    "        # End of Done\n",
    "\n",
    "    return mem_usage\n",
    "\n",
    "\n",
    "# Done: Define a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim=5, hidden_dim=32, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "# End of Done\n",
    "\n",
    "\n",
    "model = SimpleModel().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1t7_MX4ws5NH"
   },
   "source": [
    "Now create a dataloader with the mentioned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qn5mPZnHtAYi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_dataloader(batch_size, accumulation_steps):\n",
    "    # Done: create dataloader with batch size and same number of samples\n",
    "    # for different accumulation_steps\n",
    "\n",
    "    num_samples = batch_size * accumulation_steps * 5\n",
    "    x = torch.randn(num_samples, 5)\n",
    "    y = 2 * x.sum(dim=1, keepdim=True) + 3 + torch.randn(num_samples, 1) * 0.1\n",
    "    dataset = TensorDataset(x, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # End of Done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdlqgbb4tChj"
   },
   "source": [
    "Lets train with accumulation and save mem usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUngBnO5tHUH",
    "outputId": "3ec06849-23fb-4064-e06c-c1d21b454127"
   },
   "outputs": [],
   "source": [
    "batch_size = 100  # Small batch size due to memory limits you can change it\n",
    "effective_batch_size = 400\n",
    "accumulation_steps = effective_batch_size // batch_size\n",
    "\n",
    "# Done: train with accumulation_steps and save mem_usage\n",
    "# dont forget to empty cuda cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "model_accum = SimpleModel().to(device)\n",
    "dataloader_accum = create_dataloader(batch_size, accumulation_steps)\n",
    "mem_usage_accum = train_with_gradient_accumulation(model_accum, dataloader_accum, accumulation_steps)\n",
    "\n",
    "# End of Done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLmsvbEQtUs1"
   },
   "source": [
    "And now train in the classic way with no accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZyulZ2qnFh-",
    "outputId": "199b107f-cf9f-42e4-af02-37146e23df1c"
   },
   "outputs": [],
   "source": [
    "# Done: train with accumulation_steps=1 (it means standard back prop)\n",
    "# and save mem_usage\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "model_no_accum = SimpleModel().to(device)\n",
    "dataloader_no_accum = create_dataloader(batch_size, 1)\n",
    "mem_usage_no_accum = train_with_gradient_accumulation(model_no_accum, dataloader_no_accum, accumulation_steps=1)\n",
    "\n",
    "# End of Done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I7EsSvnttF2"
   },
   "source": [
    "Check if we actually used less memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "zZ4sNg0ftymJ",
    "outputId": "b8281c90-3e7d-468f-bb9f-6c02ec22dc36"
   },
   "outputs": [],
   "source": [
    "# Done: Plot memory usage\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(mem_usage_accum, label='With accumulation')\n",
    "plt.plot(mem_usage_no_accum, label='No accumulation')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('GPU Memory (MB)')\n",
    "plt.legend()\n",
    "plt.title('Gradient accumulation memory usage')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# End of Done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkEyfapirUnh"
   },
   "source": [
    "##  Conclusion\n",
    "\n",
    "- **Gradient Accumulation** helps simulate large batch sizes without requiring more memory.\n",
    "- It improves **training stability** and allows training on **memory-constrained GPUs**.\n",
    "- We implemented it **from scratch** and in **PyTorch**.\n",
    "- We visualized its **effects** on loss stabilization.\n",
    "\n",
    "Gradient Accumulation is a useful trick when working with **deep networks and large datasets** on limited hardware. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gz7HCNMKw6AQ"
   },
   "source": [
    "# Sources of Randomness (5 points)\n",
    "run the code below. the outputs are different. Why do you think this happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9YuEzyOYx43P",
    "outputId": "c0c504d5-6eba-4da0-98f4-5274ed5d4c51"
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 5).to(device)\n",
    "model = SimpleModel(input_dim=5, hidden_dim=1500,output_dim=1).to(device)\n",
    "\n",
    "out = model(input)\n",
    "print(out)\n",
    "\n",
    "model = SimpleModel(input_dim=5, hidden_dim=1500,output_dim=1).to(device)\n",
    "\n",
    "out = model(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiFRFPK7yt7n"
   },
   "source": [
    "## Controlling Sources of Randomness in PyTorch Models (with GPU)\n",
    "\n",
    "### Introduction\n",
    "Randomness plays a crucial role in deep learning, but uncontrolled randomness can lead to inconsistent results. This part explores the sources of randomness in PyTorch and how to control them, especially when using a GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Understanding Sources of Randomness**\n",
    "In PyTorch, randomness can come from multiple sources:\n",
    "\n",
    "1. **Python's built-in random module**: Used for operations that involve randomness in Python code.\n",
    "2. **NumPy**: If NumPy is used for data augmentation or initialization.\n",
    "3. **PyTorch CPU Randomness**: Random initialization of weights, dropout layers, etc.\n",
    "4. **PyTorch GPU Randomness**: When CUDA is used, operations can be non-deterministic.\n",
    "5. **cuDNN Backend**: NVIDIA's cuDNN has optimizations that may introduce non-determinism.\n",
    "\n",
    "To get reproducible results, all these sources must be controlled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWK0sdP22upC"
   },
   "source": [
    "## **2. Setting Seeds in PyTorch**\n",
    "To control randomness, we define a function that sets the seed for all sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_1V5NuG2yip"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    # Done: set these seeds:\n",
    "    # Python Hash seed\n",
    "    # Python random module\n",
    "    # NumPy seed\n",
    "    # PyTorch CPU seed\n",
    "    # PyTorch CUDA seed\n",
    "    # Multi-GPU seed\n",
    "    # Ensure deterministic cudnn\n",
    "    # Disables optimization that may introduce randomness(benchmark)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # End of Done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWyHuR0F3CeH"
   },
   "source": [
    "Now check if the results are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28KYX2nX23cN",
    "outputId": "606e6368-955d-4f74-c5e3-090805a088d7"
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 5).to(device)\n",
    "\n",
    "set_seed(123)  # Set seed to 123 (or any fixed value)\n",
    "model = SimpleModel(input_dim=5, hidden_dim=1500,output_dim=1).to(device)\n",
    "\n",
    "out = model(input)\n",
    "print(out)\n",
    "\n",
    "set_seed(123)  # Set seed to 123 (or any fixed value)\n",
    "\n",
    "model = SimpleModel(input_dim=5, hidden_dim=1500,output_dim=1).to(device)\n",
    "\n",
    "out = model(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5L0iare3WAM"
   },
   "source": [
    "## **Key Takeaways**\n",
    "1. Randomness in PyTorch comes from multiple sources: Python, NumPy, PyTorch (CPU & GPU), and cuDNN.\n",
    "2. Using `set_seed()` ensures reproducibility in experiments.\n",
    "3. cuDNN optimizations can introduce non-determinism; setting `torch.backends.cudnn.deterministic = True` helps mitigate this.\n",
    "4. Always set the seed before model initialization to ensure identical starting conditions.\n",
    "\n",
    "Reproducibility is critical for debugging and fair benchmarking of deep learning models. üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dP9w2HP4HuG"
   },
   "source": [
    "**Note**\n",
    "\n",
    "Setting torch.backends.cudnn.deterministic = True makes the code slower because it forces cuDNN to use deterministic algorithms instead of its default highly-optimized, non-deterministic implementations. Here‚Äôs why:\n",
    "1. cuDNN Optimizations\n",
    "\n",
    "cuDNN (CUDA Deep Neural Network Library) provides highly optimized implementations of deep learning operations, such as convolutions and matrix multiplications. By default, cuDNN selects the fastest algorithm available based on the given input size, hardware, and configuration.\n",
    "2. Deterministic vs. Non-Deterministic Algorithms\n",
    "\n",
    "Some of cuDNN‚Äôs fastest algorithms introduce minor sources of randomness due to floating-point precision differences in parallel execution, particularly in:\n",
    "\n",
    "    Convolution operations (e.g., torch.nn.Conv2d)\n",
    "    Batch normalization\n",
    "    Recurrent layers (e.g., LSTMs)\n",
    "\n",
    "When torch.backends.cudnn.deterministic = True, PyTorch forces cuDNN to use only deterministic versions of these algorithms. However, deterministic algorithms are not always the most optimized ones, leading to slower performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MbFW4pH9k8_"
   },
   "source": [
    "# Second-Order Gradients in Deep Learning (20 points)\n",
    "\n",
    "What are Second-Order Gradients?\n",
    "\n",
    "In deep learning, second-order gradients refer to the derivatives of gradients (i.e., the second derivative of a loss function with respect to model parameters). These are commonly used in optimization methods that require information about the curvature of the loss function.\n",
    "\n",
    "\n",
    "\n",
    "### **Mathematically**:\n",
    "- **First-order gradient**: $ g = \\nabla_\\theta L(\\theta) $ (gradient of loss $ L $ w.r.t. parameters $ \\theta $)\n",
    "- **Second-order gradient (Hessian)**: $ H = \\nabla^2_\\theta L(\\theta) $ (derivative of $ g $, which captures curvature)\n",
    "\n",
    "\n",
    "###**Why Use Second-Order Gradients?**\n",
    "\n",
    " *   Better Optimization ‚Äì Second-order methods like Newton‚Äôs Method use curvature information to converge faster than first-order methods (like SGD).\n",
    " *    Natural Gradient Descent ‚Äì Second-order gradients help in adapting the learning rate in different directions based on the Hessian matrix.\n",
    " *   Meta-Learning ‚Äì Algorithms like MAML (Model-Agnostic Meta-Learning) require second-order gradients to update learning rates.\n",
    " *   Adversarial Training ‚Äì Computing second-order derivatives is useful for generating adversarial examples.\n",
    " *   Regularization ‚Äì Used in some regularization techniques like curvature-based penalties.\n",
    "\n",
    "\n",
    "\n",
    "###**Challenges of Second-Order Gradients**\n",
    "\n",
    " * Computationally Expensive ‚Äì Computing second-order derivatives (Hessian) can be costly, especially for deep networks.\n",
    " * Memory Intensive ‚Äì Requires additional memory, making it impractical for very large models.\n",
    " * Numerical Stability ‚Äì Sometimes leads to unstable gradients and requires careful tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlP1wlI_C4-x"
   },
   "source": [
    "The second derivative (Hessian matrix) provides information about how the gradient changes. By moving in the direction of the second-order gradient, we can adaptively adjust step sizes based on how steep or flat the loss landscape is.\n",
    "\n",
    "In the simplest case:\n",
    "\n",
    "$\n",
    "Œ∏‚Ä≤=Œ∏‚àíŒ±‚àáL(Œ∏)‚àíŒ≤‚àá2L(Œ∏)\n",
    "$\n",
    "\n",
    "where:\n",
    "\n",
    " * ‚àá2L(Œ∏) (the second-order gradient) adjusts the update based on curvature,\n",
    " * Œ≤ is a small scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AEKRwhk_ynt"
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Done: define a model (1d to 1d)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        # End of Done\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Done: implement forward\n",
    "        return self.net(x)\n",
    "        # End of Done\n",
    "\n",
    "\n",
    "def generate_data():\n",
    "    # Done: Generate data for 1d to 1d model\n",
    "    # a simple linear data with noise is ok. For example: y = 2x + 3 + noise\n",
    "    x = torch.linspace(-5, 5, steps=200).unsqueeze(1)\n",
    "    noise = torch.randn_like(x) * 0.2\n",
    "    y = 2 * x + 3 + noise\n",
    "    # End of Done\n",
    "    return x, y\n",
    "\n",
    "\n",
    "model = SimpleNet()\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "x_train, y_train = generate_data()\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXtzPDRbANm5",
    "outputId": "89d0eeea-fdbf-49ee-ce51-f04477bc95c1"
   },
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Done: implement the loop using second-order gradients.\n",
    "    # first, compute first-order gradient. Then Convert first-order gradient\n",
    "    # to scalar before computing second-order gradient Update parameters\n",
    "    # manually. using first order and second order gradients.\n",
    "    # print loss in every step\n",
    "    optimizer.zero_grad()\n",
    "    preds = model(x_train)\n",
    "    loss = loss_fn(preds, y_train)\n",
    "    grads_first = torch.autograd.grad(loss, model.parameters(), create_graph=True)\n",
    "    grad_scalar = sum((g ** 2).sum() for g in grads_first)\n",
    "    grads_second = torch.autograd.grad(grad_scalar, model.parameters())\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p, g1, g2 in zip(model.parameters(), grads_first, grads_second):\n",
    "            p -= 0.01 * g1 + 0.001 * g2\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss={loss.item():.4f}\")\n",
    "    # End of Done\n",
    "\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "CuBNlCk2BUk1",
    "outputId": "2685cd68-d249-4496-c41a-eefb5a73eeca"
   },
   "outputs": [],
   "source": [
    "# Done: evaluate the model using some data and visuilize the model output\n",
    "# and compare it with the real function\n",
    "x_eval = torch.linspace(-6, 6, steps=100).unsqueeze(1)\n",
    "y_true = 2 * x_eval + 3\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_eval)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x_eval.numpy(), y_true.numpy(), label='True function')\n",
    "plt.scatter(x_eval.numpy(), y_pred.numpy(), s=10, label='Model prediction')\n",
    "plt.legend()\n",
    "plt.title('Second-order training result')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# End of Done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVsqXB18CopB"
   },
   "source": [
    "## Interpretation of Moving in the Second-Order Direction\n",
    "\n",
    "* If the second derivative is large (high curvature) ‚Üí The loss is changing rapidly ‚Üí Smaller step sizes.\n",
    "* If the second derivative is small (low curvature) ‚Üí The loss changes slowly ‚Üí Larger step sizes.\n",
    "\n",
    "This is the core idea behind Newton's method"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
