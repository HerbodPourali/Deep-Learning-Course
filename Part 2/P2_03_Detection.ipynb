{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJwtTQjaMtKk"
   },
   "source": [
    "<div align=\"center\">\n",
    "    <font color=\"0F5298\" size=\"7\">\n",
    "        Deep Learning <br>\n",
    "    </font>\n",
    "    <font color=\"2565AE\" size=\"5\">\n",
    "        CE Department <br>\n",
    "        Spring 2024 - Prof. Soleymani Baghshah <br>\n",
    "    </font>\n",
    "    <font color=\"3C99D\" size=\"5\">\n",
    "        HW2 Practical <br>\n",
    "    </font>\n",
    "    <font color=\"696880\" size=\"5\">\n",
    "        30 Points\n",
    "    </font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMASOqb4MtKm"
   },
   "outputs": [],
   "source": [
    "FULLNAME = 'YOUR NAME'\n",
    "STD_ID = 'YOUR ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8LNFrrrMtKm"
   },
   "source": [
    "# Q3. License Plate Detection and Recognition (30 points)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment, we will build a two-stage license plate recognition system:\n",
    "\n",
    "1. First stage: Detect license plates in an image (license plate detection / LPD)\n",
    "2. Second stage: Recognize characters within the detected license plate (license plate recognition / LPR)\n",
    "\n",
    "This approach is not explicitly an OCR (Optical Character Recognition), but it has some similarities with important differences. Unlike general OCR which often relies on sequence models (like RNNs or Transformers) to capture language context, license plates have a fixed format with predictable structure. This allows us to use multi-class classification or object detection models for it - first to locate plates, then to locate and classify individual characters.\n",
    "\n",
    "## Background on YOLO (You Only Look Once)\n",
    "\n",
    "### Evolution of Object Detection\n",
    "\n",
    "Object detection has evolved significantly over the years:\n",
    "- **Two-stage detectors** (like R-CNN family): First propose regions, then classify them\n",
    "- **Single-stage detectors** (like YOLO and SSD): Predict bounding boxes and classes in a single forward pass\n",
    "\n",
    "YOLO revolutionized object detection by framing it as a regression problem rather than a classification problem. Instead of generating region proposals and then classifying each region (a slow, two-stage process), YOLO divides the image into a grid and predicts bounding boxes and class probabilities directly in a single forward pass.\n",
    "\n",
    "### YOLO Architecture\n",
    "\n",
    "![YOLO Architecture](https://velog.velcdn.com/images/hunniee_j/post/cbd3888c-8b75-4325-988f-eadaded84232/image.JPG)\n",
    "\n",
    "The basic YOLO approach:\n",
    "\n",
    "1. **Grid Division**: Divide the image into an S×S grid\n",
    "2. **Bounding Box Prediction**: Each grid cell predicts B bounding boxes, each with 5 parameters (x, y, w, h, confidence)\n",
    "3. **Class Prediction**: Each grid cell also predicts class probabilities\n",
    "4. **Non-Maximum Suppression**: Remove overlapping boxes with lower confidence scores\n",
    "\n",
    "for a brief explanation of Object detection from RCNN to yolo version8 visit this [link](https://youtube.com/playlist?list=PL8VDJoEXIjppNvOzocFbRciZBrtSMi81v&si=qIh3VagQOzgWZ7Go)\n",
    "\n",
    "the latest version of yolo is YOLO 12 and [here](https://docs.ultralytics.com/models/yolo12/) is Ultralytics documentation about it (all models of yolo family available in ultralytics has accessible documentation  [here](https://docs.ultralytics.com/models/) too.)\n",
    "\n",
    "\n",
    "## Ultralytics Framework\n",
    "\n",
    "Ultralytics is a Python library that makes it easy to train, test, and deploy YOLO models. Key features:\n",
    "\n",
    "- **Easy to use API**: Simple Python API for training, validation, and inference\n",
    "- **Pre-trained models**: Various pre-trained models of different sizes (nano to extra large)\n",
    "- **Export options**: Export to various formats (ONNX, TFLite, CoreML, etc.)\n",
    "- **Multi-task learning**: Support for object detection, segmentation, and pose estimation\n",
    "\n",
    "\n",
    "\n",
    "## Assignment Tasks\n",
    "\n",
    "In this assignment, you will:\n",
    "\n",
    "1. Train a YOLO model for license plate detection on LPD dataset\n",
    "2. Train a CNN model for license plate recognition within LPR license plates (7 digit + 1 letter classificaiton task)\n",
    "3. Create an end-to-end pipeline that connects step 1 with 2\n",
    "4. Evaluate the performance of your system on the test data\n",
    "\n",
    "\n",
    "The dataset are available in [this](https://drive.google.com/drive/folders/1StRhbI28MaoiuXqA2rG5vGqKG5K2bMW6?usp=sharing) drive folder. let's dive into it!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSuRJq1ZMtKn",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "d6c3cee6-13f7-4fca-99f3-d12c9474ef66"
   },
   "outputs": [],
   "source": [
    "#Necessary installations\n",
    "\n",
    "!pip install -q ultralytics\n",
    "!pip install -q matplotlib opencv-python pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OxHQrK7vMtKo"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import random\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn-Zq_bdMtKo",
    "outputId": "5de17223-efc6-4975-c9dc-7b5c1e9c5650"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uKtqFuqHMtKo"
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/home/physical_security/Projects/__Amir__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvnhw0KzMtKo"
   },
   "source": [
    "### 1. LPD - YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHzUaXMEMtKp"
   },
   "outputs": [],
   "source": [
    "LPD_RELATIVE_PATH = 'IR-LPD'\n",
    "LPD_DIR = f'{BASE_PATH}/{LPD_RELATIVE_PATH}'\n",
    "\n",
    "images_dir = os.path.join(LPD_DIR, 'images')\n",
    "labels_dir = os.path.join(LPD_DIR, 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk1HD4VIMtKp",
    "outputId": "64d43a81-3139-4c29-93bd-308c1fb0bbd8"
   },
   "outputs": [],
   "source": [
    "image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "label_files = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]\n",
    "\n",
    "print(f\"Dataset contains {len(image_files)} images and {len(label_files)} labels\")\n",
    "\n",
    "\n",
    "objects_count = 0\n",
    "for label_file in [os.path.join(labels_dir, f) for f in label_files]:\n",
    "    with open(label_file, 'r') as f:\n",
    "        objects_count += len(f.readlines())\n",
    "\n",
    "print(f\"Total license plates in dataset: {objects_count}\")\n",
    "print(f\"Average plates per image: {objects_count / max(1, len(image_files)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUYIQ_HiMtKp"
   },
   "source": [
    "<font color=\"orange\">\n",
    "1. What is the structure of YOLO's label files (`.txt`), and why are bounding box coordinates normalized?  \n",
    "<br>2. How does the YAML configuration file in YOLO define a dataset, and what role does the `nc` (number of classes) parameter play?  \n",
    "<br>3. Why does YOLO use a grid system for predictions, and how does it handle multiple objects in a single grid cell?  \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3b3mrfCMtKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "# you must create a yaml file there is just one class object detection and you must split the dataset into two parts (80%-20% for train and val)\n",
    "# the executed result of cell is belong to train = val = all data for better understanding about the YOLO results.\n",
    "\n",
    "\n",
    "YAML_PATH =  os.path.join(LPD_DIR, 'data.yaml')\n",
    "\n",
    "# split dataset\n",
    "all_imgs = sorted(image_files)\n",
    "split_idx = int(0.8 * len(all_imgs))\n",
    "train_imgs = all_imgs[:split_idx]\n",
    "val_imgs = all_imgs[split_idx:]\n",
    "\n",
    "train_list = os.path.join(LPD_DIR, 'train.txt')\n",
    "val_list = os.path.join(LPD_DIR, 'val.txt')\n",
    "\n",
    "with open(train_list, 'w') as f:\n",
    "    for img in train_imgs:\n",
    "        f.write(os.path.join(images_dir, img) + '\n",
    "')\n",
    "\n",
    "with open(val_list, 'w') as f:\n",
    "    for img in val_imgs:\n",
    "        f.write(os.path.join(images_dir, img) + '\n",
    "')\n",
    "\n",
    "yaml_content = {\n",
    "    'path': LPD_DIR,\n",
    "    'train': train_list,\n",
    "    'val': val_list,\n",
    "    'names': ['plate'],\n",
    "    'nc': 1\n",
    "}\n",
    "\n",
    "import yaml\n",
    "with open(YAML_PATH, 'w') as f:\n",
    "    yaml.safe_dump(yaml_content, f)\n",
    "\n",
    "print(f\"YAML written to {YAML_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZgeRA39MtKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "VERSION = '8'\n",
    "MODEL_SIZE = 'n'  # Options: n, s, m, l, x\n",
    "EPOCHS = 20\n",
    "IMGSZ = 640\n",
    "BATCH = 16\n",
    "DEVICE = '0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AxNNIYnMtKp"
   },
   "source": [
    "<font color=\"orange\">\n",
    "Compare the architectural and functional advancements in YOLO versions 8 through 12. Specifically:  \n",
    "\n",
    "1. **YOLOv8**:  \n",
    "   - What was the motivation behind adopting an anchor-free design, and how did this impact training complexity and performance?  \n",
    "   - How did the integration of CSPDarknet and PANet improve feature extraction and multi-scale detection compared to earlier versions?  \n",
    "\n",
    "2. **YOLOv9 (Hypothetical/Unofficial)**:  \n",
    "   - If YOLOv9 introduced dynamic label assignment, how does this differ from static assignment in YOLOv8, and what are the implications for model accuracy and convergence speed?  \n",
    "   - What role might lightweight model variants (e.g., YOLOv9n) play in edge-device deployment, and how were they optimized for resource-constrained environments?  \n",
    "\n",
    "3. **YOLOv10**:  \n",
    "   - How did hybrid loss functions (e.g., combining CIoU and focal loss) enhance the training process, and what challenges in object detection were they designed to address?  \n",
    "   - What advancements in model pruning and quantization were introduced, and how did these techniques reduce model size without compromising accuracy?  \n",
    "\n",
    "4. **YOLOv11**:  \n",
    "   - In what ways did self-calibrated convolutions improve feature extraction, and how do they compare to traditional convolutional layers in terms of computational efficiency and accuracy?  \n",
    "   - How did the introduction of multi-task learning (e.g., joint object detection and segmentation) expand the capabilities of YOLOv11, and what new applications does this enable?  \n",
    "\n",
    "5. **YOLOv12**:  \n",
    "   - What benefits does the integration of transformer-based modules bring to YOLOv12, and how does this hybrid architecture balance the strengths of CNNs and transformers?  \n",
    "   - How does uncertainty estimation in YOLOv12 improve the reliability of predictions, particularly in safety-critical applications like autonomous driving or medical imaging?  \n",
    "   - Discuss the role of domain adaptation techniques in YOLOv12 and how they address challenges like dataset bias or environmental variability.  \n",
    "\n",
    "Based on these advancements, which version would you recommend for this aplication, and why?  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueFl2tfRMtKp"
   },
   "outputs": [],
   "source": [
    "\n",
    "LPD_model = f'yolov{VERSION}{MODEL_SIZE}.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjqSZj5yMtKq",
    "outputId": "fdf94be2-58b9-4a19-b868-17abf9d42d1a"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Starting YOLO{VERSION}{MODEL_SIZE} training for {EPOCHS} epochs...\")\n",
    "\n",
    "model = YOLO(LPD_model)\n",
    "results = model.train(\n",
    "    data=YAML_PATH,\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMGSZ,\n",
    "    batch=BATCH,\n",
    "    device=DEVICE,\n",
    "    project=os.path.join(BASE_PATH, 'model'),\n",
    "    name='LPD',\n",
    "    exist_ok=True,\n",
    ")\n",
    "\n",
    "# path to best weights\n",
    "LPD_model = os.path.join(results.save_dir, 'weights', 'best.pt')\n",
    "print(f\"Training complete! Model saved to {LPD_model}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P10Q5vcIMtKq",
    "outputId": "30c5f35c-5770-4ec5-cef6-dca826e330ff"
   },
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Testing model on sample images...\")\n",
    "model = YOLO(LPD_model)\n",
    "for img_file in random.sample(image_files, min(5, len(image_files))):\n",
    "    img_path = os.path.join(images_dir, img_file)\n",
    "    preds = model.predict(img_path, imgsz=IMGSZ, conf=0.25, device=DEVICE)\n",
    "    res_img = preds[0].plot()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(res_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(img_file)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-EQdx04MtKq"
   },
   "source": [
    "### 2. LPR - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PMfNTosMtKq"
   },
   "outputs": [],
   "source": [
    "LPR_RELATIVE_PATH = 'IR-LPR'\n",
    "\n",
    "df = pd.read_csv(f'{BASE_PATH}/{LPR_RELATIVE_PATH}/valid_samples.csv')\n",
    "df = df.sample(n=22000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "136YNTINMtKq"
   },
   "outputs": [],
   "source": [
    "\n",
    "digit_vocabulary = \"0123456789\"\n",
    "persian_letters = \"آ ب پ ت ث ج چ ح خ د ذ ر ز ژ س ش ص ض ط ظ ع غ ف ق ک گ ل م ن و ه ی\".split()\n",
    "\n",
    "\n",
    "digit_to_idx = {char: idx for idx, char in enumerate(digit_vocabulary)}\n",
    "letter_to_idx = {char: idx for idx, char in enumerate(persian_letters)}\n",
    "idx_to_digit = {idx: char for idx, char in enumerate(digit_vocabulary)}\n",
    "idx_to_letter = {idx: char for idx, char in enumerate(persian_letters)}\n",
    "\n",
    "\n",
    "persian_to_english_digits = {\n",
    "    '۰': '0', '۱': '1', '۲': '2', '۳': '3', '۴': '4',\n",
    "    '۵': '5', '۶': '6', '۷': '7', '۸': '8', '۹': '9'\n",
    "}\n",
    "\n",
    "persian_letter_normalization = {\n",
    "    \"الف\": \"آ\",\n",
    "    \"ا\" : \"آ\",\n",
    "    \"ژ (معلولین و جانبازان)\": \"ژ\",\n",
    "    \"ه‍\" : \"ه\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def translate(label):\n",
    "\n",
    "    first_two_digits = ''.join([persian_to_english_digits.get(char, char) for char in label[:2]])\n",
    "    persian_letter = label[2]\n",
    "    remaining_digits = ''.join([persian_to_english_digits.get(char, char) for char in label[3:]])\n",
    "    return first_two_digits + persian_letter + remaining_digits\n",
    "\n",
    "\n",
    "def preprocess_sample(image_path, label , full_transform=True, log=False, dir_path = BASE_PATH , relative_path= f'{LPR_RELATIVE_PATH}/detections', language='en'):\n",
    "\n",
    "    if language == 'fa' :\n",
    "      label = translate(label)\n",
    "\n",
    "    elif language != 'en':\n",
    "          raise Exception('Un-supported language!')\n",
    "\n",
    "    path = f'{dir_path}/{relative_path}'\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((96, 192)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((96, 192)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    transform = train_transform if full_transform else test_transform\n",
    "    image = Image.open(f'{path}/{image_path}').convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "\n",
    "\n",
    "    for key, value in persian_letter_normalization.items():\n",
    "        label = label.replace(key, value)\n",
    "\n",
    "\n",
    "\n",
    "    if log:\n",
    "      print(label)\n",
    "\n",
    "      for i , c in enumerate(label):\n",
    "        if c in persian_letters:\n",
    "          c = '*'\n",
    "        print(f\"{i}:{c}\", end=' | ')\n",
    "\n",
    "      print()\n",
    "\n",
    "\n",
    "    digits = [digit_to_idx[char] for char in label if char.isdigit()]\n",
    "    letter = letter_to_idx[label[2]]\n",
    "\n",
    "    return image, digits, letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2FZiyXrMtKr"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PLPRDataset(data.Dataset):\n",
    "    def __init__(self, df , split='train',dir_path=BASE_PATH,relative_path = f'{LPR_RELATIVE_PATH}/detections', language='en'):\n",
    "        self.df = df\n",
    "        self.full_transform = True if split == 'train' else False\n",
    "        self.dir_path = dir_path\n",
    "        self.relative_path = relative_path\n",
    "        self.language = language\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        label = row['label']\n",
    "        image, digits, letter = preprocess_sample(\n",
    "            image_path,\n",
    "            label,\n",
    "            full_transform=self.full_transform,\n",
    "            dir_path=self.dir_path,\n",
    "            relative_path=self.relative_path,\n",
    "            language=self.language,\n",
    "        )\n",
    "\n",
    "        # ensure 7 digits (pad with zeros if needed)\n",
    "        if len(digits) < 7:\n",
    "            digits = digits + [0] * (7 - len(digits))\n",
    "        digits = digits[:7]\n",
    "\n",
    "        digits_tensor = torch.tensor(digits, dtype=torch.long)\n",
    "        letter_tensor = torch.tensor(letter, dtype=torch.long)\n",
    "        return image, digits_tensor, letter_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNoT2vTIMtKr"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "class FCNPLPRModel(nn.Module):\n",
    "    def __init__(self, backbone_name=\"efficientnet_b0\"):\n",
    "        super(FCNPLPRModel, self).__init__()\n",
    "\n",
    "        # Use a lightweight backbone\n",
    "        self.backbone = models.efficientnet_b0(weights=None)\n",
    "        self.feature_extractor = self.backbone.features\n",
    "        self.out_channels = self.backbone.classifier[1].in_features\n",
    "\n",
    "        self.digit_head = nn.Linear(self.out_channels, 7 * 10)\n",
    "        self.letter_head = nn.Linear(self.out_channels, len(persian_letters))\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.feature_extractor(x)\n",
    "        feats = nn.functional.adaptive_avg_pool2d(feats, (1, 1))\n",
    "        feats = feats.view(feats.size(0), -1)\n",
    "\n",
    "        digit_logits = self.digit_head(feats).view(-1, 7, 10)\n",
    "        letter_output = self.letter_head(feats)\n",
    "        #digit_outputs  -> [batch_size, 7, 10]\n",
    "        #letter_output -> [batch_size, num_persian_letters]\n",
    "\n",
    "        return digit_logits, letter_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3IcaH0WMtKr"
   },
   "outputs": [],
   "source": [
    "learning_rate = 4e-4\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fP-BSMJtMtKr"
   },
   "outputs": [],
   "source": [
    "dataset = PLPRDataset(df)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        # persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxqC4NwUMtKr"
   },
   "outputs": [],
   "source": [
    "LPR_model = FCNPLPRModel().to(device)\n",
    "digit_criterion = nn.CrossEntropyLoss()\n",
    "letter_criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBsVzI50MtKr",
    "outputId": "5493458b-8317-4b7c-eaee-a802fc82dd7d"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(LPR_model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    threshold=0.05,\n",
    "    threshold_mode='rel',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAI8ZyIfMtKs"
   },
   "outputs": [],
   "source": [
    "def decode_predictions(digit_outputs, letter_output):\n",
    "    digit_predictions = torch.argmax(digit_outputs, dim=2)  # [batch_size, 7]\n",
    "    batch_size = digit_predictions.size(0)\n",
    "\n",
    "    digits = []\n",
    "    for b in range(batch_size):  # Iterate over batch\n",
    "        sample_digits = []\n",
    "        for i in range(7):  # Iterate over 7 digits\n",
    "            try:\n",
    "                sample_digits.append(idx_to_digit[digit_predictions[b][i].item()])\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError: {e} (digit_predictions[{b}][{i}] = {digit_predictions[b][i].item()})\")\n",
    "                sample_digits.append(\"?\")  # Use a placeholder for invalid indices\n",
    "        digits.append(sample_digits)\n",
    "\n",
    "    letter_prediction = torch.argmax(letter_output, dim=1)  # [batch_size]\n",
    "    letters = []\n",
    "    for b in range(batch_size):\n",
    "        try:\n",
    "            letters.append(idx_to_letter[letter_prediction[b].item()])\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError: {e} (letter_prediction[{b}] = {letter_prediction[b].item()})\")\n",
    "            letters.append(\"?\")  # Use a placeholder for invalid indices\n",
    "\n",
    "    labels = []\n",
    "    for b in range(batch_size):\n",
    "        label = \"\".join(digits[b][:2]) + letters[b] + \"\".join(digits[b][2:])\n",
    "        labels.append(label)\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_misclassification(gt, pred):\n",
    "    assert len(gt) == len(pred), \"GT and Pred must have the same length.\"\n",
    "\n",
    "    # Collect misclassified characters\n",
    "    misclassified = []\n",
    "    char_error_count = 0\n",
    "\n",
    "    for i, (gt_char, pred_char) in enumerate(zip(gt, pred)):\n",
    "        if gt_char != pred_char:\n",
    "            misclassified.append(f\"{gt_char} with {pred_char} at pos {i}\")\n",
    "            char_error_count += 1\n",
    "\n",
    "    # Format the output\n",
    "    misclassified_str = \" , \".join(misclassified) if misclassified else \"None\"\n",
    "    result = (f\"GT: {gt} | Pred: {pred} | \"\n",
    "              f\"Misclassified: {misclassified_str} | \"\n",
    "              f\"Char error count: {char_error_count}\")\n",
    "\n",
    "    return result , char_error_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_accuracy(model, df, device, dir_path=BASE_PATH, relative_path=f'{LPR_RELATIVE_PATH}/detections', language='en', log=False , cer = False):\n",
    "    model.eval()\n",
    "    FP= []\n",
    "    CE = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            image_path = row[\"image_path\"]\n",
    "            label = row[\"label\"]\n",
    "            try:\n",
    "                image, true_digits, true_letter = preprocess_sample(image_path, label, full_transform=False, dir_path=dir_path, relative_path=relative_path, language=language)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")\n",
    "                continue\n",
    "            true_label = \"\".join([digit_vocabulary[d] for d in true_digits[:2]]) \\\n",
    "                         + persian_letters[true_letter] \\\n",
    "                         + \"\".join([digit_vocabulary[d] for d in true_digits[2:]])\n",
    "            image = image.unsqueeze(0).to(device)\n",
    "            digit_outputs, letter_output = model(image)\n",
    "\n",
    "            predicted_labels = decode_predictions(digit_outputs, letter_output)\n",
    "\n",
    "            if predicted_labels[0] == true_label:\n",
    "                correct += 1\n",
    "            elif log:\n",
    "                report , char_error_count = evaluate_misclassification(true_label,predicted_labels[0])\n",
    "                CE += char_error_count\n",
    "                FP.append(report)\n",
    "            total += 1\n",
    "\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "\n",
    "    if cer:\n",
    "        print(f'CER: {100* CE/(8*df.shape[0]): .4f}%')\n",
    "\n",
    "    if log:\n",
    "         for false_positive in FP:\n",
    "            print(false_positive)\n",
    "\n",
    "\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-ay6I26MtKs"
   },
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(f'{BASE_PATH}/IR-LPR/valid_samples.csv')\n",
    "test_df = full_df.loc[~full_df.index.isin(df.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-9tpVFHMtKs",
    "outputId": "0af353f6-4823-4439-8996-052428e070cb"
   },
   "outputs": [],
   "source": [
    "best_accuracy = 85.0\n",
    "optimal_weights = None\n",
    "num_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    LPR_model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "\n",
    "    for images, digit_targets, letter_targets in progress_bar:\n",
    "        images = images.to(device)\n",
    "        digit_targets = digit_targets.to(device)\n",
    "        letter_targets = letter_targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        digit_outputs, letter_output = LPR_model(images)\n",
    "\n",
    "        digit_loss = 0\n",
    "        for i in range(7):\n",
    "            digit_loss += digit_criterion(digit_outputs[:, i, :], digit_targets[:, i])\n",
    "        letter_loss = letter_criterion(letter_output, letter_targets)\n",
    "        loss = digit_loss + letter_loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    LPR_model.eval()\n",
    "    accuracy = calculate_accuracy(\n",
    "        LPR_model,\n",
    "        test_df,\n",
    "        device,\n",
    "        relative_path= f'{LPR_RELATIVE_PATH}/detections',\n",
    "        language='en'\n",
    "    )\n",
    "    print(f\"Accuracy after Epoch {epoch+1}: {accuracy:.2f}%\")\n",
    "\n",
    "    scheduler.step(accuracy)\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        optimal_weights = LPR_model.state_dict()  # Save the current model weights\n",
    "        print(f\"New best accuracy: {best_accuracy:.2f}%. Saving model weights.\")\n",
    "        torch.save(LPR_model.state_dict(), f'{BASE_PATH}/model/PLPR-CNN.pth')\n",
    "\n",
    "if optimal_weights is not None:\n",
    "    LPR_model.load_state_dict(optimal_weights)\n",
    "    print(\"Loaded optimal weights into the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZre7cIWMtKs",
    "outputId": "97c2be5e-502d-41a3-f6f3-bbe78f873d24"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(LPR_model, test_df, device, cer=True, log=True)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSDpe-2RMtKs"
   },
   "source": [
    "### 3. E2E LPDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6qJW_hYMtKs"
   },
   "source": [
    "load the models from part 2 and 3, build a class named E2E_LPDR and plot report the metrics (accuracy and cer) for the test dataset (20% of LPD that has Splitted in section 1 ). you must also plot some samples with predicted bounding box and label too.\n",
    "\n",
    "\n",
    "note that each picture might have multiple plates but in this test data for simplicity all samples just have a single plate exists in it.\n",
    "plot some samples from your pipeline (predicted bb from an image + predicted label of it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfUQkOXlMtKt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGAj3ysJMtKt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
